{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.3) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import time\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import pytz\n",
    "import dash \n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math as math\n",
    "import plotly.graph_objs as go\n",
    "import dash_table\n",
    "import sys\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "\n",
    "# import scrython\n",
    "# import asyncio\n",
    "# import aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect()\n",
    "\n",
    "cur=conn.cursor()\n",
    "\n",
    "cur.execute('''SELECT ebayname.cardname FROM \n",
    "((SELECT DISTINCT(cardname) FROM transactions) AS ebayname \n",
    "INNER JOIN \n",
    "(SELECT pio.cardname FROM ((SELECT DISTINCT(cardname) FROM tourninfo) AS pio \n",
    "INNER JOIN \n",
    "(SELECT DISTINCT(cardname) FROM moderntourninfo) AS mod ON pio.cardname=mod.cardname)) \n",
    "as overall ON ebayname.cardname=overall.cardname) ORDER BY ebayname.cardname ASC''')   \n",
    "    \n",
    "namerows=cur.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session=sagemaker.Session(boto3.session.Session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('''SELECT cardname,carddate,price FROM public.totalgoatbot WHERE cardtype=0 AND cardname IN(SELECT ebayname.cardname FROM \n",
    "((SELECT DISTINCT(cardname) FROM transactions) AS ebayname \n",
    "INNER JOIN \n",
    "(SELECT pio.cardname FROM ((SELECT DISTINCT(cardname) FROM tourninfo) AS pio \n",
    "INNER JOIN \n",
    "(SELECT DISTINCT(cardname) FROM moderntourninfo) AS mod ON pio.cardname=mod.cardname)) \n",
    "as overall ON ebayname.cardname=overall.cardname))''')\n",
    "goatrows=cur.fetchall()\n",
    "\n",
    "goatrows=pd.DataFrame(goatrows,columns=[\"cardname\",\"carddate\",\"price\"])\n",
    "goatrows['carddate'] = goatrows['carddate'].astype('datetime64[ns]') \n",
    "\n",
    "overalllist=[]\n",
    "for cardname in goatrows[\"cardname\"].unique():\n",
    "    coeflist=[]\n",
    "    wowee=goatrows['cardname'] == cardname\n",
    "    regvals=goatrows.loc[np.logical_and(wowee.values,goatrows['carddate']>'2019-10-01')]\n",
    "    regvals[\"carddate\"]=regvals[\"carddate\"]-regvals.iloc[0][\"carddate\"]\n",
    "    regvals[\"carddate\"]=regvals[\"carddate\"].astype('int64')/86400000000000\n",
    "    regprice=LinearRegression().fit(np.array(regvals.iloc[math.floor(len(regvals))-4:len(regvals)][\"carddate\"]).reshape(-1, 1),regvals.iloc[math.floor(len(regvals))-4:len(regvals)][\"price\"])\n",
    "    pricecoef=regprice.coef_[0]\n",
    "    coeflist.append(pricecoef)\n",
    "    coeflist.append(cardname)\n",
    "    overalllist.append(coeflist[::-1])\n",
    "    \n",
    "# coefdf=pd.DataFrame(overalllist,columns=[\"cardname\",\"mecoef1\",\"avgcoef1\",\"medcoef.5\",\"avgcoef.5\"])\n",
    "coefdf=pd.DataFrame(overalllist).sort_values(by=1,axis=0)\n",
    "coefdf.columns=[\"cardname\",\"slope\"]\n",
    "\n",
    "\n",
    "# Define function using cursor.executemany() to insert the dataframe\n",
    "def execute_many(conn, datafrm, table):\n",
    "    \n",
    "    # Creating a list of tupples from the dataframe values\n",
    "    tpls = [tuple(x) for x in datafrm.to_numpy()]\n",
    "    \n",
    "    # dataframe columns with Comma-separated\n",
    "    cols = ','.join(list(datafrm.columns))\n",
    "    \n",
    "    # SQL query to execute\n",
    "    sql = \"INSERT INTO %s(%s) VALUES(%%s,%%s)\" % (table, cols)\n",
    "    cursor = conn.cursor()\n",
    "    cur.execute('''DELETE FROM public.slopes''')\n",
    "    try:\n",
    "        cursor.executemany(sql, tpls)\n",
    "        conn.commit()\n",
    "        print(\"Data inserted using execute_many() successfully...\")\n",
    "    except (Exception, psycopg2.DatabaseError) as err:\n",
    "        # pass exception to function\n",
    "        show_psycopg2_exception(err)\n",
    "        cursor.close()\n",
    "        \n",
    "execute_many(conn,coefdf,'slopes')\n",
    "\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import IdentitySerializer\n",
    "class DeepARPredictor(sagemaker.predictor.Predictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, \n",
    "                         #serializer=JSONSerializer(),\n",
    "                         serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "                         **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + ts.index.freq\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "        \n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "        prediction_index = pd.date_range(start=prediction_time, freq=freq, periods=prediction_length)\n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-02-15 20:04:47 Starting - Preparing the instances for training\n",
      "2021-02-15 20:04:47 Downloading - Downloading input data\n",
      "2021-02-15 20:04:47 Training - Training image download completed. Training in progress.\n",
      "2021-02-15 20:04:47 Uploading - Uploading generated training model\n",
      "2021-02-15 20:04:47 Completed - Training job completed\n",
      "-------------!"
     ]
    }
   ],
   "source": [
    "training_job_name=\n",
    "\n",
    "attached_estimator = sagemaker.estimator.Estimator.attach(training_job_name, sagemaker_session=sagemaker_session)\n",
    "\n",
    "predictor=attached_estimator.deploy(    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    predictor_cls=DeepARPredictor)   \n",
    "##pass endpoint name into above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://172.17.0.2:8050/\n",
      "2019-10-21 00:00:00\n",
      "                 0.95        0.5      0.05\n",
      "2021-02-16  15.106190  10.978812  5.586014\n",
      "2021-02-17  17.324198  10.123502  5.198293\n",
      "2021-02-18  14.446716   9.906779  6.288001\n",
      "2021-02-19  16.390244   9.180943  3.648102\n",
      "2021-02-20  18.214828  11.130569  5.505061\n",
      "2021-02-21  16.226559  10.436534  6.122964\n",
      "2021-02-22  15.389554  11.762131  7.478086\n"
     ]
    }
   ],
   "source": [
    "from jupyter_dash import JupyterDash\n",
    "import json\n",
    "import plotly.express as px\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "PAGE_SIZE=15\n",
    "\n",
    "app = JupyterDash(__name__)\n",
    "\n",
    "\n",
    "app.layout = html.Div([\n",
    "    dcc.Location(id='url', refresh=False),\n",
    "    html.Div(id='page-content')\n",
    "])\n",
    "\n",
    "\n",
    "page_1_layout = html.Div([dcc.Dropdown(\n",
    "        id=\"worl\",\n",
    "        options=[{\"label\": x, \"value\": x} \n",
    "                 for x in ['Winners','Losers']],\n",
    "        value='Winners',\n",
    "        clearable=False,\n",
    "    ),\n",
    "    html.Div(dash_table.DataTable(\n",
    "        id='datatable-paging-page-count',\n",
    "        columns=[\n",
    "            {\"name\": i, \"id\": i} for i in sorted([\"cardname\",\"slope\"])\n",
    "        ],\n",
    "        page_current=0,\n",
    "        page_size=PAGE_SIZE,\n",
    "        page_action='custom'\n",
    "    )),\n",
    "    html.Div(id='page-1-content'),\n",
    "    html.Br(),\n",
    "    dcc.Link('Go to Page 2', href='/page-2'),\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output('datatable-paging-page-count', 'data'),\n",
    "    Input('datatable-paging-page-count', \"page_current\"),\n",
    "    Input('worl',\"value\"))\n",
    "\n",
    "def update_table(page_current,value,page_size=20):\n",
    "    conn = psycopg2.connect()\n",
    "\n",
    "    cur=conn.cursor()\n",
    "    if value=='Winners':\n",
    "        cur.execute('''SELECT MAX(id) FROM public.slopes''')\n",
    "        max_id=cur.fetchall()\n",
    "        max_id=max_id[0][0]\n",
    "        cur.execute('''SELECT cardname,slope FROM public.slopes WHERE id<=%s AND id>%s''',\n",
    "                    (max_id-page_current*page_size,max_id-(page_current+ 1)*page_size))\n",
    "        coeftups=cur.fetchall()\n",
    "        coefdf=pd.DataFrame(coeftups,columns=[\"cardname\",\"slope\"]).iloc[::-1].to_dict('records')\n",
    "    else:\n",
    "        cur.execute('''SELECT cardname,slope FROM public.slopes WHERE id>%s AND id<=%s''',\n",
    "                    (page_current*page_size,(page_current+ 1)*page_size))\n",
    "        coeftups=cur.fetchall()\n",
    "        coefdf=pd.DataFrame(coeftups,columns=[\"cardname\",\"slope\"]).to_dict('records')\n",
    "    \n",
    "    return coefdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "page_2_layout = html.Div([\n",
    "    html.Div(dcc.Dropdown(\n",
    "        id=\"graphstyle\",\n",
    "        options=[{\"label\": x, \"value\": x} \n",
    "                 for x in ['MINAVGMED','price0','modratio','pioratio','standratio']],\n",
    "        value='MINAVGMED',\n",
    "        clearable=False,\n",
    "    ))\n",
    "    ,html.Div(dcc.Dropdown(\n",
    "        id=\"ticker\",\n",
    "        options=[{\"label\": x[0], \"value\": x[0]} \n",
    "                 for x in namerows],\n",
    "        value='Thoughtseize',\n",
    "        clearable=False,\n",
    "    )),\n",
    "    dcc.Graph(id=\"time-series-chart\"),\n",
    "    html.Img(id=\"scryfallimg\"),\n",
    "    html.Div(id='page-2-content'),\n",
    "    html.Br(),\n",
    "    dcc.Link('Go to Page 1', href='/page-1'),\n",
    "])\n",
    "\n",
    "@app.callback(Output(\"scryfallimg\", 'src'),\n",
    "              Input(\"ticker\", \"value\"))\n",
    "def scryfall_img(ticker):\n",
    "    url=requests.get('https://api.scryfall.com/cards/search?q='+str(ticker))\n",
    "    if \"card_faces\" in url.json()['data'][0]:\n",
    "        return url.json()['data'][0]['card_faces'][0]['image_uris']['normal']\n",
    "    else:\n",
    "        return url.json()['data'][0]['image_uris']['normal']\n",
    "    \n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"time-series-chart\", \"figure\"), \n",
    "    Input(\"ticker\", \"value\"),Input(\"graphstyle\", \"value\"))\n",
    "def display_time_series(ticker,graphstyle):\n",
    "\n",
    "\n",
    "    conn = psycopg2.connect()\n",
    "\n",
    "    cur=conn.cursor()\n",
    "\n",
    "     ##updated by user\n",
    "    cat=0\n",
    "    freq='D'\n",
    "    prediction_length=7\n",
    "    context_length=7\n",
    "    \n",
    "    analyname=(ticker,)\n",
    "    \n",
    "    utc_now = pytz.utc.localize(datetime.datetime.utcnow())\n",
    "    pst_now = utc_now.astimezone(pytz.timezone(\"America/Los_Angeles\"))\n",
    "\n",
    "    start_date_othervarbs=pd.Timestamp(\"2019-10-16\", freq=freq)\n",
    "    start_date = pd.Timestamp(\"2019-10-23\", freq=freq)\n",
    "    end_dataset = pd.Timestamp(pst_now.date(), freq=freq)\n",
    "    end_training = end_dataset-timedelta(days=7)\n",
    "    mid_dataset=pd.Timestamp(\"2020-09-15\",freq=freq)\n",
    "    end_testing = end_dataset\n",
    "    \n",
    "    cur.execute('''SELECT MIN(carddate),cardtype FROM public.totalgoatbot WHERE cardname=%s GROUP BY cardtype''',analyname)\n",
    "\n",
    "    min_stuff=cur.fetchall()\n",
    "\n",
    "    min_df=pd.DataFrame(min_stuff,columns=['carddate','cardtype'])\n",
    "\n",
    "    min_carddate=min_df.min()['carddate']\n",
    "\n",
    "    min_cardtype=min_df[min_df['carddate']==min_carddate].min()['cardtype']\n",
    "\n",
    "    minanalyname=(ticker,min_cardtype)\n",
    "\n",
    "    cur.execute('''SELECT carddate,price,cardtype FROM public.totalgoatbot WHERE cardname ILIKE %s AND cardtype=%s''',minanalyname)\n",
    "\n",
    "    goatbotrows=cur.fetchall()\n",
    "\n",
    "    goatdf=pd.DataFrame(goatbotrows,columns=[\"carddate\",\"price\",\"cardtype\"]).set_index(\"carddate\")\n",
    "\n",
    "    goatdf.index=pd.to_datetime(goatdf.index,infer_datetime_format=True)\n",
    "\n",
    "    goatdfarr=[]\n",
    "    \n",
    "    goatdfarr.append(goatdf)\n",
    "    goatdfarr[0]=goatdfarr[0].drop(columns='cardtype')\n",
    "    goatdfarr[0].columns=['price'+str(0)]\n",
    "\n",
    "\n",
    "    cur.execute('''SELECT carddate,AVG(pshipq) as AVGE,percentile_cont(0.5) WITHIN GROUP (ORDER BY pshipq) as MED\n",
    "        FROM (SELECT DISTINCT title,cardname,carddate,(price+shipping)/COALESCE(availablequant,cardquantity,maybecardquantity,1) as pshipq FROM public.transactions\n",
    "        WHERE possiblybad IS FALSE\n",
    "        AND ismisprint IS NULL\n",
    "        AND cardname=%s\n",
    "        AND isfoil IS FALSE\n",
    "        AND (cardlanguage='english' or cardlanguage IS NULL)\n",
    "        AND (cardset IS NULL OR cardset NOT IN('LEA','LEB','U'))\n",
    "        AND (isemblem IS NULL OR isemblem IS FALSE)\n",
    "        AND (lotis IS FALSE OR lotis IS NULL)\n",
    "        AND (isboosterbox IS NULL OR isboosterbox IS FALSE)\n",
    "        AND cardspecial IS NULL\n",
    "        AND issleeve IS NULL\n",
    "        AND isdeck IS NULL\n",
    "        AND isplaymat IS NULL\n",
    "        AND isultra IS NULL\n",
    "        AND (istoken IS NULL OR istoken IS FALSE)\n",
    "        AND (iscommanderdeck IS NULL OR iscommanderdeck IS FALSE)\n",
    "        AND isgraded IS NULL\n",
    "        AND (othercards IS NULL OR transplit IS TRUE)\n",
    "        AND saletype IN('normal')\n",
    "        AND price+shipping<100) as foo WHERE pshipq<50 \n",
    "        GROUP BY carddate ORDER BY carddate ASC''',analyname)\n",
    "\n",
    "    ebayrows=cur.fetchall()\n",
    "\n",
    "    #ebaydf=pd.DataFrame(ebayrows,columns=[\"carddate\",\"avg\",\"med\"]) \n",
    "    ebaydf=pd.DataFrame(ebayrows,columns=[\"carddate\",\"AVG\",\"MED\"]).set_index(\"carddate\")\n",
    "    ebaydf['MINAVGMED']=ebaydf.apply(lambda row: np.minimum(row.AVG,row.MED), axis = 1)\n",
    "    ebaydf=ebaydf.drop(columns=['AVG','MED'])\n",
    "    ebaydf.index=pd.to_datetime(ebaydf.index,infer_datetime_format=True)\n",
    "\n",
    "    cur.execute('''SELECT entrydate1 as entrydate,SUM(cardquant)/AVG(aggcount) as ratio FROM(\n",
    "    SELECT DISTINCT cardname0,entrydate1,cardquant,aggcount,tourntitle FROM\n",
    "    (SELECT cardname as cardname0,entrydate as entrydate0,cardquant,tourntitle FROM public.tourninfo WHERE cardname ILIKE %s) AS foo1,\n",
    "    (SELECT entrydate as entrydate1,SUM(cardquant) as aggcount FROM public.tourninfo GROUP BY entrydate) AS bar\n",
    "    WHERE entrydate0=entrydate1) AS foo\n",
    "    GROUP BY cardname0,entrydate1''',analyname)\n",
    "\n",
    "    piotournrows=cur.fetchall()\n",
    "\n",
    "    piotourndf=pd.DataFrame(piotournrows,columns=[\"carddate\",\"pioratio\"]).set_index(\"carddate\")\n",
    "    piotourndf=piotourndf.astype({'pioratio': 'float64'})\n",
    "\n",
    "    piotourndf.index=pd.to_datetime(piotourndf.index,infer_datetime_format=True)\n",
    "\n",
    "    cur.execute('''SELECT entrydate1 as entrydate,SUM(cardquant)/AVG(aggcount) as ratio FROM(\n",
    "    SELECT DISTINCT cardname0,entrydate1,cardquant,aggcount,tourntitle FROM\n",
    "    (SELECT cardname as cardname0,entrydate as entrydate0,cardquant,tourntitle FROM public.moderntourninfo WHERE cardname ILIKE %s) AS foo1,\n",
    "    (SELECT entrydate as entrydate1,SUM(cardquant) as aggcount FROM public.moderntourninfo GROUP BY entrydate) AS bar\n",
    "    WHERE entrydate0=entrydate1) AS foo\n",
    "    GROUP BY cardname0,entrydate1''',analyname)\n",
    "\n",
    "    moderntournrows=cur.fetchall()\n",
    "\n",
    "    moderntourndf=pd.DataFrame(moderntournrows,columns=[\"carddate\",\"modratio\"]).set_index(\"carddate\")\n",
    "    moderntourndf=moderntourndf.astype({'modratio': 'float64'})\n",
    "\n",
    "    moderntourndf.index=pd.to_datetime(moderntourndf.index,infer_datetime_format=True)\n",
    "\n",
    "    missingdatefix=pd.date_range(start='2019-10-15',end=end_dataset)\n",
    "\n",
    "    moderntourndf=moderntourndf.reindex(missingdatefix).fillna(0)\n",
    "\n",
    "    moderntourndf.index.name='carddate'\n",
    "\n",
    "\n",
    "    cur.execute('''SELECT entrydate1 as entrydate,SUM(cardquant)/AVG(aggcount) as ratio FROM(\n",
    "    SELECT DISTINCT cardname0,entrydate1,cardquant,aggcount,tourntitle FROM\n",
    "    (SELECT cardname as cardname0,entrydate as entrydate0,cardquant,tourntitle FROM public.standardtourninfo WHERE cardname ILIKE %s) AS foo1,\n",
    "    (SELECT entrydate as entrydate1,SUM(cardquant) as aggcount FROM public.standardtourninfo GROUP BY entrydate) AS bar\n",
    "    WHERE entrydate0=entrydate1) AS foo\n",
    "    GROUP BY cardname0,entrydate1''',analyname)\n",
    "\n",
    "    standardtournrows=cur.fetchall()\n",
    "\n",
    "    if len(standardtournrows)==0:\n",
    "        standardtourndf=pd.DataFrame([['2020-04-03',0],['2020-04-04',0]],\n",
    "                                     columns=[\"carddate\",\"standratio\"]).set_index(\"carddate\")\n",
    "    else:\n",
    "        standardtourndf=pd.DataFrame(standardtournrows,columns=[\"carddate\",\"standratio\"]).set_index(\"carddate\")\n",
    "\n",
    "    standardtourndf=standardtourndf.astype({'standratio': 'float64'})\n",
    "    standardtourndf.index=pd.to_datetime(standardtourndf.index,infer_datetime_format=True)\n",
    "    print(ebaydf.index.min())\n",
    "    goatdfarr.insert(0,ebaydf)\n",
    "    goatdfarr.append(piotourndf)\n",
    "    goatdfarr.append(moderntourndf)\n",
    "    goatdfarr.append(standardtourndf)\n",
    "    overalldf=pd.concat(goatdfarr,axis=1,sort=True)\n",
    "\n",
    "\n",
    "    overalldf=pd.concat([overalldf['MINAVGMED'].fillna(\"NaN\"),\n",
    "                         overalldf.loc[:, overalldf.columns.difference(['MINAVGMED'])]\n",
    "                         .fillna(method='ffill').fillna(method='bfill')],\n",
    "                        axis=1,join='inner')\n",
    "\n",
    "\n",
    "\n",
    "    dynamic_feat_arr_series=[]\n",
    "    \n",
    "    for col in overalldf.columns[1:]:\n",
    "        dynamic_feat_arr_series.append(list(overalldf[start_date-timedelta(days=prediction_length):end_dataset][col]))\n",
    "    if graphstyle=='MINAVGMED':        \n",
    "        args = {\n",
    "            \"ts\": overalldf[start_date:end_dataset]['MINAVGMED'].astype('float64').asfreq(freq),\n",
    "            \"return_samples\": False,\n",
    "            \"dynamic_feat\":dynamic_feat_arr_series,\n",
    "            \"quantiles\": [0.05, 0.5, 0.95],\n",
    "            \"num_samples\": 100,\n",
    "            \"cat\":namerows.index(analyname)\n",
    "        }\n",
    "\n",
    "        sage_preds=predictor.predict(**args)\n",
    "        print(sage_preds)\n",
    "        \n",
    "        overalldf=overalldf[overalldf.index>=ebaydf.index.min()]\n",
    "\n",
    "        fig = px.line(overalldf, y=graphstyle)\n",
    "        \n",
    "        fig.add_trace(go.Scatter(x=sage_preds.index,y=sage_preds['0.5'].values,\n",
    "                                connectgaps=True, name='<b>Predictions</b>'))\n",
    "        fig.add_trace(go.Scatter(x=sage_preds.index,y=sage_preds['0.05'].values,\n",
    "                                connectgaps=True, name='<b>Predictions Lower</b>'))\n",
    "        fig.add_trace(go.Scatter(x=sage_preds.index,y=sage_preds['0.95'].values,\n",
    "                                connectgaps=True, name='<b>Predictions Upper</b>'))\n",
    "        \n",
    "    else:\n",
    "        overalldf=overalldf[overalldf.index>=ebaydf.index.min()]\n",
    "\n",
    "        fig = px.line(overalldf, y=graphstyle)\n",
    "        \n",
    "    return fig\n",
    "\n",
    "\n",
    "@app.callback(dash.dependencies.Output('page-content', 'children'),\n",
    "              [dash.dependencies.Input('url', 'pathname')])\n",
    "\n",
    "def display_page(pathname):\n",
    "    if pathname == '/page-2':\n",
    "        return page_2_layout\n",
    "    else:\n",
    "        return page_1_layout\n",
    "\n",
    "    \n",
    "\n",
    "app.run_server(mode=\"external\",host='172.17.0.2',port='8050',debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
