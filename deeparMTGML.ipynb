{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==0.23.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.23.4)\n",
      "Requirement already satisfied: pytz>=2011k in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas==0.23.4) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas==0.23.4) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas==0.23.4) (1.18.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas==0.23.4) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas==0.23.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "\n",
    "import psycopg2\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import pytz\n",
    "import math as math\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = 'mtgmlbucket'  # replace with an existing bucket if needed\n",
    "s3_prefix = 'mtgml-notebook'    # prefix used for all data stored within the bucket\n",
    "\n",
    "role = sagemaker.get_execution_role()             # IAM role to use by SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur=conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyname='Supreme Verdict'\n",
    "\n",
    "cur.execute('''SELECT carddate,price,cardtype FROM public.totalgoatbot WHERE \n",
    "cardname IN(SELECT DISTINCT(cardname) FROM public.tourninfo) AND cardname ILIKE %s''',(analyname,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "goatbotrows=cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "goatdf=pd.DataFrame(goatbotrows,columns=[\"carddate\",\"price\",\"cardtype\"]).set_index(\"carddate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            price  cardtype\n",
      "carddate                   \n",
      "2020-01-03  5.20   0       \n",
      "2020-05-02  4.84   0       \n",
      "2020-05-01  4.82   0       \n",
      "2020-04-30  4.87   0       \n",
      "2020-04-29  4.73   0       \n",
      "2020-04-28  4.64   0       \n",
      "2020-04-27  4.70   0       \n",
      "2020-04-26  4.61   0       \n",
      "2020-04-25  4.36   0       \n",
      "2020-04-24  4.02   0       \n",
      "2020-04-23  3.90   0       \n",
      "2020-04-22  3.88   0       \n",
      "2020-04-21  3.67   0       \n",
      "2020-04-20  3.83   0       \n",
      "2020-04-19  3.83   0       \n",
      "2020-04-18  3.92   0       \n",
      "2020-04-17  3.80   0       \n",
      "2020-04-16  3.75   0       \n",
      "2020-04-15  3.60   0       \n",
      "2020-04-14  3.66   0       \n",
      "2020-04-13  3.68   0       \n",
      "2020-04-12  3.85   0       \n",
      "2020-04-11  4.53   0       \n",
      "2020-04-10  4.91   0       \n",
      "2020-04-09  4.77   0       \n",
      "2020-04-08  4.85   0       \n",
      "2020-04-07  5.49   0       \n",
      "2020-04-06  4.79   0       \n",
      "2020-04-05  4.29   0       \n",
      "2020-04-04  4.53   0       \n",
      "2020-04-03  4.52   0       \n",
      "2020-04-02  4.42   0       \n",
      "2020-04-01  4.63   0       \n",
      "2020-03-31  4.70   0       \n",
      "2020-03-30  5.32   0       \n",
      "2020-03-29  5.07   0       \n",
      "2020-03-28  4.51   0       \n",
      "2020-03-27  3.73   0       \n",
      "2020-03-26  3.54   0       \n",
      "2020-03-25  3.93   0       \n",
      "2020-03-24  3.96   0       \n",
      "2020-03-23  4.22   0       \n",
      "2020-03-22  4.28   0       \n",
      "2020-03-21  4.53   0       \n",
      "2020-03-20  4.50   0       \n",
      "2020-03-19  5.12   0       \n",
      "2020-03-18  5.06   0       \n",
      "2020-03-17  5.43   0       \n",
      "2020-03-16  5.75   0       \n",
      "2020-03-15  6.46   0       \n",
      "2020-03-14  6.67   0       \n",
      "2020-03-13  6.47   0       \n",
      "2020-03-12  6.37   0       \n",
      "2020-03-11  5.71   0       \n",
      "2020-03-10  4.76   0       \n",
      "2020-03-09  4.61   0       \n",
      "2020-03-08  4.92   0       \n",
      "2020-03-07  4.50   0       \n",
      "2020-03-06  4.17   0       \n",
      "2020-03-05  4.34   0       \n",
      "2020-03-04  4.51   0       \n",
      "2020-03-03  4.88   0       \n",
      "2020-03-02  4.30   0       \n",
      "2020-03-01  5.20   0       \n",
      "2020-02-29  5.39   0       \n",
      "2020-02-28  4.99   0       \n",
      "2020-02-27  4.87   0       \n",
      "2020-02-26  4.75   0       \n",
      "2020-02-25  4.31   0       \n",
      "2020-02-24  4.35   0       \n",
      "2020-02-23  4.27   0       \n",
      "2020-02-22  4.23   0       \n",
      "2020-02-21  4.37   0       \n",
      "2020-02-20  4.94   0       \n",
      "2020-02-19  5.07   0       \n",
      "2020-02-18  5.16   0       \n",
      "2020-02-17  5.18   0       \n",
      "2020-02-16  5.20   0       \n",
      "2020-02-15  5.26   0       \n",
      "2020-02-14  6.07   0       \n",
      "2020-02-13  6.41   0       \n",
      "2020-02-12  6.44   0       \n",
      "2020-02-11  6.29   0       \n",
      "2020-02-10  6.44   0       \n",
      "2020-02-09  6.90   0       \n",
      "2020-02-08  7.33   0       \n",
      "2020-02-07  7.08   0       \n",
      "2020-02-06  7.63   0       \n",
      "2020-02-05  7.25   0       \n",
      "2020-02-04  6.45   0       \n",
      "2020-02-03  6.36   0       \n",
      "2020-02-02  6.84   0       \n",
      "2020-02-01  6.15   0       \n",
      "2020-01-31  6.77   0       \n",
      "2020-01-30  7.34   0       \n",
      "2020-01-29  7.88   0       \n",
      "2020-01-28  8.30   0       \n",
      "2020-01-27  9.39   0       \n",
      "2020-01-26  10.63  0       \n",
      "2020-01-25  10.42  0       \n",
      "2020-01-24  9.95   0       \n",
      "2020-01-23  8.64   0       \n",
      "2020-01-22  8.20   0       \n",
      "2020-01-21  8.20   0       \n",
      "2020-01-20  7.03   0       \n",
      "2020-01-19  6.74   0       \n",
      "2020-01-18  7.50   0       \n",
      "2020-01-17  7.97   0       \n",
      "2020-01-16  7.79   0       \n",
      "2020-01-15  8.48   0       \n",
      "2020-01-14  8.53   0       \n",
      "2020-01-13  9.20   0       \n",
      "2020-01-12  9.64   0       \n",
      "2020-01-11  9.61   0       \n",
      "2020-01-10  10.45  0       \n",
      "2020-01-09  9.28   0       \n",
      "2020-01-08  7.43   0       \n",
      "2020-01-07  6.02   0       \n",
      "2020-01-06  5.69   0       \n",
      "2020-01-05  5.58   0       \n",
      "2020-01-04  5.62   0       \n",
      "2019-09-05  1.43   0       \n",
      "2019-09-04  1.27   0       \n",
      "2019-09-03  1.32   0       \n",
      "2019-09-02  1.57   0       \n",
      "2019-09-01  1.66   0       \n",
      "2019-08-31  1.72   0       \n",
      "2019-08-30  1.73   0       \n",
      "2019-08-29  1.59   0       \n",
      "2019-08-28  1.45   0       \n",
      "2019-08-27  1.22   0       \n",
      "2019-08-26  1.16   0       \n",
      "2019-08-25  1.19   0       \n",
      "2019-08-24  1.31   0       \n",
      "2019-08-23  1.48   0       \n",
      "2019-08-22  1.39   0       \n",
      "2019-08-21  1.44   0       \n",
      "2019-08-20  1.50   0       \n",
      "2019-08-19  1.48   0       \n",
      "2019-08-18  1.51   0       \n",
      "2019-08-17  1.71   0       \n",
      "2019-08-16  1.71   0       \n",
      "2019-08-15  1.46   0       \n",
      "2019-08-14  1.32   0       \n",
      "2019-08-13  1.34   0       \n",
      "2019-08-12  1.34   0       \n",
      "2019-08-11  1.14   0       \n",
      "2019-08-10  1.10   0       \n",
      "2019-08-09  0.93   0       \n",
      "2019-08-08  0.78   0       \n",
      "2019-08-07  0.84   0       \n",
      "2019-08-06  0.94   0       \n",
      "2019-08-05  0.94   0       \n",
      "2019-08-04  1.06   0       \n",
      "2019-08-03  0.94   0       \n",
      "2019-08-02  0.96   0       \n",
      "2019-08-01  1.01   0       \n",
      "2019-07-31  1.01   0       \n",
      "2019-07-30  1.25   0       \n",
      "2019-07-29  1.19   0       \n",
      "2019-07-28  0.94   0       \n",
      "2019-07-27  0.94   0       \n",
      "2019-07-26  0.98   0       \n",
      "2019-07-25  0.91   0       \n",
      "2019-07-24  0.92   0       \n",
      "2019-07-23  0.91   0       \n",
      "2019-07-22  0.98   0       \n",
      "2019-07-21  1.05   0       \n",
      "2019-07-20  1.02   0       \n",
      "2019-07-19  1.16   0       \n",
      "2019-07-18  1.30   0       \n",
      "2019-07-17  1.39   0       \n",
      "2019-07-16  1.30   0       \n",
      "2019-07-15  1.33   0       \n",
      "2019-07-14  1.07   0       \n",
      "2019-07-13  1.16   0       \n",
      "2019-07-12  1.19   0       \n",
      "2019-07-11  1.23   0       \n",
      "2019-07-10  1.11   0       \n",
      "2019-07-09  1.26   0       \n",
      "2019-07-08  1.40   0       \n",
      "2019-07-07  1.49   0       \n",
      "2019-07-06  1.58   0       \n",
      "2019-07-05  1.65   0       \n",
      "2019-07-04  1.61   0       \n",
      "2019-07-03  1.70   0       \n",
      "2019-07-02  1.55   0       \n",
      "2019-07-01  1.46   0       \n",
      "2019-06-30  1.32   0       \n",
      "2019-06-29  1.12   0       \n",
      "2019-06-28  1.19   0       \n",
      "2019-06-27  1.17   0       \n",
      "2019-06-26  1.15   0       \n",
      "2019-06-25  1.29   0       \n",
      "2019-06-24  1.38   0       \n",
      "2019-06-23  1.65   0       \n",
      "2019-06-22  1.64   0       \n",
      "2019-06-21  1.47   0       \n",
      "2019-06-20  1.55   0       \n",
      "2019-06-19  1.46   0       \n",
      "2019-06-18  1.44   0       \n",
      "2019-06-17  1.41   0       \n",
      "2019-06-16  1.34   0       \n",
      "2019-06-15  1.26   0       \n",
      "2019-06-14  1.45   0       \n",
      "2019-06-13  1.51   0       \n",
      "2019-06-12  1.52   0       \n",
      "2019-06-11  1.39   0       \n",
      "2019-06-10  1.31   0       \n",
      "2019-06-09  1.55   0       \n",
      "2019-06-08  1.63   0       \n",
      "2019-06-07  1.65   0       \n",
      "2019-06-06  1.64   0       \n",
      "2019-06-05  1.62   0       \n",
      "2019-06-04  1.79   0       \n",
      "2019-06-03  1.63   0       \n",
      "2019-06-02  1.38   0       \n",
      "2019-06-01  1.63   0       \n",
      "2019-05-31  1.56   0       \n",
      "2019-05-30  1.46   0       \n",
      "2019-05-29  1.45   0       \n",
      "2019-05-28  1.42   0       \n",
      "2019-05-27  1.50   0       \n",
      "2019-05-26  1.31   0       \n",
      "2019-05-25  1.20   0       \n",
      "2019-05-24  1.17   0       \n",
      "2019-05-23  1.21   0       \n",
      "2019-05-22  1.18   0       \n",
      "2019-05-21  1.08   0       \n",
      "2019-05-20  1.04   0       \n",
      "2019-05-19  1.12   0       \n",
      "2019-05-18  1.03   0       \n",
      "2019-05-17  1.14   0       \n",
      "2019-05-16  1.31   0       \n",
      "2019-05-15  1.31   0       \n",
      "2019-05-14  1.63   0       \n",
      "2019-05-13  1.48   0       \n",
      "2019-05-12  1.20   0       \n",
      "2019-05-11  1.15   0       \n",
      "2019-05-10  1.13   0       \n",
      "2019-05-09  1.21   0       \n",
      "2019-05-08  1.28   0       \n",
      "2019-05-07  1.28   0       \n",
      "2019-05-06  1.46   0       \n",
      "2019-05-05  1.35   0       \n",
      "2019-05-04  1.19   0       \n",
      "2019-05-03  1.16   0       \n",
      "2019-05-02  1.08   0       \n",
      "2019-05-01  0.93   0       \n",
      "2019-04-30  0.74   0       \n",
      "2019-04-29  0.66   0       \n",
      "2019-04-28  0.59   0       \n",
      "2019-04-27  0.58   0       \n",
      "2019-04-26  0.40   0       \n",
      "2019-04-25  0.51   0       \n",
      "2019-04-24  0.60   0       \n",
      "2019-04-23  0.73   0       \n",
      "2019-04-22  0.96   0       \n",
      "2019-04-21  1.12   0       \n",
      "2019-04-20  1.04   0       \n",
      "2019-04-19  1.13   0       \n",
      "2019-04-18  1.17   0       \n",
      "2019-04-17  1.23   0       \n",
      "2019-04-16  1.21   0       \n",
      "2019-04-15  1.24   0       \n",
      "2019-04-14  1.12   0       \n",
      "2019-04-13  1.13   0       \n",
      "2019-04-12  1.02   0       \n",
      "2019-04-11  1.19   0       \n",
      "2019-04-10  1.08   0       \n",
      "2019-04-09  1.19   0       \n",
      "2019-04-08  1.06   0       \n",
      "2019-04-07  0.78   0       \n",
      "2019-04-06  0.77   0       \n",
      "2019-04-05  0.77   0       \n",
      "2019-04-04  0.71   0       \n",
      "2019-04-03  0.67   0       \n",
      "2019-04-02  0.71   0       \n",
      "2019-04-01  0.72   0       \n",
      "2019-03-31  0.74   0       \n",
      "2019-03-30  0.74   0       \n",
      "2019-03-29  0.79   0       \n",
      "2019-03-28  0.79   0       \n",
      "2019-03-27  0.77   0       \n",
      "2019-03-26  0.75   0       \n",
      "2019-03-25  0.75   0       \n",
      "2019-03-24  0.79   0       \n",
      "2019-03-23  0.55   0       \n",
      "2019-03-22  0.56   0       \n",
      "2019-03-21  0.58   0       \n",
      "2019-03-20  0.65   0       \n",
      "2019-03-19  0.66   0       \n",
      "2019-03-18  0.70   0       \n",
      "2019-03-17  0.70   0       \n",
      "2019-03-16  0.65   0       \n",
      "2019-03-15  0.64   0       \n",
      "2019-03-14  0.67   0       \n",
      "2019-03-13  0.64   0       \n",
      "2019-03-12  0.71   0       \n",
      "2019-03-11  0.68   0       \n",
      "2019-03-10  0.67   0       \n",
      "2019-03-09  0.55   0       \n",
      "2019-03-08  0.38   0       \n",
      "2019-03-07  0.43   0       \n",
      "2019-03-06  0.46   0       \n",
      "2019-03-05  0.39   0       \n",
      "2019-03-04  0.42   0       \n",
      "2019-03-03  0.45   0       \n",
      "2019-03-02  0.39   0       \n",
      "2019-03-01  0.53   0       \n",
      "2019-02-28  0.60   0       \n",
      "2019-02-27  0.45   0       \n",
      "2019-02-26  0.35   0       \n",
      "2019-02-25  0.34   0       \n",
      "2019-02-24  0.26   0       \n",
      "2019-02-23  0.23   0       \n",
      "2019-02-22  0.26   0       \n",
      "2019-02-21  0.26   0       \n",
      "2019-02-20  0.29   0       \n",
      "2019-02-19  0.30   0       \n",
      "2019-02-18  0.33   0       \n",
      "2019-02-17  0.24   0       \n",
      "2019-02-16  0.25   0       \n",
      "2019-02-15  0.19   0       \n",
      "2019-02-14  0.19   0       \n",
      "2019-02-13  0.23   0       \n",
      "2019-02-12  0.22   0       \n",
      "2019-02-11  0.29   0       \n",
      "2019-02-10  0.22   0       \n",
      "2019-02-09  0.20   0       \n",
      "2019-02-08  0.20   0       \n",
      "2019-02-07  0.22   0       \n",
      "2019-02-06  0.28   0       \n",
      "2019-02-05  0.24   0       \n",
      "2019-02-04  0.18   0       \n",
      "2019-02-03  0.18   0       \n",
      "2019-02-02  0.16   0       \n",
      "2019-02-01  0.21   0       \n",
      "2019-01-31  0.23   0       \n",
      "2019-01-30  0.22   0       \n",
      "2019-01-29  0.25   0       \n",
      "2019-01-28  0.26   0       \n",
      "2019-01-27  0.26   0       \n",
      "2019-01-26  0.22   0       \n",
      "2019-01-25  0.27   0       \n",
      "2019-01-24  0.23   0       \n",
      "2019-01-23  0.26   0       \n",
      "2019-01-22  0.38   0       \n",
      "2019-01-21  0.47   0       \n",
      "2019-01-20  0.40   0       \n",
      "2019-01-19  0.36   0       \n",
      "2019-01-18  0.42   0       \n",
      "2019-01-17  0.46   0       \n",
      "2019-01-16  0.60   0       \n",
      "2019-01-15  0.62   0       \n",
      "2019-01-14  0.65   0       \n",
      "2019-01-13  0.64   0       \n",
      "2019-01-12  0.62   0       \n",
      "2019-01-11  0.58   0       \n",
      "2019-01-10  0.60   0       \n",
      "2019-01-09  0.61   0       \n",
      "2019-01-08  0.50   0       \n",
      "2019-01-07  0.51   0       \n",
      "2019-01-06  0.48   0       \n",
      "2019-01-05  0.50   0       \n",
      "2019-01-04  0.58   0       \n",
      "2019-01-03  0.68   0       \n",
      "2019-01-02  0.69   0       \n",
      "2019-01-01  0.74   0       \n",
      "2018-12-31  0.84   0       \n",
      "2018-12-30  0.77   0       \n",
      "2018-12-29  0.70   0       \n",
      "2018-12-28  0.62   0       \n",
      "2018-12-27  0.53   0       \n",
      "2018-12-26  0.56   0       \n",
      "2018-12-25  0.56   0       \n",
      "2018-12-24  0.48   0       \n",
      "2018-12-23  0.40   0       \n",
      "2018-12-22  0.43   0       \n",
      "2018-12-21  0.41   0       \n",
      "2018-12-20  0.43   0       \n",
      "2018-12-19  0.54   0       \n",
      "2018-12-18  0.51   0       \n",
      "2018-12-17  0.51   0       \n",
      "2018-12-16  0.40   0       \n",
      "2018-12-15  0.30   0       \n",
      "2018-12-14  0.22   0       \n",
      "2018-12-13  0.23   0       \n",
      "2018-12-12  0.24   0       \n",
      "2018-12-11  0.23   0       \n",
      "2018-12-10  0.27   0       \n",
      "2018-12-09  0.22   0       \n",
      "2018-12-08  0.14   0       \n",
      "2018-12-07  0.15   0       \n",
      "2018-12-06  0.23   0       \n",
      "2018-12-05  0.15   0       \n",
      "2018-12-04  0.56   0       \n",
      "2018-12-03  0.52   0       \n",
      "2018-12-02  0.65   0       \n",
      "2018-12-01  0.76   0       \n",
      "2018-11-30  0.66   0       \n",
      "2018-11-29  0.58   0       \n",
      "2018-11-28  0.56   0       \n",
      "2018-11-27  0.54   0       \n",
      "2018-11-26  0.54   0       \n",
      "2018-11-25  0.47   0       \n",
      "2018-11-24  0.38   0       \n",
      "2018-11-23  0.51   0       \n",
      "2018-11-22  0.53   0       \n",
      "2018-11-21  0.60   0       \n",
      "2018-11-20  0.48   0       \n",
      "2018-11-19  0.38   0       \n",
      "2018-11-18  0.43   0       \n",
      "2018-11-17  0.44   0       \n",
      "2018-11-16  0.42   0       \n",
      "2018-11-15  0.51   0       \n",
      "2018-11-14  0.44   0       \n",
      "2018-11-13  0.44   0       \n",
      "2018-11-12  0.35   0       \n",
      "2018-11-11  0.55   0       \n",
      "2018-11-10  0.60   0       \n",
      "2018-11-09  0.61   0       \n",
      "2018-11-08  0.61   0       \n",
      "2018-11-07  0.67   0       \n",
      "2018-11-06  0.67   0       \n",
      "2018-11-05  0.70   0       \n",
      "2018-11-04  0.69   0       \n",
      "2018-11-03  0.68   0       \n",
      "2018-11-02  0.76   0       \n",
      "2018-11-01  0.70   0       \n",
      "2018-10-31  0.61   0       \n",
      "2018-10-30  0.80   0       \n",
      "2018-10-29  0.77   0       \n",
      "2018-10-28  0.67   0       \n",
      "2018-10-27  0.72   0       \n",
      "2018-10-26  0.76   0       \n",
      "2018-10-25  0.78   0       \n",
      "2018-10-24  0.85   0       \n",
      "2018-10-23  0.86   0       \n",
      "2018-10-22  0.83   0       \n",
      "2018-10-21  0.72   0       \n",
      "2018-10-20  0.72   0       \n",
      "2018-10-19  0.63   0       \n",
      "2018-10-18  0.58   0       \n",
      "2018-10-17  0.80   0       \n",
      "2018-10-16  0.83   0       \n",
      "2018-10-15  0.81   0       \n",
      "2018-10-14  0.83   0       \n",
      "2018-10-13  0.75   0       \n",
      "2018-10-12  0.75   0       \n",
      "2018-10-11  0.77   0       \n",
      "2018-10-10  0.77   0       \n",
      "2018-10-09  0.78   0       \n",
      "2018-10-08  0.95   0       \n",
      "2018-10-07  0.89   0       \n",
      "2018-10-06  0.97   0       \n",
      "2018-10-05  1.06   0       \n",
      "2018-10-04  1.14   0       \n",
      "2018-10-03  1.15   0       \n",
      "2018-10-02  1.08   0       \n",
      "2018-10-01  1.19   0       \n",
      "2018-09-30  1.09   0       \n",
      "2018-09-29  0.82   0       \n",
      "2018-09-28  1.04   0       \n",
      "2018-09-27  1.15   0       \n",
      "2018-09-26  1.39   0       \n",
      "2018-09-25  1.41   0       \n",
      "2018-09-24  1.41   0       \n",
      "2018-09-23  1.52   0       \n",
      "2018-09-22  1.65   0       \n",
      "2018-09-21  1.62   0       \n",
      "2018-09-20  1.70   0       \n",
      "2018-09-19  1.80   0       \n",
      "2018-09-18  1.88   0       \n",
      "2018-09-17  1.87   0       \n",
      "2018-09-16  1.83   0       \n",
      "2018-09-15  1.80   0       \n",
      "2018-09-14  1.72   0       \n",
      "2018-09-13  1.81   0       \n",
      "2018-09-12  2.19   0       \n",
      "2018-09-11  2.36   0       \n",
      "2018-09-10  2.51   0       \n",
      "2018-09-09  2.23   0       \n",
      "2018-09-08  1.97   0       \n",
      "2018-09-07  1.95   0       \n",
      "2018-09-06  2.04   0       \n",
      "2018-09-05  2.01   0       \n",
      "2018-09-04  1.91   0       \n",
      "2018-09-03  1.75   0       \n",
      "2018-09-02  1.53   0       \n",
      "2018-09-01  1.47   0       \n",
      "2018-08-31  1.36   0       \n",
      "2018-08-30  1.24   0       \n",
      "2018-08-29  1.24   0       \n",
      "2018-08-28  1.37   0       \n",
      "2018-08-27  1.50   0       \n",
      "2018-08-26  1.62   0       \n",
      "2018-08-25  1.68   0       \n",
      "2018-08-24  1.55   0       \n",
      "2018-08-23  1.56   0       \n",
      "2018-08-22  1.43   0       \n",
      "2018-08-21  1.36   0       \n",
      "2018-08-20  1.58   0       \n",
      "2018-08-19  1.51   0       \n",
      "2018-08-18  1.41   0       \n",
      "2018-08-17  1.42   0       \n",
      "2018-08-16  1.55   0       \n",
      "2018-08-15  1.51   0       \n",
      "2018-08-14  1.61   0       \n",
      "2018-08-13  1.50   0       \n",
      "2018-08-12  1.48   0       \n",
      "2018-08-11  1.57   0       \n",
      "2018-08-10  1.41   0       \n",
      "2018-08-09  1.38   0       \n",
      "2018-08-08  1.44   0       \n",
      "2018-08-07  1.41   0       \n",
      "2018-08-06  1.30   0       \n",
      "2018-08-05  1.39   0       \n",
      "2018-08-04  1.30   0       \n",
      "2018-08-03  1.14   0       \n",
      "2018-08-02  1.26   0       \n",
      "2018-08-01  1.40   0       \n",
      "2018-07-31  1.57   0       \n",
      "2018-07-30  1.66   0       \n",
      "2018-07-29  1.65   0       \n",
      "2018-07-28  1.75   0       \n",
      "2018-07-27  1.75   0       \n",
      "2018-07-26  1.84   0       \n",
      "2018-07-25  1.71   0       \n",
      "2018-07-24  1.87   0       \n",
      "2018-07-23  1.68   0       \n",
      "2018-07-22  1.53   0       \n",
      "2018-07-21  1.19   0       \n",
      "2018-07-20  1.05   0       \n",
      "2018-07-19  1.06   0       \n",
      "2018-07-18  1.16   0       \n",
      "2018-07-17  1.22   0       \n",
      "2018-07-16  1.23   0       \n",
      "2018-07-15  1.20   0       \n",
      "2018-07-14  0.97   0       \n",
      "2018-07-13  1.19   0       \n",
      "2018-07-12  1.48   0       \n",
      "2018-07-11  1.44   0       \n",
      "2018-07-10  1.52   0       \n",
      "2018-07-09  1.89   0       \n",
      "2018-07-08  1.80   0       \n",
      "2018-07-07  1.79   0       \n",
      "2018-07-06  1.76   0       \n",
      "2018-07-05  1.63   0       \n",
      "2018-07-04  1.70   0       \n",
      "2018-07-03  1.59   0       \n",
      "2018-07-02  1.43   0       \n",
      "2018-07-01  1.42   0       \n",
      "2018-06-30  1.38   0       \n",
      "2018-06-29  1.35   0       \n",
      "2018-06-28  1.33   0       \n",
      "2018-06-27  1.30   0       \n",
      "2018-06-26  1.18   0       \n",
      "2018-06-25  1.46   0       \n",
      "2018-06-24  1.37   0       \n",
      "2018-06-23  1.57   0       \n",
      "2018-06-22  1.65   0       \n",
      "2018-06-21  1.63   0       \n",
      "2018-06-20  1.73   0       \n",
      "2018-06-19  1.71   0       \n",
      "2018-06-18  1.83   0       \n",
      "2018-06-17  1.79   0       \n",
      "2018-06-16  1.76   0       \n",
      "2018-06-15  1.78   0       \n",
      "2018-06-14  1.98   0       \n",
      "2018-06-13  1.82   0       \n",
      "2018-06-12  1.67   0       \n",
      "2018-06-11  1.84   0       \n",
      "2018-06-10  1.78   0       \n",
      "2018-06-09  1.75   0       \n",
      "2018-06-08  1.60   0       \n",
      "2018-06-07  1.57   0       \n",
      "2018-06-06  1.41   0       \n",
      "2018-06-05  1.65   0       \n",
      "2018-06-04  1.88   0       \n",
      "2018-06-03  2.10   0       \n",
      "2018-06-02  2.22   0       \n",
      "2018-06-01  2.21   0       \n",
      "2018-05-31  2.13   0       \n",
      "2018-05-30  2.08   0       \n",
      "2018-05-29  1.93   0       \n",
      "2018-05-28  1.78   0       \n",
      "2018-05-27  1.57   0       \n",
      "2018-05-26  1.58   0       \n",
      "2018-05-25  1.84   0       \n",
      "2018-05-24  1.96   0       \n",
      "2018-05-23  1.90   0       \n",
      "2018-05-22  1.95   0       \n",
      "2018-05-21  1.97   0       \n",
      "2018-05-20  2.05   0       \n",
      "2018-05-19  1.98   0       \n",
      "2018-05-18  2.08   0       \n",
      "2018-05-17  2.07   0       \n",
      "2018-05-16  1.98   0       \n",
      "2018-05-15  1.74   0       \n",
      "2018-05-14  1.80   0       \n",
      "2018-05-13  1.84   0       \n",
      "2018-05-12  1.79   0       \n",
      "2018-05-11  2.02   0       \n",
      "2018-05-10  1.94   0       \n",
      "2018-05-09  1.80   0       \n",
      "2018-05-08  1.86   0       \n",
      "2018-05-07  1.53   0       \n",
      "2018-05-06  1.27   0       \n",
      "2018-05-05  1.44   0       \n",
      "2018-05-04  1.49   0       \n",
      "2018-05-03  1.29   0       \n",
      "2018-05-02  1.53   0       \n",
      "2018-05-01  1.93   0       \n",
      "2018-04-30  1.53   0       \n",
      "2018-04-29  1.46   0       \n",
      "2018-04-28  1.53   0       \n",
      "2018-04-27  1.95   0       \n",
      "2018-04-26  2.10   0       \n",
      "2018-04-25  1.94   0       \n",
      "2018-04-24  1.60   0       \n",
      "2018-04-23  1.81   0       \n",
      "2018-04-22  2.05   0       \n",
      "2018-04-21  2.11   0       \n",
      "2018-04-20  2.24   0       \n",
      "2018-04-19  2.30   0       \n",
      "2018-04-18  2.38   0       \n",
      "2018-04-16  2.38   0       \n",
      "2018-04-15  2.23   0       \n",
      "2018-04-14  2.21   0       \n",
      "2018-04-13  2.37   0       \n",
      "2018-04-12  2.54   0       \n",
      "2018-04-11  2.68   0       \n",
      "2018-04-10  2.69   0       \n",
      "2018-04-09  2.67   0       \n",
      "2018-04-08  2.22   0       \n",
      "2018-04-07  2.31   0       \n",
      "2018-04-06  2.06   0       \n",
      "2018-04-05  2.16   0       \n",
      "2018-04-04  2.38   0       \n",
      "2018-04-03  2.47   0       \n",
      "2018-04-02  2.63   0       \n",
      "2018-04-01  2.73   0       \n",
      "2018-03-31  2.75   0       \n",
      "2018-03-30  2.71   0       \n",
      "2018-03-29  2.83   0       \n",
      "2018-03-28  2.78   0       \n",
      "2018-03-27  2.66   0       \n",
      "2018-03-26  2.67   0       \n",
      "2018-03-25  2.69   0       \n",
      "2018-03-24  2.46   0       \n",
      "2018-03-23  2.53   0       \n",
      "2018-03-22  2.43   0       \n",
      "2018-03-21  2.34   0       \n",
      "2018-03-20  2.24   0       \n",
      "2018-03-19  2.16   0       \n",
      "2018-03-18  2.15   0       \n",
      "2018-03-17  1.95   0       \n",
      "2018-03-16  1.83   0       \n",
      "2018-03-15  1.79   0       \n",
      "2018-03-14  1.78   0       \n",
      "2018-03-13  1.69   0       \n",
      "2018-03-12  1.83   0       \n",
      "2018-03-11  1.75   0       \n",
      "2018-03-10  1.92   0       \n",
      "2018-03-09  2.11   0       \n",
      "2018-03-08  2.04   0       \n",
      "2018-03-07  2.05   0       \n",
      "2018-03-06  1.97   0       \n",
      "2018-03-05  1.79   0       \n",
      "2018-03-04  1.88   0       \n",
      "2018-03-03  1.96   0       \n",
      "2018-03-02  1.82   0       \n",
      "2018-03-01  1.66   0       \n",
      "2018-02-28  1.92   0       \n",
      "2018-02-27  2.00   0       \n",
      "2018-02-26  2.11   0       \n",
      "2018-02-25  2.01   0       \n",
      "2018-02-24  2.06   0       \n",
      "2018-02-23  2.09   0       \n",
      "2018-02-22  2.27   0       \n",
      "2018-02-21  2.44   0       \n",
      "2018-02-20  2.48   0       \n",
      "2018-02-19  2.87   0       \n",
      "2018-02-18  2.82   0       \n",
      "2018-02-17  2.78   0       \n",
      "2018-02-16  2.71   0       \n",
      "2018-02-15  2.68   0       \n",
      "2018-02-14  2.63   0       \n",
      "2018-02-13  2.54   0       \n",
      "2018-02-12  2.28   0       \n",
      "2018-02-11  2.24   0       \n",
      "2018-02-10  2.01   0       \n",
      "2018-02-09  2.02   0       \n",
      "2018-02-08  1.97   0       \n",
      "2018-02-07  1.97   0       \n",
      "2018-02-06  2.22   0       \n",
      "2018-02-05  2.29   0       \n",
      "2018-02-04  2.34   0       \n",
      "2018-02-03  2.17   0       \n",
      "2018-02-02  2.14   0       \n",
      "2018-02-01  2.23   0       \n",
      "2018-01-31  2.02   0       \n",
      "2018-01-30  2.07   0       \n",
      "2018-01-29  2.07   0       \n",
      "2018-01-28  2.20   0       \n",
      "2018-01-27  2.17   0       \n",
      "2018-01-26  2.17   0       \n",
      "2018-01-25  2.04   0       \n",
      "2018-01-24  1.93   0       \n",
      "2018-01-23  2.26   0       \n",
      "2018-01-22  2.30   0       \n",
      "2018-01-21  2.43   0       \n",
      "2018-01-20  2.37   0       \n",
      "2018-01-19  2.16   0       \n",
      "2018-01-18  2.35   0       \n",
      "2018-01-17  2.50   0       \n",
      "2018-01-16  2.69   0       \n",
      "2018-01-15  2.80   0       \n",
      "2018-01-14  2.78   0       \n",
      "2018-01-13  2.34   0       \n",
      "2018-01-12  2.27   0       \n",
      "2018-01-11  2.20   0       \n",
      "2018-01-10  2.11   0       \n",
      "2018-01-09  2.10   0       \n",
      "2018-01-08  2.13   0       \n",
      "2018-01-07  1.83   0       \n",
      "2018-01-06  1.83   0       \n",
      "2018-01-05  1.61   0       \n",
      "2018-01-04  1.58   0       \n",
      "2018-01-03  1.54   0       \n",
      "2018-01-02  1.58   0       \n",
      "2018-01-01  1.36   0       \n",
      "2017-12-31  1.26   0       \n",
      "2017-12-30  1.48   0       \n",
      "2017-12-29  1.63   0       \n",
      "2017-12-28  1.65   0       \n",
      "2017-12-27  1.67   0       \n",
      "2017-12-26  1.79   0       \n",
      "2017-12-25  1.81   0       \n",
      "2017-12-24  1.67   0       \n",
      "2017-12-23  1.88   0       \n",
      "2017-12-22  1.90   0       \n",
      "2017-12-21  1.85   0       \n",
      "2017-12-20  2.00   0       \n",
      "2017-12-19  2.18   0       \n",
      "2017-12-18  1.97   0       \n",
      "2017-12-17  2.07   0       \n",
      "2017-12-16  2.00   0       \n",
      "2017-12-15  1.92   0       \n",
      "2017-12-14  1.86   0       \n",
      "2017-12-13  2.09   0       \n",
      "2017-12-12  2.08   0       \n",
      "2017-12-11  2.08   0       \n",
      "2017-12-10  2.06   0       \n",
      "2017-12-09  2.03   0       \n",
      "2017-12-08  1.90   0       \n",
      "2017-12-07  2.06   0       \n",
      "2017-12-06  1.67   0       \n",
      "2017-12-05  1.49   0       \n",
      "2017-12-04  1.52   0       \n",
      "2017-12-03  1.64   0       \n",
      "2017-12-02  1.91   0       \n",
      "2017-12-01  2.13   0       \n",
      "2017-11-30  2.04   0       \n",
      "2017-11-29  1.95   0       \n",
      "2017-11-28  1.86   0       \n",
      "2017-11-27  1.66   0       \n",
      "2017-11-26  1.54   0       \n",
      "2017-11-25  1.67   0       \n",
      "2017-11-24  1.57   0       \n",
      "2017-11-23  1.35   0       \n",
      "2017-11-22  1.35   0       \n",
      "2017-11-21  1.31   0       \n",
      "2017-11-20  1.34   0       \n",
      "2017-11-19  1.32   0       \n",
      "2017-11-18  1.33   0       \n",
      "2017-11-17  1.45   0       \n",
      "2017-11-16  1.78   0       \n",
      "2017-11-15  2.09   0       \n",
      "2017-11-14  2.27   0       \n",
      "2017-11-13  2.48   0       \n",
      "2017-11-12  2.57   0       \n",
      "2017-11-11  3.12   0       \n",
      "2017-11-10  3.15   0       \n",
      "2017-11-09  3.18   0       \n",
      "2017-11-08  3.31   0       \n",
      "2017-11-07  3.00   0       \n",
      "2017-11-06  2.80   0       \n",
      "2017-11-05  2.72   0       \n",
      "2017-11-04  2.57   0       \n",
      "2017-11-03  2.68   0       \n",
      "2017-11-02  2.46   0       \n",
      "2017-11-01  2.47   0       \n",
      "2017-10-31  2.33   0       \n",
      "2017-10-30  2.36   0       \n",
      "2017-10-29  2.17   0       \n",
      "2017-10-28  1.92   0       \n",
      "2017-10-27  1.81   0       \n",
      "2017-10-26  2.01   0       \n",
      "2017-10-25  2.01   0       \n",
      "2017-10-24  1.71   0       \n",
      "2017-10-23  1.64   0       \n",
      "2017-10-22  1.45   0       \n",
      "2017-10-21  1.60   0       \n",
      "2017-10-20  1.55   0       \n",
      "2017-10-19  1.44   0       \n",
      "2017-10-18  1.57   0       \n",
      "2017-10-17  1.69   0       \n",
      "2017-10-16  1.67   0       \n",
      "2017-10-15  1.59   0       \n",
      "2017-10-14  1.54   0       \n",
      "2017-10-13  1.54   0       \n",
      "2017-10-12  1.39   0       \n",
      "2017-10-11  1.58   0       \n",
      "2017-10-10  1.71   0       \n",
      "2017-10-09  1.77   0       \n",
      "2017-10-08  1.88   0       \n",
      "2017-10-07  1.78   0       \n",
      "2017-10-06  1.74   0       \n",
      "2017-10-05  1.92   0       \n",
      "2017-10-04  1.89   0       \n",
      "2017-10-03  2.01   0       \n",
      "2017-10-02  2.13   0       \n",
      "2017-10-01  1.68   0       \n",
      "2017-09-30  1.34   0       \n",
      "2017-09-29  1.45   0       \n",
      "2017-09-28  1.51   0       \n",
      "2017-09-27  1.80   0       \n",
      "2017-09-26  1.84   0       \n",
      "2017-09-25  2.12   0       \n",
      "2017-09-24  2.32   0       \n",
      "2017-09-23  2.57   0       \n",
      "2017-09-22  2.39   0       \n",
      "2017-09-21  2.18   0       \n",
      "2017-09-20  1.96   0       \n",
      "2017-09-19  1.89   0       \n",
      "2017-09-18  2.05   0       \n",
      "2017-09-17  2.07   0       \n",
      "2017-09-16  2.14   0       \n",
      "2017-09-15  2.23   0       \n",
      "2017-09-14  2.17   0       \n",
      "2017-09-13  2.19   0       \n",
      "2017-09-12  2.34   0       \n",
      "2017-09-11  2.54   0       \n",
      "2017-09-10  2.44   0       \n",
      "2017-09-09  2.29   0       \n",
      "2017-09-08  3.05   0       \n",
      "2017-09-07  3.57   0       \n",
      "2017-09-06  3.59   0       \n",
      "2017-09-05  4.05   0       \n",
      "2017-09-04  4.15   0       \n",
      "2017-09-03  3.72   0       \n",
      "2017-09-02  3.35   0       \n",
      "2017-09-01  3.20   0       \n",
      "2017-08-31  2.81   0       \n",
      "2017-08-30  2.68   0       \n",
      "2017-08-29  2.56   0       \n",
      "2017-08-28  2.79   0       \n",
      "2017-08-27  2.65   0       \n",
      "2017-08-26  2.49   0       \n",
      "2017-08-25  2.63   0       \n",
      "2017-08-24  2.27   0       \n",
      "2017-08-23  2.27   0       \n",
      "2017-08-22  2.40   0       \n",
      "2017-08-21  2.93   0       \n",
      "2017-08-20  3.12   0       \n",
      "2017-08-19  3.18   0       \n",
      "2017-08-18  3.23   0       \n",
      "2017-08-17  3.47   0       \n",
      "2017-08-16  3.64   0       \n",
      "2017-08-15  3.84   0       \n",
      "2017-08-14  3.83   0       \n",
      "2017-08-13  4.18   0       \n",
      "2017-08-12  3.97   0       \n",
      "2017-08-11  3.94   0       \n",
      "2017-08-10  3.66   0       \n",
      "2017-08-09  3.59   0       \n",
      "2017-08-08  3.65   0       \n",
      "2017-08-07  3.49   0       \n",
      "2017-08-06  3.40   0       \n",
      "2017-08-05  3.65   0       \n",
      "2017-08-04  3.76   0       \n",
      "2017-08-03  3.44   0       \n",
      "2017-08-02  3.09   0       \n",
      "2017-08-01  3.35   0       \n",
      "2017-07-31  3.41   0       \n",
      "2017-07-30  3.34   0       \n",
      "2017-07-29  3.66   0       \n",
      "2017-07-28  3.79   0       \n",
      "2017-07-27  3.92   0       \n",
      "2017-07-26  3.89   0       \n",
      "2017-07-25  3.97   0       \n",
      "2017-07-24  4.16   0       \n",
      "2017-07-23  3.88   0       \n",
      "2017-07-22  3.74   0       \n",
      "2017-07-21  3.60   0       \n",
      "2017-07-20  3.45   0       \n",
      "2017-07-19  3.38   0       \n",
      "2017-07-18  3.69   0       \n",
      "2017-07-17  3.59   0       \n",
      "2017-07-16  3.63   0       \n",
      "2017-07-15  3.47   0       \n",
      "2017-07-14  3.10   0       \n",
      "2017-07-13  3.11   0       \n",
      "2017-07-12  3.32   0       \n",
      "2017-07-11  3.42   0       \n",
      "2017-07-10  3.91   0       \n",
      "2017-07-09  4.16   0       \n",
      "2017-07-08  3.92   0       \n",
      "2017-07-07  3.84   0       \n",
      "2017-07-06  3.79   0       \n",
      "2017-07-05  3.54   0       \n",
      "2017-07-04  3.57   0       \n",
      "2017-07-03  3.06   0       \n",
      "2017-07-02  2.86   0       \n",
      "2017-07-01  2.85   0       \n",
      "2017-06-30  2.50   0       \n",
      "2017-06-29  2.14   0       \n",
      "2020-06-28  7.07   1       \n",
      "2020-06-27  7.07   1       \n",
      "2020-06-26  7.05   1       \n",
      "2020-06-25  7.05   1       \n",
      "2020-06-24  7.07   1       \n",
      "2020-06-22  7.07   1       \n",
      "2020-06-21  7.08   1       \n",
      "2020-06-20  7.15   1       \n",
      "2020-06-19  7.17   1       \n",
      "2020-06-18  7.18   1       \n",
      "2020-06-17  7.17   1       \n",
      "2020-06-16  7.29   1       \n",
      "2020-06-15  7.43   1       \n",
      "2020-06-14  7.43   1       \n",
      "2020-06-13  7.40   1       \n",
      "2020-06-10  7.40   1       \n",
      "2020-06-09  7.45   1       \n",
      "2020-06-08  7.45   1       \n",
      "2020-06-07  7.47   1       \n",
      "2020-06-06  7.50   1       \n",
      "2020-06-04  7.50   1       \n",
      "2020-06-03  7.44   1       \n",
      "2020-06-02  7.44   1       \n",
      "2020-06-01  7.66   1       \n",
      "2020-05-31  7.67   1       \n",
      "2020-05-28  7.67   1       \n",
      "2020-05-27  7.68   1       \n",
      "2020-05-26  7.70   1       \n",
      "2020-05-25  7.67   1       \n",
      "2020-05-24  7.68   1       \n",
      "2020-05-23  7.67   1       \n",
      "2020-05-22  6.77   1       \n",
      "2020-05-21  6.77   1       \n",
      "2020-05-20  7.08   1       \n",
      "2020-05-19  7.31   1       \n",
      "2020-05-18  7.31   1       \n",
      "2020-05-17  7.20   1       \n",
      "2020-05-16  7.15   1       \n",
      "2020-05-15  7.14   1       \n",
      "2020-05-14  7.23   1       \n",
      "2020-05-13  7.25   1       \n",
      "2020-05-12  7.13   1       \n",
      "2020-05-11  6.96   1       \n",
      "2020-05-10  6.92   1       \n",
      "2020-05-09  6.69   1       \n",
      "2020-05-08  6.36   1       \n",
      "2020-05-07  6.08   1       \n",
      "2020-05-06  5.97   1       \n",
      "2020-05-05  6.21   1       \n",
      "2020-05-04  6.27   1       \n",
      "2020-05-03  6.28   1       \n",
      "2020-05-02  6.13   1       \n",
      "2020-05-01  5.84   1       \n",
      "2020-04-30  5.89   1       \n",
      "2020-04-29  5.83   1       \n",
      "2020-04-28  5.73   1       \n",
      "2020-04-27  5.80   1       \n",
      "2020-04-26  6.33   1       \n",
      "2020-04-25  5.99   1       \n",
      "2020-04-24  5.86   1       \n",
      "2020-04-23  5.71   1       \n",
      "2020-04-22  5.58   1       \n",
      "2020-04-21  5.56   1       \n",
      "2020-04-20  5.53   1       \n",
      "2020-04-19  5.64   1       \n",
      "2020-04-18  5.85   1       \n",
      "2020-04-17  5.82   1       \n",
      "2020-04-16  5.81   1       \n",
      "2020-04-15  5.81   1       \n",
      "2020-04-14  5.77   1       \n",
      "2020-04-13  5.75   1       \n",
      "2020-04-12  5.94   1       \n",
      "2020-04-11  6.16   1       \n",
      "2020-04-10  6.33   1       \n",
      "2020-04-09  6.32   1       \n",
      "2020-04-08  6.34   1       \n",
      "2020-04-07  6.61   1       \n",
      "2020-04-06  6.60   1       \n",
      "2020-04-05  6.87   1       \n",
      "2020-04-04  7.05   1       \n",
      "2020-03-31  7.05   1       \n",
      "2020-03-30  7.06   1       \n",
      "2020-03-29  7.19   1       \n",
      "2020-03-28  7.60   1       \n",
      "2020-03-27  7.63   1       \n",
      "2020-03-26  7.63   1       \n",
      "2020-03-25  7.67   1       \n",
      "2020-03-24  7.97   1       \n",
      "2020-03-23  7.87   1       \n",
      "2020-03-22  7.79   1       \n",
      "2020-03-21  7.89   1       \n",
      "2020-03-17  7.89   1       \n",
      "2020-03-16  7.91   1       \n",
      "2020-03-15  8.29   1       \n",
      "2020-03-14  8.64   1       \n",
      "2020-03-13  9.09   1       \n",
      "2020-03-12  9.54   1       \n",
      "2020-03-11  9.66   1       \n",
      "2020-03-10  9.66   1       \n",
      "2020-03-09  9.68   1       \n",
      "2020-03-08  9.79   1       \n",
      "2020-03-05  9.79   1       \n",
      "2020-03-04  9.84   1       \n",
      "2020-03-03  9.96   1       \n",
      "2020-03-02  10.68  1       \n",
      "2020-03-01  10.71  1       \n",
      "2020-02-29  10.92  1       \n",
      "2020-02-28  11.02  1       \n",
      "2020-02-27  11.05  1       \n",
      "2020-02-25  11.05  1       \n",
      "2020-02-24  11.04  1       \n",
      "2020-02-23  11.35  1       \n",
      "2020-02-22  11.42  1       \n",
      "2020-02-20  11.42  1       \n",
      "2020-02-19  11.13  1       \n",
      "2020-02-18  11.05  1       \n",
      "2020-02-17  11.01  1       \n",
      "2020-02-16  10.74  1       \n",
      "2020-02-15  10.72  1       \n",
      "2020-02-14  10.73  1       \n",
      "2020-02-13  10.71  1       \n",
      "2020-02-12  10.73  1       \n",
      "2020-02-11  10.72  1       \n",
      "2020-02-10  10.72  1       \n",
      "2020-02-09  10.06  1       \n",
      "2020-02-08  9.93   1       \n",
      "2020-02-07  9.94   1       \n",
      "2020-02-06  9.93   1       \n",
      "2020-02-05  9.90   1       \n",
      "2020-02-04  10.04  1       \n",
      "2020-02-03  11.71  1       \n",
      "2020-02-02  11.74  1       \n",
      "2020-02-01  11.84  1       \n",
      "2020-01-31  11.88  1       \n",
      "2020-01-30  11.78  1       \n",
      "2020-01-29  11.82  1       \n",
      "2020-01-28  11.95  1       \n",
      "2020-01-27  12.54  1       \n",
      "2020-01-26  12.53  1       \n",
      "2020-01-25  12.56  1       \n",
      "2020-01-24  12.06  1       \n",
      "2020-01-23  11.86  1       \n",
      "2020-01-22  12.22  1       \n",
      "2020-01-21  11.98  1       \n",
      "2020-01-20  12.29  1       \n",
      "2020-01-19  12.66  1       \n",
      "2020-01-18  12.66  1       \n",
      "2020-01-17  12.67  1       \n",
      "2020-01-16  11.74  1       \n",
      "2020-01-15  11.98  1       \n",
      "2020-01-14  12.09  1       \n",
      "2020-01-13  11.97  1       \n",
      "2020-01-12  11.87  1       \n",
      "2020-01-11  11.92  1       \n",
      "2020-01-10  11.94  1       \n",
      "2020-01-09  12.07  1       \n",
      "2020-01-08  11.96  1       \n",
      "2020-01-07  12.29  1       \n",
      "2020-01-06  12.72  1       \n",
      "2020-01-05  13.23  1       \n",
      "2020-01-04  13.11  1       \n",
      "2020-01-03  13.10  1       \n",
      "2020-01-02  13.10  1       \n",
      "2020-01-01  13.12  1       \n",
      "2019-12-31  13.13  1       \n",
      "2019-12-30  13.14  1       \n",
      "2019-12-29  13.22  1       \n",
      "2019-12-28  13.29  1       \n",
      "2019-12-27  13.60  1       \n",
      "2019-12-26  13.25  1       \n",
      "2019-12-25  13.09  1       \n",
      "2019-12-24  12.59  1       \n",
      "2019-12-23  12.44  1       \n",
      "2019-12-22  11.67  1       \n",
      "2019-12-21  11.48  1       \n",
      "2019-12-20  11.47  1       \n",
      "2019-12-19  11.52  1       \n",
      "2019-12-18  11.54  1       \n",
      "2019-12-17  11.49  1       \n",
      "2019-12-16  11.27  1       \n",
      "2019-12-15  10.80  1       \n",
      "2019-12-14  10.88  1       \n",
      "2019-12-13  10.96  1       \n",
      "2019-12-12  10.86  1       \n",
      "2019-12-11  10.85  1       \n",
      "2019-12-10  11.25  1       \n",
      "2019-12-09  11.68  1       \n",
      "2019-12-08  11.89  1       \n",
      "2019-12-07  11.93  1       \n",
      "2019-12-06  11.93  1       \n",
      "2019-12-05  12.35  1       \n",
      "2019-12-04  12.12  1       \n",
      "2019-12-03  12.19  1       \n",
      "2019-12-02  12.20  1       \n",
      "2019-12-01  12.17  1       \n",
      "2019-11-30  12.17  1       \n",
      "2019-11-29  12.50  1       \n",
      "2019-11-28  12.76  1       \n",
      "2019-11-27  12.87  1       \n",
      "2019-11-26  12.94  1       \n",
      "2019-11-25  13.12  1       \n",
      "2019-11-24  12.52  1       \n",
      "2019-11-23  12.66  1       \n",
      "2019-11-22  12.80  1       \n",
      "2019-11-21  12.16  1       \n",
      "2019-11-20  12.01  1       \n",
      "2019-11-19  12.06  1       \n",
      "2019-11-18  11.94  1       \n",
      "2019-11-17  11.93  1       \n",
      "2019-11-16  12.04  1       \n",
      "2019-11-15  13.36  1       \n",
      "2019-11-14  13.50  1       \n",
      "2019-11-13  12.86  1       \n",
      "2019-11-12  12.92  1       \n",
      "2019-11-11  13.09  1       \n",
      "2019-11-10  11.57  1       \n",
      "2019-11-09  10.34  1       \n",
      "2019-11-08  10.07  1       \n",
      "2019-11-07  10.02  1       \n",
      "2019-11-06  9.60   1       \n",
      "2019-11-05  9.01   1       \n",
      "2019-11-04  8.91   1       \n",
      "2019-11-03  8.84   1       \n",
      "2019-11-02  8.92   1       \n",
      "2019-11-01  9.90   1       \n",
      "2019-10-31  9.51   1       \n",
      "2019-10-30  7.97   1       \n",
      "2019-10-29  7.61   1       \n",
      "2019-10-28  8.22   1       \n",
      "2019-10-27  8.17   1       \n",
      "2019-10-26  7.97   1       \n",
      "2019-10-25  6.93   1       \n",
      "2019-10-24  4.95   1       \n",
      "2019-10-23  3.95   1       \n",
      "2019-10-22  3.23   1       \n",
      "2019-10-21  3.13   1       \n",
      "2019-10-20  3.12   1       \n",
      "2019-10-19  2.87   1       \n",
      "2019-10-18  2.83   1       \n",
      "2019-10-17  2.80   1       \n",
      "2019-10-16  2.69   1       \n",
      "2019-10-15  2.67   1       \n",
      "2019-10-14  2.66   1       \n",
      "2019-10-13  2.62   1       \n",
      "2019-10-12  2.65   1       \n",
      "2019-10-11  2.66   1       \n",
      "2019-10-10  2.65   1       \n",
      "2019-10-09  2.66   1       \n",
      "2019-10-08  2.67   1       \n",
      "2019-10-07  2.65   1       \n",
      "2019-10-06  2.66   1       \n",
      "2020-01-02  5.02   0       \n",
      "2020-01-01  5.77   0       \n",
      "2019-12-31  5.74   0       \n",
      "2019-12-30  5.94   0       \n",
      "2019-12-29  6.36   0       \n",
      "2019-12-28  7.53   0       \n",
      "2019-12-27  7.83   0       \n",
      "2019-12-26  7.78   0       \n",
      "2019-12-25  7.79   0       \n",
      "2019-12-24  8.72   0       \n",
      "2019-12-23  10.08  0       \n",
      "2019-12-22  11.18  0       \n",
      "2019-12-21  10.14  0       \n",
      "2019-12-20  9.00   0       \n",
      "2019-12-19  8.40   0       \n",
      "2019-12-18  9.17   0       \n",
      "2019-12-17  9.43   0       \n",
      "2019-12-16  7.96   0       \n",
      "2019-12-15  7.93   0       \n",
      "2019-12-14  8.06   0       \n",
      "2019-12-13  8.48   0       \n",
      "2019-12-12  8.24   0       \n",
      "2019-12-11  8.26   0       \n",
      "2019-12-10  7.47   0       \n",
      "2019-12-09  8.06   0       \n",
      "2019-12-08  8.62   0       \n",
      "2019-12-07  8.25   0       \n",
      "2019-12-06  8.55   0       \n",
      "2019-12-05  8.24   0       \n",
      "2019-12-04  8.45   0       \n",
      "2019-12-03  7.46   0       \n",
      "2019-12-02  6.48   0       \n",
      "2019-12-01  6.63   0       \n",
      "2020-06-28  3.67   0       \n",
      "2020-06-27  3.72   0       \n",
      "2020-06-26  3.62   0       \n",
      "2020-06-25  3.59   0       \n",
      "2020-06-24  3.40   0       \n",
      "2020-06-23  3.37   0       \n",
      "2020-06-22  3.19   0       \n",
      "2020-06-21  3.15   0       \n",
      "2020-06-20  3.82   0       \n",
      "2020-06-19  3.78   0       \n",
      "2020-06-18  4.08   0       \n",
      "2020-06-17  4.69   0       \n",
      "2020-06-16  4.84   0       \n",
      "2020-06-15  4.72   0       \n",
      "2020-06-14  4.66   0       \n",
      "2020-06-13  4.49   0       \n",
      "2020-06-12  5.08   0       \n",
      "2020-06-11  4.59   0       \n",
      "2020-06-10  4.13   0       \n",
      "2020-06-09  4.09   0       \n",
      "2020-06-08  4.27   0       \n",
      "2020-06-07  4.43   0       \n",
      "2020-06-06  4.39   0       \n",
      "2020-06-05  4.55   0       \n",
      "2020-06-04  4.24   0       \n",
      "2020-06-03  3.82   0       \n",
      "2020-06-02  3.84   0       \n",
      "2020-06-01  3.99   0       \n",
      "2020-05-31  3.87   0       \n",
      "2020-05-30  4.25   0       \n",
      "2020-05-29  4.47   0       \n",
      "2020-05-28  4.47   0       \n",
      "2020-05-27  4.86   0       \n",
      "2020-05-26  5.32   0       \n",
      "2020-05-25  5.17   0       \n",
      "2020-05-24  4.99   0       \n",
      "2020-05-23  5.56   0       \n",
      "2020-05-22  4.73   0       \n",
      "2020-05-21  5.39   0       \n",
      "2020-05-20  5.60   0       \n",
      "2020-05-19  5.52   0       \n",
      "2020-05-18  5.40   0       \n",
      "2020-05-17  5.55   0       \n",
      "2020-05-16  5.95   0       \n",
      "2020-05-15  5.69   0       \n",
      "2020-05-14  5.35   0       \n",
      "2020-05-13  4.87   0       \n",
      "2020-05-12  4.41   0       \n",
      "2020-05-11  4.30   0       \n",
      "2020-05-10  5.92   0       \n",
      "2020-05-09  6.28   0       \n",
      "2020-05-08  5.94   0       \n",
      "2020-05-07  5.08   0       \n",
      "2020-05-06  5.37   0       \n",
      "2020-05-05  5.71   0       \n",
      "2020-05-04  5.40   0       \n",
      "2020-05-03  5.18   0       \n",
      "2019-09-03  2.98   1       \n",
      "2019-09-02  2.98   1       \n",
      "2019-09-01  2.95   1       \n",
      "2019-08-31  2.93   1       \n",
      "2019-08-30  2.84   1       \n",
      "2019-08-29  2.62   1       \n",
      "2019-08-28  2.61   1       \n",
      "2019-08-27  2.61   1       \n",
      "2019-08-26  2.63   1       \n",
      "2019-08-25  2.64   1       \n",
      "2019-08-23  2.64   1       \n",
      "2019-08-22  2.77   1       \n",
      "2019-08-21  2.79   1       \n",
      "2019-08-19  2.79   1       \n",
      "2019-08-18  2.78   1       \n",
      "2019-08-17  2.70   1       \n",
      "2019-08-16  2.57   1       \n",
      "2019-08-15  2.65   1       \n",
      "2019-08-14  2.69   1       \n",
      "2019-08-13  2.69   1       \n",
      "2019-08-12  2.71   1       \n",
      "2019-08-11  2.71   1       \n",
      "2019-08-10  2.72   1       \n",
      "2019-08-09  2.70   1       \n",
      "2019-08-08  2.67   1       \n",
      "2019-08-07  2.71   1       \n",
      "2019-08-06  2.73   1       \n",
      "2019-08-05  2.79   1       \n",
      "2019-08-04  2.80   1       \n",
      "2019-08-01  2.80   1       \n",
      "2019-07-31  3.20   1       \n",
      "2019-07-30  3.48   1       \n",
      "2019-07-29  3.52   1       \n",
      "2019-07-28  3.52   1       \n",
      "2019-07-27  3.54   1       \n",
      "2019-07-26  3.54   1       \n",
      "2019-07-25  3.55   1       \n",
      "2019-07-24  3.54   1       \n",
      "2019-07-23  3.56   1       \n",
      "2019-07-22  3.51   1       \n",
      "2019-07-21  3.25   1       \n",
      "2019-07-20  3.24   1       \n",
      "2019-07-19  3.24   1       \n",
      "2019-07-18  3.26   1       \n",
      "2019-07-17  3.24   1       \n",
      "2019-07-16  3.31   1       \n",
      "2019-07-15  3.62   1       \n",
      "2019-07-14  3.61   1       \n",
      "2019-07-13  3.63   1       \n",
      "2019-07-11  3.63   1       \n",
      "2019-07-10  3.62   1       \n",
      "2019-07-09  3.63   1       \n",
      "2019-07-08  3.64   1       \n",
      "2019-07-07  3.67   1       \n",
      "2019-07-06  3.68   1       \n",
      "2019-07-05  3.71   1       \n",
      "2019-07-04  3.70   1       \n",
      "2019-07-03  3.70   1       \n",
      "2019-07-02  3.68   1       \n",
      "2019-07-01  4.05   1       \n",
      "2019-06-30  3.95   1       \n",
      "2019-06-29  3.69   1       \n",
      "2019-06-28  3.70   1       \n",
      "2019-06-27  3.56   1       \n",
      "2019-06-26  3.50   1       \n",
      "2019-06-25  3.48   1       \n",
      "2019-06-24  3.50   1       \n",
      "2019-06-23  3.76   1       \n",
      "2019-06-22  3.69   1       \n",
      "2019-06-21  3.36   1       \n",
      "2019-06-20  3.51   1       \n",
      "2019-06-19  3.57   1       \n",
      "2019-06-18  3.47   1       \n",
      "2019-06-17  3.34   1       \n",
      "2019-06-16  3.31   1       \n",
      "2019-06-15  3.12   1       \n",
      "2019-06-14  3.06   1       \n",
      "2019-06-13  3.01   1       \n",
      "2019-06-12  2.74   1       \n",
      "2019-06-11  2.67   1       \n",
      "2019-06-10  2.63   1       \n",
      "2019-06-09  2.60   1       \n",
      "2019-06-08  2.77   1       \n",
      "2019-06-07  2.76   1       \n",
      "2019-06-06  2.77   1       \n",
      "2019-06-05  2.77   1       \n",
      "2019-06-04  2.74   1       \n",
      "2019-06-03  2.69   1       \n",
      "2019-06-02  2.71   1       \n",
      "2019-06-01  2.66   1       \n",
      "2019-05-31  2.78   1       \n",
      "2019-05-30  2.76   1       \n",
      "2019-05-29  2.58   1       \n",
      "2019-05-28  2.67   1       \n",
      "2019-05-27  2.77   1       \n",
      "2019-05-26  2.68   1       \n",
      "2019-05-25  2.70   1       \n",
      "2019-05-24  2.70   1       \n",
      "2019-05-23  2.72   1       \n",
      "2019-05-22  2.76   1       \n",
      "2019-05-21  2.80   1       \n",
      "2019-05-20  2.81   1       \n",
      "2019-05-19  2.83   1       \n",
      "2019-05-18  2.86   1       \n",
      "2019-05-17  3.00   1       \n",
      "2019-05-16  3.00   1       \n",
      "2019-05-15  2.86   1       \n",
      "2019-05-14  2.79   1       \n",
      "2019-05-13  2.81   1       \n",
      "2019-05-12  2.65   1       \n",
      "2019-05-11  2.58   1       \n",
      "2019-05-10  2.49   1       \n",
      "2019-05-09  2.43   1       \n",
      "2019-05-08  2.40   1       \n",
      "2019-05-07  2.32   1       \n",
      "2019-05-06  2.23   1       \n",
      "2019-05-05  2.05   1       \n",
      "2019-05-04  2.02   1       \n",
      "2019-05-03  2.01   1       \n",
      "2019-05-02  2.01   1       \n",
      "2019-05-01  1.98   1       \n",
      "2019-04-30  1.99   1       \n",
      "2019-04-29  1.99   1       \n",
      "2019-04-28  2.04   1       \n",
      "2019-04-27  2.07   1       \n",
      "2019-04-26  2.04   1       \n",
      "2019-04-25  2.02   1       \n",
      "2019-04-24  2.02   1       \n",
      "2019-04-23  2.01   1       \n",
      "2019-04-22  2.02   1       \n",
      "2019-04-21  2.07   1       \n",
      "2019-04-20  1.96   1       \n",
      "2019-04-19  1.91   1       \n",
      "2019-04-18  1.90   1       \n",
      "2019-04-17  1.88   1       \n",
      "2019-04-16  1.71   1       \n",
      "2019-04-15  1.59   1       \n",
      "2019-04-14  1.53   1       \n",
      "2019-04-13  1.53   1       \n",
      "2019-04-12  1.45   1       \n",
      "2019-04-11  1.44   1       \n",
      "2019-04-10  1.62   1       \n",
      "2019-04-09  1.58   1       \n",
      "2019-04-08  1.40   1       \n",
      "2019-04-07  1.37   1       \n",
      "2019-04-06  1.37   1       \n",
      "2019-04-05  1.35   1       \n",
      "2019-04-04  1.35   1       \n",
      "2019-04-03  1.32   1       \n",
      "2019-04-02  1.30   1       \n",
      "2019-04-01  1.29   1       \n",
      "2019-03-31  1.28   1       \n",
      "2019-03-30  1.34   1       \n",
      "2019-03-29  1.52   1       \n",
      "2019-03-28  1.59   1       \n",
      "2019-03-27  1.52   1       \n",
      "2019-03-26  1.35   1       \n",
      "2019-03-25  1.27   1       \n",
      "2019-03-24  1.41   1       \n",
      "2019-03-23  1.39   1       \n",
      "2019-03-22  1.32   1       \n",
      "2019-03-21  1.39   1       \n",
      "2019-03-20  1.33   1       \n",
      "2019-03-19  1.11   1       \n",
      "2019-03-18  1.11   1       \n",
      "2019-03-17  1.10   1       \n",
      "2019-03-16  1.09   1       \n",
      "2019-03-15  1.08   1       \n",
      "2019-03-14  1.08   1       \n",
      "2019-03-13  1.10   1       \n",
      "2019-03-12  1.11   1       \n",
      "2019-03-11  1.01   1       \n",
      "2019-03-10  1.12   1       \n",
      "2019-03-09  1.31   1       \n",
      "2019-03-08  1.28   1       \n",
      "2019-03-07  1.17   1       \n",
      "2019-03-06  1.07   1       \n",
      "2019-03-05  0.98   1       \n",
      "2019-03-04  0.92   1       \n",
      "2019-03-03  0.93   1       \n",
      "2019-03-02  0.96   1       \n",
      "2019-03-01  0.91   1       \n",
      "2019-02-28  0.89   1       \n",
      "2019-02-27  0.87   1       \n",
      "2019-02-26  0.84   1       \n",
      "2019-02-25  0.83   1       \n",
      "2019-02-24  0.79   1       \n",
      "2019-02-23  0.80   1       \n",
      "2019-02-22  0.78   1       \n",
      "2019-02-21  0.70   1       \n",
      "2019-02-20  0.72   1       \n",
      "2019-02-19  0.74   1       \n",
      "2019-02-18  0.74   1       \n",
      "2019-02-17  0.72   1       \n",
      "2019-02-16  0.69   1       \n",
      "2019-02-15  0.74   1       \n",
      "2019-02-14  0.78   1       \n",
      "2019-02-13  0.77   1       \n",
      "2019-02-12  0.69   1       \n",
      "2019-02-11  0.68   1       \n",
      "2019-02-09  0.68   1       \n",
      "2019-02-08  0.66   1       \n",
      "2019-02-07  0.67   1       \n",
      "2019-02-05  0.67   1       \n",
      "2019-02-04  0.68   1       \n",
      "2019-02-03  0.70   1       \n",
      "2019-02-02  0.69   1       \n",
      "2019-02-01  0.81   1       \n",
      "2019-01-31  0.87   1       \n",
      "2019-01-30  0.92   1       \n",
      "2019-01-29  0.96   1       \n",
      "2019-01-28  1.03   1       \n",
      "2019-01-27  1.03   1       \n",
      "2019-01-26  1.01   1       \n",
      "2019-01-25  0.85   1       \n",
      "2019-01-24  0.83   1       \n",
      "2019-01-23  0.89   1       \n",
      "2019-01-22  0.93   1       \n",
      "2019-01-21  0.93   1       \n",
      "2019-01-20  0.92   1       \n",
      "2019-01-19  0.91   1       \n",
      "2019-01-18  0.91   1       \n",
      "2019-01-17  0.99   1       \n",
      "2019-01-16  1.17   1       \n",
      "2019-01-15  1.18   1       \n",
      "2019-01-14  1.18   1       \n",
      "2019-01-13  1.15   1       \n",
      "2019-01-12  1.20   1       \n",
      "2019-01-11  1.24   1       \n",
      "2019-01-10  1.24   1       \n",
      "2019-01-09  1.25   1       \n",
      "2019-01-06  1.25   1       \n",
      "2019-01-05  1.26   1       \n",
      "2019-01-04  1.26   1       \n",
      "2019-01-03  1.25   1       \n",
      "2019-01-02  1.25   1       \n",
      "2019-01-01  1.06   1       \n",
      "2018-12-31  1.02   1       \n",
      "2018-12-30  1.00   1       \n",
      "2018-12-28  1.00   1       \n",
      "2018-12-27  1.01   1       \n",
      "2018-12-26  1.02   1       \n",
      "2018-12-25  1.01   1       \n",
      "2018-12-23  1.01   1       \n",
      "2018-12-22  1.02   1       \n",
      "2018-12-21  1.03   1       \n",
      "2018-12-20  1.03   1       \n",
      "2018-12-19  1.01   1       \n",
      "2018-12-18  1.00   1       \n",
      "2018-12-16  1.00   1       \n",
      "2018-12-15  0.99   1       \n",
      "2018-12-14  0.99   1       \n",
      "2018-12-13  1.01   1       \n",
      "2018-12-12  1.13   1       \n",
      "2018-12-10  1.13   1       \n",
      "2018-12-09  1.15   1       \n",
      "2018-12-08  1.15   1       \n",
      "2018-12-07  1.20   1       \n",
      "2018-12-06  1.22   1       \n",
      "2018-12-05  1.19   1       \n",
      "2018-12-04  1.19   1       \n",
      "2018-12-03  1.20   1       \n",
      "2018-12-02  1.20   1       \n",
      "2018-12-01  1.18   1       \n",
      "2018-11-30  1.17   1       \n",
      "2018-11-28  1.17   1       \n",
      "2018-11-27  1.15   1       \n",
      "2018-11-25  1.15   1       \n",
      "2018-11-24  1.14   1       \n",
      "2018-11-23  1.12   1       \n",
      "2018-11-22  1.10   1       \n",
      "2018-11-19  1.10   1       \n",
      "2018-11-18  1.03   1       \n",
      "2018-11-17  0.99   1       \n",
      "2018-11-16  0.97   1       \n",
      "2018-11-15  0.97   1       \n",
      "2018-11-14  0.95   1       \n",
      "2018-11-13  0.93   1       \n",
      "2018-11-12  0.88   1       \n",
      "2018-11-11  0.84   1       \n",
      "2018-11-10  0.86   1       \n",
      "2018-11-09  0.94   1       \n",
      "2018-11-08  1.00   1       \n",
      "2018-11-07  0.93   1       \n",
      "2018-11-06  0.93   1       \n",
      "2018-11-05  0.98   1       \n",
      "2018-11-04  0.95   1       \n",
      "2018-11-03  0.93   1       \n",
      "2018-11-02  0.96   1       \n",
      "2018-11-01  0.85   1       \n",
      "2018-10-31  1.03   1       \n",
      "2018-10-30  1.22   1       \n",
      "2018-10-29  1.27   1       \n",
      "2018-10-28  1.40   1       \n",
      "2018-10-27  1.47   1       \n",
      "2018-10-26  1.51   1       \n",
      "2018-10-25  1.54   1       \n",
      "2018-10-24  1.54   1       \n",
      "2018-10-23  1.85   1       \n",
      "2018-10-22  1.94   1       \n",
      "2018-10-21  1.96   1       \n",
      "2018-10-20  2.26   1       \n",
      "2018-10-19  2.39   1       \n",
      "2018-10-18  2.33   1       \n",
      "2018-10-17  2.53   1       \n",
      "2018-10-16  2.75   1       \n",
      "2018-10-15  2.77   1       \n",
      "2018-10-14  2.71   1       \n",
      "2018-10-13  2.71   1       \n",
      "2018-10-12  2.66   1       \n",
      "2018-10-11  2.69   1       \n",
      "2018-10-10  2.76   1       \n",
      "2018-10-09  2.78   1       \n",
      "2018-10-08  2.85   1       \n",
      "2018-10-07  2.85   1       \n",
      "2018-10-06  2.89   1       \n",
      "2018-10-05  3.01   1       \n",
      "2018-10-04  3.01   1       \n",
      "2018-10-03  3.14   1       \n",
      "2018-09-30  3.14   1       \n",
      "2018-09-29  3.30   1       \n",
      "2018-09-28  3.31   1       \n",
      "2018-09-27  3.31   1       \n",
      "2018-09-26  3.32   1       \n",
      "2018-09-25  3.58   1       \n",
      "2018-09-24  3.62   1       \n",
      "2018-09-23  3.51   1       \n",
      "2018-09-22  3.43   1       \n",
      "2018-09-21  3.32   1       \n",
      "2018-09-19  3.32   1       \n",
      "2018-09-18  3.35   1       \n",
      "2018-09-17  3.35   1       \n",
      "2018-09-16  3.33   1       \n",
      "2018-09-15  3.33   1       \n",
      "2018-09-14  3.32   1       \n",
      "2018-09-13  3.29   1       \n",
      "2018-09-12  3.29   1       \n",
      "2018-09-11  3.11   1       \n",
      "2018-09-10  3.09   1       \n",
      "2018-09-09  3.09   1       \n",
      "2018-09-08  3.12   1       \n",
      "2018-09-07  3.12   1       \n",
      "2018-09-06  3.13   1       \n",
      "2018-09-05  3.11   1       \n",
      "2018-09-04  3.09   1       \n",
      "2018-09-03  2.93   1       \n",
      "2018-09-02  2.90   1       \n",
      "2018-09-01  2.88   1       \n",
      "2018-08-31  2.85   1       \n",
      "2018-08-30  2.85   1       \n",
      "2018-08-29  2.84   1       \n",
      "2018-08-28  2.80   1       \n",
      "2018-08-27  2.80   1       \n",
      "2018-08-26  2.79   1       \n",
      "2018-08-25  2.78   1       \n",
      "2018-08-24  2.75   1       \n",
      "2018-08-22  2.75   1       \n",
      "2018-08-21  2.73   1       \n",
      "2018-08-20  2.73   1       \n",
      "2018-08-19  2.75   1       \n",
      "2018-08-18  2.75   1       \n",
      "2018-08-17  2.77   1       \n",
      "2018-08-16  2.90   1       \n",
      "2018-08-15  3.06   1       \n",
      "2018-08-14  3.05   1       \n",
      "2018-08-13  2.91   1       \n",
      "2018-08-12  2.90   1       \n",
      "2018-08-11  2.80   1       \n",
      "2018-08-10  2.75   1       \n",
      "2018-08-09  2.83   1       \n",
      "2018-08-08  2.56   1       \n",
      "2018-08-07  2.55   1       \n",
      "2018-08-06  2.59   1       \n",
      "2018-08-05  2.68   1       \n",
      "2018-08-04  2.67   1       \n",
      "2018-08-03  2.66   1       \n",
      "2018-08-02  2.72   1       \n",
      "2018-08-01  2.69   1       \n",
      "2018-07-31  2.78   1       \n",
      "2018-07-30  2.81   1       \n",
      "2018-07-29  2.80   1       \n",
      "2018-07-28  2.89   1       \n",
      "2018-07-27  2.89   1       \n",
      "2018-07-26  2.90   1       \n",
      "2018-07-25  2.86   1       \n",
      "2018-07-24  2.91   1       \n",
      "2018-07-23  2.90   1       \n",
      "2018-07-22  2.87   1       \n",
      "2018-07-21  2.84   1       \n",
      "2018-07-20  2.84   1       \n",
      "2018-07-19  2.82   1       \n",
      "2018-07-18  2.83   1       \n",
      "2018-07-17  2.71   1       \n",
      "2018-07-16  2.67   1       \n",
      "2018-07-12  2.67   1       \n",
      "2018-07-11  2.70   1       \n",
      "2018-07-10  2.77   1       \n",
      "2018-07-09  2.81   1       \n",
      "2018-07-08  2.79   1       \n",
      "2018-07-07  2.96   1       \n",
      "2018-07-06  2.99   1       \n",
      "2018-07-05  3.00   1       \n",
      "2018-07-04  2.96   1       \n",
      "2018-07-03  2.95   1       \n",
      "2018-07-02  2.94   1       \n",
      "2018-07-01  2.91   1       \n",
      "2018-06-30  2.78   1       \n",
      "2018-06-29  2.68   1       \n",
      "2018-06-27  2.68   1       \n",
      "2018-06-26  2.69   1       \n",
      "2018-06-25  2.69   1       \n",
      "2018-06-24  2.71   1       \n",
      "2018-06-22  2.71   1       \n",
      "2018-06-21  2.69   1       \n",
      "2018-06-20  2.68   1       \n",
      "2018-06-19  2.68   1       \n",
      "2018-06-18  2.69   1       \n",
      "2018-06-17  2.68   1       \n",
      "2018-06-16  2.55   1       \n",
      "2018-06-15  2.52   1       \n",
      "2018-06-14  2.50   1       \n",
      "2018-06-13  2.59   1       \n",
      "2018-06-12  2.69   1       \n",
      "2018-06-11  2.69   1       \n",
      "2018-06-10  2.70   1       \n",
      "2018-06-09  2.66   1       \n",
      "2018-06-08  2.63   1       \n",
      "2018-06-07  2.62   1       \n",
      "2018-06-06  2.64   1       \n",
      "2018-06-05  2.74   1       \n",
      "2018-06-04  2.71   1       \n",
      "2018-06-03  2.66   1       \n",
      "2018-06-02  2.64   1       \n",
      "2018-06-01  2.64   1       \n",
      "2018-05-31  2.57   1       \n",
      "2018-05-30  2.53   1       \n",
      "2018-05-29  2.53   1       \n",
      "2018-05-28  2.51   1       \n",
      "2018-05-27  2.61   1       \n",
      "2018-05-26  2.75   1       \n",
      "2018-05-25  2.81   1       \n",
      "2018-05-24  2.78   1       \n",
      "2018-05-23  2.56   1       \n",
      "2018-05-22  2.36   1       \n",
      "2018-05-21  2.31   1       \n",
      "2018-05-20  2.29   1       \n",
      "2018-05-19  2.28   1       \n",
      "2018-05-18  2.26   1       \n",
      "2018-05-17  2.47   1       \n",
      "2018-05-16  2.50   1       \n",
      "2018-05-15  2.34   1       \n",
      "2018-05-14  2.31   1       \n",
      "2018-05-13  2.29   1       \n",
      "2018-05-12  2.32   1       \n",
      "2018-05-11  2.37   1       \n",
      "2018-05-10  2.57   1       \n",
      "2018-05-09  2.74   1       \n",
      "2018-05-08  2.74   1       \n",
      "2018-05-07  3.01   1       \n",
      "2018-05-06  3.20   1       \n",
      "2018-05-05  3.40   1       \n",
      "2018-05-04  3.47   1       \n",
      "2018-05-03  3.44   1       \n",
      "2018-05-02  3.26   1       \n",
      "2018-05-01  3.24   1       \n",
      "2018-04-30  3.28   1       \n",
      "2018-04-29  3.50   1       \n",
      "2018-04-28  3.53   1       \n",
      "2018-04-26  3.53   1       \n",
      "2018-04-25  3.49   1       \n",
      "2018-04-24  3.42   1       \n",
      "2018-04-23  3.41   1       \n",
      "2018-04-22  3.37   1       \n",
      "2018-04-21  3.34   1       \n",
      "2018-04-20  3.33   1       \n",
      "2018-04-19  3.26   1       \n",
      "2018-04-18  3.32   1       \n",
      "2018-04-17  3.44   1       \n",
      "2018-04-16  3.46   1       \n",
      "2018-04-15  3.48   1       \n",
      "2018-04-14  3.72   1       \n",
      "2018-04-13  3.85   1       \n",
      "2018-04-12  3.88   1       \n",
      "2018-04-11  3.95   1       \n",
      "2018-04-10  3.78   1       \n",
      "2018-04-09  3.68   1       \n",
      "2018-04-08  3.54   1       \n",
      "2018-04-07  3.47   1       \n",
      "2018-04-06  3.60   1       \n",
      "2018-04-05  3.71   1       \n",
      "2018-04-04  3.48   1       \n",
      "2018-04-03  3.49   1       \n",
      "2018-04-02  3.23   1       \n",
      "2018-04-01  3.08   1       \n",
      "2018-03-31  2.89   1       \n",
      "2018-03-30  2.83   1       \n",
      "2018-03-29  2.90   1       \n",
      "2018-03-28  2.89   1       \n",
      "2018-03-27  2.92   1       \n",
      "2018-03-26  2.72   1       \n",
      "2018-03-25  2.71   1       \n",
      "2018-03-24  2.66   1       \n",
      "2018-03-23  2.67   1       \n",
      "2018-03-22  2.66   1       \n",
      "2018-03-21  2.56   1       \n",
      "2018-03-20  2.40   1       \n",
      "2018-03-19  2.42   1       \n",
      "2018-03-18  2.33   1       \n",
      "2018-03-17  2.22   1       \n",
      "2018-03-16  2.31   1       \n",
      "2018-03-15  2.27   1       \n",
      "2018-03-14  2.26   1       \n",
      "2018-03-13  2.31   1       \n",
      "2018-03-12  2.26   1       \n",
      "2018-03-11  2.24   1       \n",
      "2018-03-10  2.23   1       \n",
      "2018-03-09  2.24   1       \n",
      "2018-03-08  2.32   1       \n",
      "2018-03-07  2.33   1       \n",
      "2018-03-06  2.34   1       \n",
      "2018-03-05  2.36   1       \n",
      "2018-03-04  2.26   1       \n",
      "2018-03-03  2.22   1       \n",
      "2018-03-02  2.21   1       \n",
      "2018-03-01  2.23   1       \n",
      "2018-02-28  2.25   1       \n",
      "2018-02-27  2.23   1       \n",
      "2018-02-26  2.33   1       \n",
      "2018-02-25  2.38   1       \n",
      "2018-02-24  2.41   1       \n",
      "2018-02-23  2.48   1       \n",
      "2018-02-22  2.49   1       \n",
      "2018-02-21  2.51   1       \n",
      "2018-02-20  2.55   1       \n",
      "2018-02-19  2.73   1       \n",
      "2018-02-18  2.86   1       \n",
      "2018-02-17  2.82   1       \n",
      "2018-02-16  2.95   1       \n",
      "2018-02-15  2.91   1       \n",
      "2018-02-14  2.69   1       \n",
      "2018-02-13  2.85   1       \n",
      "2018-02-12  2.73   1       \n",
      "2018-02-11  2.34   1       \n",
      "2018-02-10  2.26   1       \n",
      "2018-02-09  2.26   1       \n",
      "2018-02-08  2.27   1       \n",
      "2018-02-07  2.30   1       \n",
      "2018-02-06  2.51   1       \n",
      "2018-02-05  2.69   1       \n",
      "2018-02-04  2.24   1       \n",
      "2018-02-03  2.30   1       \n",
      "2018-02-02  2.38   1       \n",
      "2018-02-01  2.47   1       \n",
      "2018-01-31  2.45   1       \n",
      "2018-01-30  2.48   1       \n",
      "2018-01-29  2.48   1       \n",
      "2018-01-28  2.51   1       \n",
      "2018-01-27  2.62   1       \n",
      "2018-01-26  2.65   1       \n",
      "2018-01-25  2.65   1       \n",
      "2018-01-24  2.63   1       \n",
      "2018-01-23  2.53   1       \n",
      "2018-01-22  2.48   1       \n",
      "2018-01-21  2.76   1       \n",
      "2018-01-20  2.89   1       \n",
      "2018-01-19  2.86   1       \n",
      "2018-01-18  2.65   1       \n",
      "2018-01-17  2.45   1       \n",
      "2018-01-16  2.60   1       \n",
      "2018-01-15  2.78   1       \n",
      "2018-01-14  2.61   1       \n",
      "2018-01-13  2.54   1       \n",
      "2018-01-12  2.52   1       \n",
      "2018-01-11  2.53   1       \n",
      "2018-01-10  2.32   1       \n",
      "2018-01-09  2.22   1       \n",
      "2018-01-08  2.01   1       \n",
      "2018-01-07  1.91   1       \n",
      "2018-01-06  2.00   1       \n",
      "2018-01-05  1.77   1       \n",
      "2018-01-04  1.76   1       \n",
      "2018-01-03  1.82   1       \n",
      "2018-01-02  1.73   1       \n",
      "2018-01-01  1.63   1       \n",
      "2017-12-31  1.63   1       \n",
      "2017-12-30  1.84   1       \n",
      "2017-12-29  1.90   1       \n",
      "2017-12-26  1.90   1       \n",
      "2017-12-25  1.91   1       \n",
      "2017-12-24  1.99   1       \n",
      "2017-12-22  1.99   1       \n",
      "2017-12-21  2.07   1       \n",
      "2017-12-20  2.12   1       \n",
      "2017-12-19  2.08   1       \n",
      "2017-12-15  2.08   1       \n",
      "2017-12-14  1.93   1       \n",
      "2017-12-13  1.97   1       \n",
      "2017-12-12  1.92   1       \n",
      "2017-12-11  1.87   1       \n",
      "2017-12-10  1.83   1       \n",
      "2017-12-09  1.90   1       \n",
      "2017-12-08  1.97   1       \n",
      "2017-12-07  1.92   1       \n",
      "2017-12-06  1.74   1       \n",
      "2017-12-05  1.61   1       \n",
      "2017-12-04  1.49   1       \n",
      "2017-12-03  1.41   1       \n",
      "2017-12-02  1.57   1       \n",
      "2017-12-01  1.60   1       \n",
      "2017-11-30  1.54   1       \n",
      "2017-11-29  1.53   1       \n",
      "2017-11-28  1.56   1       \n",
      "2017-11-27  1.58   1       \n",
      "2017-11-26  1.48   1       \n",
      "2017-11-25  1.33   1       \n",
      "2017-11-24  1.35   1       \n",
      "2017-11-23  1.30   1       \n",
      "2017-11-22  1.22   1       \n",
      "2017-11-21  1.21   1       \n",
      "2017-11-20  1.27   1       \n",
      "2017-11-19  1.29   1       \n",
      "2017-11-18  1.63   1       \n",
      "2017-11-17  1.49   1       \n",
      "2020-06-28  6.28   2       \n",
      "2020-06-13  6.28   2       \n",
      "2020-06-12  6.49   2       \n",
      "2020-06-11  6.57   2       \n",
      "2020-06-10  6.29   2       \n",
      "2020-06-09  6.28   2       \n",
      "2020-05-11  6.28   2       \n",
      "2020-05-10  6.32   2       \n",
      "2020-05-09  6.11   2       \n",
      "2020-05-08  5.99   2       \n",
      "2020-05-07  6.03   2       \n",
      "2020-05-06  6.35   2       \n",
      "2020-05-05  6.57   2       \n",
      "2020-04-26  6.57   2       \n",
      "2020-04-25  6.51   2       \n",
      "2020-04-24  6.28   2       \n",
      "2020-04-23  6.28   2       \n",
      "2020-04-22  6.22   2       \n",
      "2020-04-21  5.99   2       \n",
      "2020-03-30  5.99   2       \n",
      "2020-03-29  5.98   2       \n",
      "2020-03-28  4.82   2       \n",
      "2020-03-27  4.29   2       \n",
      "2020-03-26  4.30   2       \n",
      "2020-03-25  4.64   2       \n",
      "2020-03-24  4.77   2       \n",
      "2020-03-23  4.99   2       \n",
      "2020-03-22  6.31   2       \n",
      "2020-03-21  8.01   2       \n",
      "2020-03-20  8.09   2       \n",
      "2020-03-19  8.47   2       \n",
      "2020-03-18  8.56   2       \n",
      "2020-03-17  8.56   2       \n",
      "2020-03-16  8.29   2       \n",
      "2020-03-15  8.35   2       \n",
      "2020-03-14  8.45   2       \n",
      "2020-03-13  8.96   2       \n",
      "2020-03-12  9.47   2       \n",
      "2020-03-11  9.49   2       \n",
      "2020-03-10  9.49   2       \n",
      "2020-03-09  9.60   2       \n",
      "2020-03-08  9.64   2       \n",
      "2020-03-07  9.64   2       \n",
      "2020-03-06  9.70   2       \n",
      "2020-03-05  9.72   2       \n",
      "2020-03-04  9.72   2       \n",
      "2020-03-03  9.94   2       \n",
      "2020-03-02  10.03  2       \n",
      "2020-02-29  10.03  2       \n",
      "2020-02-28  10.04  2       \n",
      "2020-02-27  10.11  2       \n",
      "2020-02-23  10.11  2       \n",
      "2020-02-22  10.15  2       \n",
      "2020-02-21  10.19  2       \n",
      "2020-02-10  10.19  2       \n",
      "2020-02-09  10.21  2       \n",
      "2020-02-08  10.21  2       \n",
      "2020-02-07  10.19  2       \n",
      "2020-02-03  10.19  2       \n",
      "2020-02-02  10.78  2       \n",
      "2020-02-01  10.91  2       \n",
      "2020-01-30  10.91  2       \n",
      "2020-01-29  10.97  2       \n",
      "2020-01-28  10.96  2       \n",
      "2020-01-27  10.88  2       \n",
      "2020-01-26  10.93  2       \n",
      "2020-01-25  11.20  2       \n",
      "2020-01-24  11.23  2       \n",
      "2020-01-23  11.23  2       \n",
      "2020-01-22  11.27  2       \n",
      "2020-01-21  11.45  2       \n",
      "2020-01-20  11.47  2       \n",
      "2020-01-19  11.52  2       \n",
      "2020-01-18  11.22  2       \n",
      "2020-01-17  11.22  2       \n",
      "2020-01-16  11.14  2       \n",
      "2020-01-15  11.22  2       \n",
      "2020-01-14  11.25  2       \n",
      "2020-01-13  11.26  2       \n",
      "2020-01-12  11.15  2       \n",
      "2020-01-11  10.38  2       \n",
      "2020-01-10  10.34  2       \n",
      "2020-01-09  10.40  2       \n",
      "2020-01-08  10.89  2       \n",
      "2020-01-07  10.90  2       \n",
      "2020-01-06  10.91  2       \n",
      "2020-01-05  10.91  2       \n",
      "2020-01-04  10.93  2       \n",
      "2020-01-03  10.93  2       \n",
      "2020-01-02  10.97  2       \n",
      "2020-01-01  10.94  2       \n",
      "2019-12-31  10.94  2       \n",
      "2019-12-30  10.96  2       \n",
      "2019-12-27  10.96  2       \n",
      "2019-12-26  10.95  2       \n",
      "2019-12-25  10.90  2       \n",
      "2019-12-24  11.08  2       \n",
      "2019-12-23  10.80  2       \n",
      "2019-12-22  10.38  2       \n",
      "2019-12-21  10.63  2       \n",
      "2019-12-20  10.69  2       \n",
      "2019-12-19  10.63  2       \n",
      "2019-12-18  10.63  2       \n",
      "2019-12-17  10.60  2       \n",
      "2019-12-16  10.58  2       \n",
      "2019-12-15  10.58  2       \n",
      "2019-12-14  10.68  2       \n",
      "2019-12-13  10.86  2       \n",
      "2019-12-12  11.38  2       \n",
      "2019-12-11  11.38  2       \n",
      "2019-12-10  11.39  2       \n",
      "2019-12-09  11.40  2       \n",
      "2019-12-08  11.40  2       \n",
      "2019-12-07  11.41  2       \n",
      "2019-12-06  11.44  2       \n",
      "2019-12-05  11.44  2       \n",
      "2019-12-04  11.45  2       \n",
      "2019-12-03  11.40  2       \n",
      "2019-12-02  11.38  2       \n",
      "2019-11-27  11.38  2       \n",
      "2019-11-26  11.45  2       \n",
      "2019-11-25  11.38  2       \n",
      "2019-11-24  11.29  2       \n",
      "2019-11-23  11.49  2       \n",
      "2019-11-22  11.72  2       \n",
      "2019-11-21  11.48  2       \n",
      "2019-11-20  11.41  2       \n",
      "2019-11-19  11.48  2       \n",
      "2019-11-18  11.50  2       \n",
      "2019-11-17  11.49  2       \n",
      "2019-11-16  11.88  2       \n",
      "2019-11-15  12.29  2       \n",
      "2019-11-14  12.35  2       \n",
      "2019-11-13  11.81  2       \n",
      "2019-11-12  10.60  2       \n",
      "2019-11-11  10.29  2       \n",
      "2019-11-10  9.77   2       \n",
      "2019-11-09  9.65   2       \n",
      "2019-11-08  9.14   2       \n",
      "2019-11-07  9.18   2       \n",
      "2019-11-06  8.73   2       \n",
      "2019-11-05  8.03   2       \n",
      "2019-11-04  7.96   2       \n",
      "2019-11-03  7.83   2       \n",
      "2019-11-02  7.93   2       \n",
      "2019-11-01  7.82   2       \n",
      "2019-10-31  7.50   2       \n",
      "2019-10-30  6.35   2       \n",
      "2019-10-29  6.17   2       \n",
      "2019-10-28  6.12   2       \n",
      "2019-10-27  5.79   2       \n",
      "2019-10-26  5.75   2       \n",
      "2019-10-25  5.49   2       \n",
      "2019-10-24  4.78   2       \n",
      "2019-10-23  3.37   2       \n",
      "2019-10-22  2.77   2       \n",
      "2019-10-21  2.42   2       \n",
      "2019-10-20  2.37   2       \n",
      "2019-10-16  2.37   2       \n",
      "2019-10-15  2.40   2       \n",
      "2019-10-14  2.44   2       \n",
      "2019-10-13  2.45   2       \n",
      "2019-10-12  2.45   2       \n",
      "2019-10-11  2.42   2       \n",
      "2019-10-10  2.41   2       \n",
      "2019-10-08  2.41   2       \n",
      "2019-10-07  2.43   2       \n",
      "2019-10-06  2.44   2       \n",
      "2019-10-03  2.44   2       \n",
      "2019-10-02  2.60   2       \n",
      "2019-10-01  2.79   2       \n",
      "2019-09-26  2.79   2       \n",
      "2019-09-25  2.99   2       \n",
      "2019-09-24  3.09   2       \n",
      "2019-09-23  3.09   2       \n",
      "2019-09-22  3.02   2       \n",
      "2019-09-21  2.96   2       \n",
      "2019-09-14  2.96   2       \n",
      "2019-09-13  2.97   2       \n",
      "2019-09-12  2.96   2       \n",
      "2019-09-11  2.96   2       \n",
      "2019-09-10  2.97   2       \n",
      "2019-09-09  3.00   2       \n",
      "2019-09-08  2.99   2       \n",
      "2019-09-06  2.99   2       \n",
      "2019-09-05  2.94   2       \n",
      "2019-09-04  2.95   2       \n",
      "2019-09-03  2.95   2       \n",
      "2019-09-02  3.04   2       \n",
      "2019-09-01  3.12   2       \n",
      "2019-08-31  3.12   2       \n",
      "2019-08-30  3.01   2       \n",
      "2019-08-29  2.97   2       \n",
      "2019-08-28  2.96   2       \n",
      "2019-08-27  2.92   2       \n",
      "2019-08-26  2.92   2       \n",
      "2019-08-25  2.91   2       \n",
      "2019-08-23  2.91   2       \n",
      "2019-08-22  2.89   2       \n",
      "2019-08-21  2.88   2       \n",
      "2019-08-19  2.88   2       \n",
      "2019-08-18  2.90   2       \n",
      "2019-08-16  2.90   2       \n",
      "2019-08-15  2.93   2       \n",
      "2019-08-11  2.93   2       \n",
      "2019-08-10  2.89   2       \n",
      "2019-08-09  2.87   2       \n",
      "2019-08-08  2.87   2       \n",
      "2019-08-07  2.88   2       \n",
      "2019-08-06  2.98   2       \n",
      "2019-08-05  3.08   2       \n",
      "2019-08-04  3.09   2       \n",
      "2019-08-03  3.19   2       \n",
      "2019-08-01  3.19   2       \n",
      "2019-07-31  3.20   2       \n",
      "2019-07-30  3.20   2       \n",
      "2019-07-29  3.21   2       \n",
      "2019-07-28  3.21   2       \n",
      "2019-07-27  3.46   2       \n",
      "2019-07-26  3.54   2       \n",
      "2019-07-25  3.49   2       \n",
      "2019-07-21  3.49   2       \n",
      "2019-07-20  3.50   2       \n",
      "2019-07-17  3.50   2       \n",
      "2019-07-16  3.97   2       \n",
      "2019-07-15  4.22   2       \n",
      "2019-07-14  4.19   2       \n",
      "2019-07-12  4.19   2       \n",
      "2019-07-11  4.21   2       \n",
      "2019-07-10  4.19   2       \n",
      "2019-07-09  4.18   2       \n",
      "2019-07-08  4.17   2       \n",
      "2019-07-07  4.14   2       \n",
      "2019-07-06  4.07   2       \n",
      "2019-07-05  4.08   2       \n",
      "2019-07-04  4.03   2       \n",
      "2019-07-03  4.03   2       \n",
      "2019-07-02  3.99   2       \n",
      "2019-07-01  3.99   2       \n",
      "2019-06-30  3.56   2       \n",
      "2019-06-29  3.30   2       \n",
      "2019-06-28  3.30   2       \n",
      "2019-06-27  3.37   2       \n",
      "2019-06-26  3.52   2       \n",
      "2019-06-25  3.55   2       \n",
      "2019-06-24  3.54   2       \n",
      "2019-06-23  3.49   2       \n",
      "2019-06-22  3.33   2       \n",
      "2019-06-21  3.29   2       \n",
      "2019-06-20  3.23   2       \n",
      "2019-06-19  3.07   2       \n",
      "2019-06-18  3.07   2       \n",
      "2019-06-17  3.03   2       \n",
      "2019-06-16  2.82   2       \n",
      "2019-06-15  2.82   2       \n",
      "2019-06-14  2.73   2       \n",
      "2019-06-13  2.67   2       \n",
      "2019-06-12  2.62   2       \n",
      "2019-06-11  2.75   2       \n",
      "2019-06-10  2.79   2       \n",
      "2019-06-09  2.78   2       \n",
      "2019-06-08  2.73   2       \n",
      "2019-06-07  2.72   2       \n",
      "2019-06-06  2.71   2       \n",
      "2019-06-05  2.68   2       \n",
      "2019-06-04  2.69   2       \n",
      "2019-06-03  2.70   2       \n",
      "2019-06-02  2.69   2       \n",
      "2019-06-01  2.71   2       \n",
      "2019-05-30  2.71   2       \n",
      "2019-05-29  2.72   2       \n",
      "2019-05-28  2.72   2       \n",
      "2019-05-27  2.67   2       \n",
      "2019-05-26  2.62   2       \n",
      "2019-05-25  2.61   2       \n",
      "2019-05-24  2.59   2       \n",
      "2019-05-23  2.55   2       \n",
      "2019-05-19  2.55   2       \n",
      "2019-05-18  2.56   2       \n",
      "2019-05-16  2.56   2       \n",
      "2019-05-15  2.55   2       \n",
      "2019-05-14  2.54   2       \n",
      "2019-05-13  2.51   2       \n",
      "2019-05-12  2.41   2       \n",
      "2019-05-11  2.28   2       \n",
      "2019-05-10  2.20   2       \n",
      "2019-05-07  2.20   2       \n",
      "2019-05-06  2.19   2       \n",
      "2019-05-05  2.19   2       \n",
      "2019-05-04  2.17   2       \n",
      "2019-05-03  2.19   2       \n",
      "2019-05-02  2.43   2       \n",
      "2019-05-01  2.42   2       \n",
      "2019-04-30  2.44   2       \n",
      "2019-04-29  2.40   2       \n",
      "2019-04-28  2.05   2       \n",
      "2019-04-27  1.83   2       \n",
      "2019-04-26  1.80   2       \n",
      "2019-04-25  1.76   2       \n",
      "2019-04-24  1.76   2       \n",
      "2019-04-23  1.77   2       \n",
      "2019-04-22  1.77   2       \n",
      "2019-04-21  1.74   2       \n",
      "2019-04-20  1.71   2       \n",
      "2019-04-19  1.74   2       \n",
      "2019-04-15  1.74   2       \n",
      "2019-04-14  1.72   2       \n",
      "2019-04-13  1.64   2       \n",
      "2019-04-12  1.55   2       \n",
      "2019-04-11  1.55   2       \n",
      "2019-04-10  1.59   2       \n",
      "2019-04-09  1.76   2       \n",
      "2019-04-08  1.71   2       \n",
      "2019-04-07  1.56   2       \n",
      "2019-04-06  1.51   2       \n",
      "2019-04-05  1.50   2       \n",
      "2019-04-04  1.49   2       \n",
      "2019-04-03  1.48   2       \n",
      "2019-04-02  1.47   2       \n",
      "2019-04-01  1.46   2       \n",
      "2019-03-31  1.43   2       \n",
      "2019-03-30  1.42   2       \n",
      "2019-03-29  1.43   2       \n",
      "2019-03-28  1.39   2       \n",
      "2019-03-27  1.40   2       \n",
      "2019-03-26  1.34   2       \n",
      "2019-03-25  1.32   2       \n",
      "2019-03-24  1.29   2       \n",
      "2019-03-23  1.28   2       \n",
      "2019-03-22  1.27   2       \n",
      "2019-03-21  1.25   2       \n",
      "2019-03-20  1.25   2       \n",
      "2019-03-19  1.21   2       \n",
      "2019-03-18  1.12   2       \n",
      "2019-03-17  1.11   2       \n",
      "2019-03-16  1.05   2       \n",
      "2019-03-15  1.04   2       \n",
      "2019-03-14  1.00   2       \n",
      "2019-03-13  0.93   2       \n",
      "2019-03-12  0.92   2       \n",
      "2019-03-11  0.93   2       \n",
      "2019-03-10  0.91   2       \n",
      "2019-03-09  0.90   2       \n",
      "2019-03-08  0.85   2       \n",
      "2019-03-07  0.96   2       \n",
      "2019-03-06  0.92   2       \n",
      "2019-03-05  0.83   2       \n",
      "2019-03-04  0.83   2       \n",
      "2019-03-03  0.81   2       \n",
      "2019-03-02  0.72   2       \n",
      "2019-03-01  0.74   2       \n",
      "2019-02-28  0.81   2       \n",
      "2019-02-27  0.73   2       \n",
      "2019-02-26  0.72   2       \n",
      "2019-02-25  0.71   2       \n",
      "2019-02-24  0.68   2       \n",
      "2019-02-23  0.62   2       \n",
      "2019-02-22  0.55   2       \n",
      "2019-02-21  0.41   2       \n",
      "2019-02-20  0.43   2       \n",
      "2019-02-16  0.43   2       \n",
      "2019-02-15  0.67   2       \n",
      "2019-02-14  0.85   2       \n",
      "2019-02-13  0.93   2       \n",
      "2019-02-12  0.88   2       \n",
      "2019-02-11  0.86   2       \n",
      "2019-02-10  0.85   2       \n",
      "2019-02-09  0.86   2       \n",
      "2019-02-08  0.82   2       \n",
      "2019-02-07  0.80   2       \n",
      "2019-02-03  0.80   2       \n",
      "2019-02-02  0.83   2       \n",
      "2019-02-01  0.90   2       \n",
      "2019-01-31  0.91   2       \n",
      "2019-01-27  0.91   2       \n",
      "2019-01-26  1.11   2       \n",
      "2019-01-25  1.18   2       \n",
      "2019-01-24  0.99   2       \n",
      "2019-01-23  1.30   2       \n",
      "2019-01-22  1.44   2       \n",
      "2019-01-21  1.46   2       \n",
      "2019-01-20  1.51   2       \n",
      "2019-01-19  1.51   2       \n",
      "2019-01-18  1.52   2       \n",
      "2019-01-17  1.55   2       \n",
      "2019-01-16  1.55   2       \n",
      "2019-01-15  1.56   2       \n",
      "2019-01-14  1.54   2       \n",
      "2019-01-13  1.41   2       \n",
      "2019-01-12  1.39   2       \n",
      "2019-01-11  1.29   2       \n",
      "2019-01-10  1.29   2       \n",
      "2019-01-09  1.35   2       \n",
      "2019-01-08  1.29   2       \n",
      "2019-01-07  1.20   2       \n",
      "2019-01-06  1.26   2       \n",
      "2019-01-03  1.26   2       \n",
      "2019-01-02  1.31   2       \n",
      "2019-01-01  1.38   2       \n",
      "2018-12-31  1.38   2       \n",
      "2018-12-30  1.40   2       \n",
      "2018-12-29  1.48   2       \n",
      "2018-12-27  1.48   2       \n",
      "2018-12-26  1.49   2       \n",
      "2018-12-24  1.49   2       \n",
      "2018-12-23  1.50   2       \n",
      "2018-12-22  1.50   2       \n",
      "2018-12-21  2.03   2       \n",
      "2018-12-20  2.29   2       \n",
      "2018-12-19  2.29   2       \n",
      "2018-12-18  2.28   2       \n",
      "2018-12-17  2.38   2       \n",
      "2018-12-16  2.38   2       \n",
      "2018-12-15  2.41   2       \n",
      "2018-12-14  2.52   2       \n",
      "2018-12-13  2.52   2       \n",
      "2018-12-12  2.53   2       \n",
      "2018-12-11  2.62   2       \n",
      "2018-12-08  2.62   2       \n",
      "2018-12-07  2.64   2       \n",
      "2018-12-06  2.65   2       \n",
      "2018-12-04  2.65   2       \n",
      "2018-12-03  2.61   2       \n",
      "2018-12-02  2.51   2       \n",
      "2018-12-01  2.51   2       \n",
      "2018-11-30  2.52   2       \n",
      "2018-11-25  2.52   2       \n",
      "2018-11-24  2.64   2       \n",
      "2018-11-23  2.66   2       \n",
      "2018-11-22  2.64   2       \n",
      "2018-11-21  2.62   2       \n",
      "2018-11-16  2.62   2       \n",
      "2018-11-15  2.69   2       \n",
      "2018-11-14  2.76   2       \n",
      "2018-11-13  2.78   2       \n",
      "2018-11-12  2.80   2       \n",
      "2018-11-10  2.80   2       \n",
      "2018-11-09  2.77   2       \n",
      "2018-11-08  2.86   2       \n",
      "2018-11-07  2.89   2       \n",
      "2018-11-06  2.72   2       \n",
      "2018-11-02  2.72   2       \n",
      "2018-11-01  2.77   2       \n",
      "2018-10-31  2.80   2       \n",
      "2018-10-30  2.80   2       \n",
      "2018-10-29  2.89   2       \n",
      "2018-10-28  2.93   2       \n",
      "2018-10-27  3.10   2       \n",
      "2018-10-26  3.11   2       \n",
      "2018-10-25  3.11   2       \n",
      "2018-10-24  3.30   2       \n",
      "2018-10-23  3.31   2       \n",
      "2018-10-22  3.31   2       \n",
      "2018-10-21  3.35   2       \n",
      "2018-10-19  3.35   2       \n",
      "2018-10-18  3.32   2       \n",
      "2018-10-17  3.31   2       \n",
      "2018-10-13  3.31   2       \n",
      "2018-10-12  3.32   2       \n",
      "2018-10-11  3.34   2       \n",
      "2018-10-08  3.34   2       \n",
      "2018-10-07  3.35   2       \n",
      "2018-10-06  3.35   2       \n",
      "2018-10-05  3.38   2       \n",
      "2018-10-04  3.50   2       \n",
      "2018-10-02  3.50   2       \n",
      "2018-10-01  3.48   2       \n",
      "2018-09-28  3.48   2       \n",
      "2018-09-27  3.49   2       \n",
      "2018-09-26  3.52   2       \n",
      "2018-09-25  3.53   2       \n",
      "2018-09-24  3.55   2       \n",
      "2018-09-23  3.55   2       \n",
      "2018-09-22  3.36   2       \n",
      "2018-09-21  3.34   2       \n",
      "2018-09-14  3.34   2       \n",
      "2018-09-13  3.33   2       \n",
      "2018-09-12  3.33   2       \n",
      "2018-09-11  3.30   2       \n",
      "2018-09-10  3.29   2       \n",
      "2018-09-09  3.37   2       \n",
      "2018-09-08  3.43   2       \n",
      "2018-09-07  3.40   2       \n",
      "2018-09-03  3.40   2       \n",
      "2018-09-02  3.38   2       \n",
      "2018-09-01  3.37   2       \n",
      "2018-08-31  3.37   2       \n",
      "2018-08-30  3.38   2       \n",
      "2018-08-29  3.38   2       \n",
      "2019-11-30  7.50   0       \n",
      "2019-11-29  7.64   0       \n",
      "2019-11-28  8.31   0       \n",
      "2019-11-27  7.63   0       \n",
      "2019-11-26  7.24   0       \n",
      "2019-11-25  7.22   0       \n",
      "2019-11-24  7.11   0       \n",
      "2019-11-23  7.27   0       \n",
      "2019-11-22  7.10   0       \n",
      "2019-11-21  6.28   0       \n",
      "2019-11-20  6.90   0       \n",
      "2019-11-19  6.39   0       \n",
      "2019-11-18  7.83   0       \n",
      "2019-11-17  8.29   0       \n",
      "2019-11-16  9.46   0       \n",
      "2019-11-15  11.14  0       \n",
      "2019-11-14  12.08  0       \n",
      "2019-11-13  11.58  0       \n",
      "2019-11-12  11.17  0       \n",
      "2019-11-11  9.68   0       \n",
      "2019-11-10  9.34   0       \n",
      "2019-11-09  9.47   0       \n",
      "2019-11-08  8.42   0       \n",
      "2019-11-07  6.97   0       \n",
      "2019-11-06  7.10   0       \n",
      "2019-11-05  5.90   0       \n",
      "2019-11-04  5.67   0       \n",
      "2019-11-03  6.64   0       \n",
      "2019-11-02  7.34   0       \n",
      "2019-11-01  7.55   0       \n",
      "2019-10-31  7.56   0       \n",
      "2019-10-30  6.08   0       \n",
      "2019-10-29  6.01   0       \n",
      "2019-10-28  5.98   0       \n",
      "2019-10-27  5.57   0       \n",
      "2019-10-26  4.47   0       \n",
      "2019-10-25  3.90   0       \n",
      "2019-10-24  4.46   0       \n",
      "2019-10-23  3.53   0       \n",
      "2019-10-22  2.84   0       \n",
      "2019-10-21  1.31   0       \n",
      "2019-10-20  1.33   0       \n",
      "2019-10-19  1.39   0       \n",
      "2019-10-18  1.30   0       \n",
      "2019-10-17  1.27   0       \n",
      "2019-10-16  1.27   0       \n",
      "2019-10-15  1.38   0       \n",
      "2019-10-14  1.43   0       \n",
      "2019-10-13  0.94   0       \n",
      "2019-10-12  0.90   0       \n",
      "2019-10-11  0.80   0       \n",
      "2019-10-10  0.78   0       \n",
      "2019-10-09  0.94   0       \n",
      "2019-10-08  0.82   0       \n",
      "2019-10-07  0.91   0       \n",
      "2019-10-06  0.92   0       \n",
      "2019-10-05  0.99   0       \n",
      "2019-10-04  0.93   0       \n",
      "2019-10-03  0.99   0       \n",
      "2019-10-02  1.16   0       \n",
      "2019-10-01  1.16   0       \n",
      "2019-09-30  1.00   0       \n",
      "2019-09-29  1.16   0       \n",
      "2019-09-28  1.34   0       \n",
      "2019-09-27  1.38   0       \n",
      "2019-09-26  1.23   0       \n",
      "2019-09-25  1.29   0       \n",
      "2019-09-24  1.35   0       \n",
      "2019-09-23  1.32   0       \n",
      "2019-09-22  1.29   0       \n",
      "2019-09-21  1.21   0       \n",
      "2019-09-20  1.46   0       \n",
      "2019-09-19  1.37   0       \n",
      "2019-09-18  1.38   0       \n",
      "2019-09-17  1.56   0       \n",
      "2019-09-16  1.54   0       \n",
      "2019-09-15  1.46   0       \n",
      "2019-09-14  1.36   0       \n",
      "2019-09-13  1.22   0       \n",
      "2019-09-12  1.24   0       \n",
      "2019-09-11  1.42   0       \n",
      "2019-09-10  1.43   0       \n",
      "2019-09-09  1.44   0       \n",
      "2019-09-08  1.48   0       \n",
      "2019-09-07  1.66   0       \n",
      "2019-09-06  1.65   0       \n",
      "2018-04-13  4.05   2       \n",
      "2018-04-12  4.02   2       \n",
      "2018-04-11  3.99   2       \n",
      "2018-04-10  3.98   2       \n",
      "2018-04-09  3.87   2       \n",
      "2018-04-08  3.84   2       \n",
      "2018-04-07  3.84   2       \n",
      "2018-04-06  3.73   2       \n",
      "2018-04-05  3.67   2       \n",
      "2018-04-04  3.37   2       \n",
      "2018-04-03  3.33   2       \n",
      "2018-04-02  3.29   2       \n",
      "2018-04-01  3.27   2       \n",
      "2018-03-31  3.16   2       \n",
      "2018-03-30  3.11   2       \n",
      "2018-03-29  2.97   2       \n",
      "2018-03-28  2.91   2       \n",
      "2018-03-27  2.92   2       \n",
      "2018-03-26  2.95   2       \n",
      "2018-03-25  2.95   2       \n",
      "2018-03-24  2.94   2       \n",
      "2018-03-23  2.92   2       \n",
      "2018-03-22  2.89   2       \n",
      "2018-03-21  2.91   2       \n",
      "2018-03-20  2.90   2       \n",
      "2018-03-19  2.92   2       \n",
      "2018-03-18  2.89   2       \n",
      "2018-03-17  2.88   2       \n",
      "2018-03-16  2.86   2       \n",
      "2018-03-14  2.86   2       \n",
      "2018-03-13  2.97   2       \n",
      "2018-03-12  2.99   2       \n",
      "2018-03-11  3.00   2       \n",
      "2018-03-10  2.99   2       \n",
      "2018-03-09  2.97   2       \n",
      "2018-03-05  2.97   2       \n",
      "2018-03-04  2.99   2       \n",
      "2018-03-03  3.01   2       \n",
      "2018-03-02  3.12   2       \n",
      "2018-03-01  3.20   2       \n",
      "2018-02-28  3.38   2       \n",
      "2018-02-26  3.38   2       \n",
      "2018-02-25  3.39   2       \n",
      "2018-02-21  3.39   2       \n",
      "2018-02-20  3.40   2       \n",
      "2018-02-19  3.41   2       \n",
      "2018-02-18  3.37   2       \n",
      "2018-02-17  3.30   2       \n",
      "2018-02-16  2.97   2       \n",
      "2018-02-15  2.76   2       \n",
      "2018-02-13  2.76   2       \n",
      "2018-02-12  2.73   2       \n",
      "2018-02-11  2.72   2       \n",
      "2018-02-06  2.72   2       \n",
      "2018-02-05  2.92   2       \n",
      "2018-02-04  2.94   2       \n",
      "2018-02-03  2.96   2       \n",
      "2018-02-01  2.96   2       \n",
      "2018-01-31  3.02   2       \n",
      "2018-01-30  3.03   2       \n",
      "2018-01-29  3.03   2       \n",
      "2018-01-28  3.17   2       \n",
      "2018-01-27  3.21   2       \n",
      "2018-01-26  3.21   2       \n",
      "2018-01-25  3.18   2       \n",
      "2018-01-24  3.37   2       \n",
      "2018-01-23  3.50   2       \n",
      "2018-01-22  3.83   2       \n",
      "2018-01-21  3.90   2       \n",
      "2018-01-20  3.91   2       \n",
      "2018-01-19  4.07   2       \n",
      "2018-01-18  4.14   2       \n",
      "2018-01-17  4.14   2       \n",
      "2018-01-16  4.29   2       \n",
      "2018-01-15  4.33   2       \n",
      "2018-01-14  4.32   2       \n",
      "2018-01-13  4.32   2       \n",
      "2018-01-12  4.45   2       \n",
      "2018-01-11  4.53   2       \n",
      "2018-01-10  4.68   2       \n",
      "2018-01-09  4.69   2       \n",
      "2018-01-08  4.69   2       \n",
      "2018-01-07  4.67   2       \n",
      "2018-01-06  4.66   2       \n",
      "2017-12-29  4.66   2       \n",
      "2017-12-28  4.65   2       \n",
      "2017-12-24  4.65   2       \n",
      "2017-12-23  4.62   2       \n",
      "2017-12-22  4.61   2       \n",
      "2017-12-21  4.67   2       \n",
      "2017-12-20  4.83   2       \n",
      "2017-12-17  4.83   2       \n",
      "2017-12-16  4.85   2       \n",
      "2017-12-15  4.87   2       \n",
      "2017-12-14  4.90   2       \n",
      "2017-12-12  4.90   2       \n",
      "2017-12-11  4.70   2       \n",
      "2017-12-10  4.66   2       \n",
      "2017-12-07  4.66   2       \n",
      "2017-12-06  4.44   2       \n",
      "2017-12-05  4.44   2       \n",
      "2017-12-04  4.45   2       \n",
      "2017-12-02  4.45   2       \n",
      "2017-12-01  4.46   2       \n",
      "2017-11-29  4.46   2       \n",
      "2017-11-28  4.50   2       \n",
      "2017-11-27  4.50   2       \n",
      "2017-11-26  4.51   2       \n",
      "2017-11-25  4.49   2       \n",
      "2017-11-24  4.48   2       \n",
      "2017-11-23  4.50   2       \n",
      "2017-11-18  4.50   2       \n",
      "2017-11-17  4.47   2       \n",
      "2017-11-15  4.47   2       \n",
      "2017-11-14  4.48   2       \n",
      "2017-11-13  4.49   2       \n",
      "2017-11-12  4.49   2       \n",
      "2017-11-11  4.48   2       \n",
      "2017-11-08  4.48   2       \n",
      "2017-11-07  4.47   2       \n",
      "2017-11-06  4.44   2       \n",
      "2017-11-05  4.46   2       \n",
      "2017-11-04  4.47   2       \n",
      "2017-11-03  4.47   2       \n",
      "2017-11-02  4.46   2       \n",
      "2017-11-01  4.43   2       \n",
      "2017-10-30  4.43   2       \n",
      "2017-10-29  4.42   2       \n",
      "2017-10-28  4.53   2       \n",
      "2017-10-27  4.35   2       \n",
      "2017-10-26  4.32   2       \n",
      "2017-10-22  4.32   2       \n",
      "2017-10-21  4.28   2       \n",
      "2017-10-10  4.28   2       \n",
      "2017-10-09  4.32   2       \n",
      "2017-10-08  4.30   2       \n",
      "2017-10-07  4.28   2       \n",
      "2017-10-06  4.30   2       \n",
      "2017-10-04  4.30   2       \n",
      "2017-10-03  4.35   2       \n",
      "2017-10-02  4.66   2       \n",
      "2017-10-01  4.69   2       \n",
      "2017-09-30  4.69   2       \n",
      "2017-09-29  4.70   2       \n",
      "2017-09-28  4.75   2       \n",
      "2017-09-27  4.78   2       \n",
      "2017-09-26  4.72   2       \n",
      "2017-09-25  4.72   2       \n",
      "2017-09-24  4.60   2       \n",
      "2017-09-23  4.54   2       \n",
      "2017-09-22  4.34   2       \n",
      "2017-09-21  4.33   2       \n",
      "2017-09-20  4.35   2       \n",
      "2017-09-19  4.32   2       \n",
      "2017-09-18  4.35   2       \n",
      "2017-09-17  4.35   2       \n",
      "2017-09-16  4.36   2       \n",
      "2017-09-14  4.36   2       \n",
      "2017-09-13  4.35   2       \n",
      "2017-09-12  4.35   2       \n",
      "2017-09-11  4.34   2       \n",
      "2017-09-10  4.34   2       \n",
      "2017-09-09  4.19   2       \n",
      "2017-09-08  4.23   2       \n",
      "2017-09-07  4.27   2       \n",
      "2017-09-06  4.26   2       \n",
      "2017-09-03  4.26   2       \n",
      "2017-09-02  4.25   2       \n",
      "2017-08-30  4.25   2       \n",
      "2017-08-29  4.26   2       \n",
      "2017-08-28  4.25   2       \n",
      "2017-08-26  4.25   2       \n",
      "2017-08-25  4.24   2       \n",
      "2017-08-22  4.24   2       \n",
      "2017-08-21  4.23   2       \n",
      "2017-08-20  4.38   2       \n",
      "2017-08-18  4.38   2       \n",
      "2017-08-17  4.39   2       \n",
      "2017-08-15  4.39   2       \n",
      "2017-08-14  4.37   2       \n",
      "2017-08-13  4.53   2       \n",
      "2017-08-11  4.53   2       \n",
      "2017-08-10  4.51   2       \n",
      "2017-08-09  4.46   2       \n",
      "2017-08-08  4.45   2       \n",
      "2017-08-07  4.45   2       \n",
      "2017-08-06  4.44   2       \n",
      "2017-08-05  4.43   2       \n",
      "2017-08-04  4.57   2       \n",
      "2017-08-03  4.41   2       \n",
      "2017-08-02  4.37   2       \n",
      "2017-08-01  4.37   2       \n",
      "2017-07-31  4.25   2       \n",
      "2017-07-30  4.24   2       \n",
      "2017-07-29  4.22   2       \n",
      "2017-07-28  4.10   2       \n",
      "2017-07-27  4.04   2       \n",
      "2017-07-26  4.06   2       \n",
      "2017-07-25  4.24   2       \n",
      "2017-07-24  4.18   2       \n",
      "2017-07-23  4.08   2       \n",
      "2017-07-22  3.83   2       \n",
      "2017-07-21  3.62   2       \n",
      "2017-07-20  3.72   2       \n",
      "2017-07-19  3.70   2       \n",
      "2017-07-18  3.74   2       \n",
      "2017-07-17  3.71   2       \n",
      "2017-07-16  3.73   2       \n",
      "2017-07-15  3.53   2       \n",
      "2017-07-14  3.52   2       \n",
      "2017-07-13  3.51   2       \n",
      "2017-07-12  3.51   2       \n",
      "2017-07-11  3.52   2       \n",
      "2017-07-10  3.55   2       \n",
      "2017-07-09  3.49   2       \n",
      "2017-07-08  3.56   2       \n",
      "2017-07-07  3.61   2       \n",
      "2017-07-06  3.77   2       \n",
      "2017-07-05  3.70   2       \n",
      "2017-07-04  3.41   2       \n",
      "2017-07-03  3.28   2       \n",
      "2017-07-02  3.26   2       \n",
      "2017-07-01  3.26   2       \n",
      "2017-06-30  3.10   2       \n",
      "2017-06-29  3.05   2       \n",
      "2020-06-28  5.28   3       \n",
      "2020-06-27  5.40   3       \n",
      "2020-06-26  5.87   3       \n",
      "2020-06-03  5.87   3       \n",
      "2020-06-02  5.51   3       \n",
      "2020-06-01  5.21   3       \n",
      "2020-05-23  5.21   3       \n",
      "2020-05-22  5.36   3       \n",
      "2020-05-21  5.73   3       \n",
      "2020-05-20  5.73   3       \n",
      "2020-05-19  5.37   3       \n",
      "2020-05-18  5.15   3       \n",
      "2020-05-17  5.53   3       \n",
      "2020-05-15  5.53   3       \n",
      "2020-05-14  6.17   3       \n",
      "2020-05-13  6.26   3       \n",
      "2020-05-12  6.84   3       \n",
      "2020-05-11  6.99   3       \n",
      "2020-05-10  6.99   3       \n",
      "2020-05-09  6.35   3       \n",
      "2020-05-08  6.26   3       \n",
      "2020-04-12  6.26   3       \n",
      "2020-04-11  6.78   3       \n",
      "2020-04-10  6.99   3       \n",
      "2020-03-30  6.99   3       \n",
      "2020-03-29  6.97   3       \n",
      "2020-03-28  6.72   3       \n",
      "2020-03-26  6.72   3       \n",
      "2020-03-25  6.69   3       \n",
      "2020-03-24  5.99   3       \n",
      "2020-03-14  5.99   3       \n",
      "2020-03-13  5.69   3       \n",
      "2020-03-12  5.39   3       \n",
      "2020-03-10  5.39   3       \n",
      "2020-03-09  6.67   3       \n",
      "2020-03-08  8.05   3       \n",
      "2020-02-11  8.05   3       \n",
      "2020-02-10  8.13   3       \n",
      "2020-02-09  8.88   3       \n",
      "2020-02-08  9.04   3       \n",
      "2020-01-28  9.04   3       \n",
      "2020-01-27  8.83   3       \n",
      "2020-01-26  8.05   3       \n",
      "2020-01-22  8.05   3       \n",
      "2020-01-21  8.13   3       \n",
      "2020-01-20  9.04   3       \n",
      "2020-01-18  9.04   3       \n",
      "2020-01-17  9.50   3       \n",
      "2020-01-16  10.04  3       \n",
      "2020-01-13  10.04  3       \n",
      "2020-01-12  9.87   3       \n",
      "2020-01-11  8.05   3       \n",
      "2020-01-05  8.05   3       \n",
      "2020-01-04  7.81   3       \n",
      "2020-01-03  7.32   3       \n",
      "2020-01-01  7.32   3       \n",
      "2019-12-31  7.75   3       \n",
      "2019-12-30  9.04   3       \n",
      "2019-12-29  9.04   3       \n",
      "2019-12-28  9.92   3       \n",
      "2019-12-27  10.04  3       \n",
      "2019-12-25  10.04  3       \n",
      "2019-12-24  10.46  3       \n",
      "2019-12-23  11.04  3       \n",
      "2019-12-22  9.68   3       \n",
      "2019-12-21  8.55   3       \n",
      "2019-12-20  7.77   3       \n",
      "2019-12-18  7.77   3       \n",
      "2019-12-17  7.56   3       \n",
      "2019-12-16  7.04   3       \n",
      "2019-12-15  7.04   3       \n",
      "2019-12-14  7.25   3       \n",
      "2019-12-13  7.77   3       \n",
      "2019-12-08  7.77   3       \n",
      "2019-12-07  8.32   3       \n",
      "2019-12-06  8.50   3       \n",
      "2019-12-04  8.50   3       \n",
      "2019-12-03  8.54   3       \n",
      "2019-12-02  9.49   3       \n",
      "2019-12-01  9.49   3       \n",
      "2019-11-30  9.21   3       \n",
      "2019-11-29  7.79   3       \n",
      "2019-11-25  7.79   3       \n",
      "2019-11-24  8.52   3       \n",
      "2019-11-23  8.44   3       \n",
      "2019-11-22  7.96   3       \n",
      "2019-11-21  6.47   3       \n",
      "2019-11-20  6.22   3       \n",
      "2019-11-19  6.80   3       \n",
      "2019-11-18  6.80   3       \n",
      "2019-11-17  7.32   3       \n",
      "2019-11-16  7.53   3       \n",
      "2019-11-11  7.53   3       \n",
      "2019-11-10  7.44   3       \n",
      "2019-11-09  6.74   3       \n",
      "2019-11-08  6.74   3       \n",
      "2019-11-07  6.72   3       \n",
      "2019-11-06  6.14   3       \n",
      "2019-11-05  6.14   3       \n",
      "2019-11-04  6.00   3       \n",
      "2019-11-03  5.03   3       \n",
      "2019-11-02  4.57   3       \n",
      "2019-11-01  4.57   3       \n",
      "2019-10-31  4.79   3       \n",
      "2019-10-30  4.10   3       \n",
      "2019-10-29  4.35   3       \n",
      "2019-10-28  4.12   3       \n",
      "2019-10-27  3.80   3       \n",
      "2019-10-26  3.86   3       \n",
      "2019-10-25  3.99   3       \n",
      "2019-10-24  3.44   3       \n",
      "2019-10-23  2.48   3       \n",
      "2019-10-22  2.25   3       \n",
      "2019-10-21  1.83   3       \n",
      "2019-10-20  1.77   3       \n",
      "2019-09-19  1.77   3       \n",
      "2019-09-18  1.62   3       \n",
      "2019-09-17  1.55   3       \n",
      "2019-09-08  1.55   3       \n",
      "2019-09-07  1.77   3       \n",
      "2019-08-03  1.77   3       \n",
      "2019-08-02  1.83   3       \n",
      "2019-08-01  1.99   3       \n",
      "2019-07-24  1.99   3       \n",
      "2019-07-23  2.05   3       \n",
      "2019-07-22  2.28   3       \n",
      "2019-07-21  2.28   3       \n",
      "2019-07-20  2.20   3       \n",
      "2019-07-19  1.99   3       \n",
      "2019-07-01  1.99   3       \n",
      "2019-06-30  1.85   3       \n",
      "2019-06-29  1.77   3       \n",
      "2019-06-27  1.77   3       \n",
      "2019-06-26  1.82   3       \n",
      "2019-06-25  1.99   3       \n",
      "2019-06-23  1.99   3       \n",
      "2019-06-22  1.62   3       \n",
      "2019-06-21  1.55   3       \n",
      "2019-06-09  1.55   3       \n",
      "2019-06-08  1.64   3       \n",
      "2019-06-07  1.77   3       \n",
      "2019-06-06  1.65   3       \n",
      "2019-06-05  1.33   3       \n",
      "2019-05-31  1.33   3       \n",
      "2019-05-30  1.15   3       \n",
      "2019-05-29  1.11   3       \n",
      "2019-05-26  1.11   3       \n",
      "2019-05-25  1.23   3       \n",
      "2019-05-24  1.30   3       \n",
      "2019-04-29  1.30   3       \n",
      "2019-04-28  1.48   3       \n",
      "2019-04-27  1.52   3       \n",
      "2019-04-24  1.52   3       \n",
      "2019-04-23  1.57   3       \n",
      "2019-04-22  1.74   3       \n",
      "2019-04-20  1.74   3       \n",
      "2019-04-19  1.52   3       \n",
      "2019-04-14  1.52   3       \n",
      "2019-04-13  1.33   3       \n",
      "2019-04-08  1.33   3       \n",
      "2019-04-07  1.14   3       \n",
      "2019-04-05  1.14   3       \n",
      "2019-04-04  0.98   3       \n",
      "2019-03-30  0.98   3       \n",
      "2019-03-29  0.84   3       \n",
      "2019-03-24  0.84   3       \n",
      "2019-03-23  0.72   3       \n",
      "2019-03-20  0.72   3       \n",
      "2019-03-19  0.63   3       \n",
      "2019-03-18  0.60   3       \n",
      "2019-03-17  0.62   3       \n",
      "2019-03-16  0.72   3       \n",
      "2019-03-12  0.72   3       \n",
      "2019-03-11  0.71   3       \n",
      "2019-03-10  0.60   3       \n",
      "2019-03-09  0.46   3       \n",
      "2019-03-08  0.40   3       \n",
      "2019-03-03  0.40   3       \n",
      "2019-03-02  0.33   3       \n",
      "2019-03-01  0.31   3       \n",
      "2019-02-28  0.21   3       \n",
      "2019-02-27  0.23   3       \n",
      "2019-02-26  0.23   3       \n",
      "2019-02-25  0.25   3       \n",
      "2019-02-24  0.36   3       \n",
      "2019-02-13  0.36   3       \n",
      "2019-02-12  0.45   3       \n",
      "2019-02-10  0.45   3       \n",
      "2019-02-09  0.54   3       \n",
      "2019-02-01  0.54   3       \n",
      "2019-01-31  0.62   3       \n",
      "2019-01-30  0.63   3       \n",
      "2019-01-22  0.63   3       \n",
      "2019-01-21  0.57   3       \n",
      "2019-01-20  0.51   3       \n",
      "2019-01-11  0.51   3       \n",
      "2019-01-10  0.60   3       \n",
      "2018-12-31  0.60   3       \n",
      "2018-12-30  0.51   3       \n",
      "2018-12-29  0.49   3       \n",
      "2018-12-25  0.49   3       \n",
      "2018-12-24  0.44   3       \n",
      "2018-12-23  0.49   3       \n",
      "2018-12-16  0.49   3       \n",
      "2018-12-15  0.57   3       \n",
      "2018-12-14  0.58   3       \n",
      "2018-12-13  0.68   3       \n",
      "2018-12-12  0.70   3       \n",
      "2018-12-08  0.70   3       \n",
      "2018-12-07  0.78   3       \n",
      "2018-12-06  0.82   3       \n",
      "2018-12-05  0.89   3       \n",
      "2018-12-04  1.01   3       \n",
      "2018-11-16  1.01   3       \n",
      "2018-11-15  1.19   3       \n",
      "2018-11-14  1.20   3       \n",
      "2018-11-09  1.20   3       \n",
      "2018-11-08  1.31   3       \n",
      "2018-11-07  1.39   3       \n",
      "2018-10-03  1.39   3       \n",
      "2018-10-02  1.48   3       \n",
      "2018-10-01  1.64   3       \n",
      "2018-09-30  1.73   3       \n",
      "2018-09-23  1.73   3       \n",
      "2018-09-22  1.82   3       \n",
      "2018-09-21  1.95   3       \n",
      "2018-09-14  1.95   3       \n",
      "2018-09-13  2.12   3       \n",
      "2018-09-12  2.24   3       \n",
      "2018-09-11  2.24   3       \n",
      "2018-09-10  2.18   3       \n",
      "2018-09-09  1.95   3       \n",
      "2018-09-08  1.95   3       \n",
      "2018-09-07  1.88   3       \n",
      "2018-08-28  3.39   2       \n",
      "2018-08-27  3.41   2       \n",
      "2018-08-26  3.41   2       \n",
      "2018-08-25  3.44   2       \n",
      "2018-08-24  3.60   2       \n",
      "2018-08-23  3.63   2       \n",
      "2018-08-22  3.63   2       \n",
      "2018-08-21  3.62   2       \n",
      "2018-08-20  3.67   2       \n",
      "2018-08-19  3.67   2       \n",
      "2018-08-18  3.84   2       \n",
      "2018-08-17  3.85   2       \n",
      "2018-08-16  3.85   2       \n",
      "2018-08-15  3.82   2       \n",
      "2018-08-14  3.66   2       \n",
      "2018-08-12  3.66   2       \n",
      "2018-08-11  3.68   2       \n",
      "2018-08-10  3.66   2       \n",
      "2018-08-09  3.65   2       \n",
      "2018-08-08  3.65   2       \n",
      "2018-08-07  3.67   2       \n",
      "2018-08-03  3.67   2       \n",
      "2018-08-02  3.73   2       \n",
      "2018-08-01  3.64   2       \n",
      "2018-07-31  3.75   2       \n",
      "2018-07-30  3.69   2       \n",
      "2018-07-29  3.65   2       \n",
      "2018-07-28  3.65   2       \n",
      "2018-07-27  3.69   2       \n",
      "2018-07-26  3.69   2       \n",
      "2018-07-25  3.68   2       \n",
      "2018-07-24  3.68   2       \n",
      "2018-07-23  3.56   2       \n",
      "2018-07-22  3.66   2       \n",
      "2018-07-21  3.69   2       \n",
      "2018-07-20  3.56   2       \n",
      "2018-07-19  3.51   2       \n",
      "2018-07-09  3.51   2       \n",
      "2018-07-08  3.49   2       \n",
      "2018-07-07  3.47   2       \n",
      "2018-07-06  3.30   2       \n",
      "2018-07-05  3.38   2       \n",
      "2018-07-04  3.45   2       \n",
      "2018-07-03  3.46   2       \n",
      "2018-07-02  3.64   2       \n",
      "2018-06-21  3.64   2       \n",
      "2018-06-20  3.65   2       \n",
      "2018-06-13  3.65   2       \n",
      "2018-06-12  3.67   2       \n",
      "2018-06-11  3.80   2       \n",
      "2018-06-10  3.86   2       \n",
      "2018-06-05  3.86   2       \n",
      "2018-06-04  3.87   2       \n",
      "2018-06-03  3.89   2       \n",
      "2018-06-02  3.87   2       \n",
      "2018-05-31  3.87   2       \n",
      "2018-05-30  3.90   2       \n",
      "2018-05-29  3.92   2       \n",
      "2018-05-28  3.93   2       \n",
      "2018-05-27  4.01   2       \n",
      "2018-05-26  4.04   2       \n",
      "2018-05-23  4.04   2       \n",
      "2018-05-22  4.03   2       \n",
      "2018-05-16  4.03   2       \n",
      "2018-05-15  3.99   2       \n",
      "2018-05-14  3.97   2       \n",
      "2018-05-12  3.97   2       \n",
      "2018-05-11  3.99   2       \n",
      "2018-05-10  4.24   2       \n",
      "2018-05-09  4.25   2       \n",
      "2018-05-08  4.25   2       \n",
      "2018-05-07  4.24   2       \n",
      "2018-05-06  4.23   2       \n",
      "2018-05-05  4.24   2       \n",
      "2018-04-29  4.24   2       \n",
      "2018-04-28  4.25   2       \n",
      "2018-04-27  4.27   2       \n",
      "2018-04-25  4.27   2       \n",
      "2018-04-24  4.26   2       \n",
      "2018-04-23  4.25   2       \n",
      "2018-04-22  4.22   2       \n",
      "2018-04-20  4.22   2       \n",
      "2018-04-19  4.25   2       \n",
      "2018-04-18  4.21   2       \n",
      "2018-04-17  4.19   2       \n",
      "2018-04-16  4.19   2       \n",
      "2018-04-15  4.07   2       \n",
      "2018-04-14  4.04   2       \n",
      "2019-10-05  2.65   1       \n",
      "2019-10-04  2.65   1       \n",
      "2019-10-03  2.63   1       \n",
      "2019-10-02  2.64   1       \n",
      "2019-10-01  2.62   1       \n",
      "2019-09-30  2.62   1       \n",
      "2019-09-29  2.63   1       \n",
      "2019-09-28  2.64   1       \n",
      "2019-09-27  2.64   1       \n",
      "2019-09-26  2.62   1       \n",
      "2019-09-25  2.74   1       \n",
      "2019-09-24  2.82   1       \n",
      "2019-09-22  2.82   1       \n",
      "2019-09-21  2.84   1       \n",
      "2019-09-20  2.81   1       \n",
      "2019-09-19  2.82   1       \n",
      "2019-09-18  2.80   1       \n",
      "2019-09-17  2.81   1       \n",
      "2019-09-16  2.82   1       \n",
      "2019-09-15  2.81   1       \n",
      "2019-09-14  2.80   1       \n",
      "2019-09-13  2.79   1       \n",
      "2019-09-12  2.79   1       \n",
      "2019-09-11  2.77   1       \n",
      "2019-09-10  2.77   1       \n",
      "2019-09-09  2.78   1       \n",
      "2019-09-08  3.07   1       \n",
      "2019-09-07  3.07   1       \n",
      "2019-09-06  2.95   1       \n",
      "2019-09-05  2.95   1       \n",
      "2019-09-04  2.96   1       \n",
      "2018-07-09  1.67   3       \n",
      "2018-07-08  1.59   3       \n",
      "2018-07-07  1.45   3       \n",
      "2018-07-06  1.38   3       \n",
      "2018-07-05  1.42   3       \n",
      "2018-07-04  1.42   3       \n",
      "2018-07-03  1.28   3       \n",
      "2018-07-02  1.33   3       \n",
      "2018-07-01  1.39   3       \n",
      "2018-06-26  1.39   3       \n",
      "2018-06-25  2.24   3       \n",
      "2018-06-24  2.28   3       \n",
      "2018-06-21  2.28   3       \n",
      "2018-06-20  2.06   3       \n",
      "2018-06-19  1.99   3       \n",
      "2018-06-13  1.99   3       \n",
      "2018-06-12  2.14   3       \n",
      "2018-06-11  2.28   3       \n",
      "2018-06-10  2.05   3       \n",
      "2018-06-09  1.99   3       \n",
      "2018-06-07  1.99   3       \n",
      "2018-06-06  2.45   3       \n",
      "2018-06-05  2.47   3       \n",
      "2018-06-02  2.47   3       \n",
      "2018-06-01  2.25   3       \n",
      "2018-05-31  2.18   3       \n",
      "2018-05-24  2.18   3       \n",
      "2018-05-23  1.69   3       \n",
      "2018-05-22  1.72   3       \n",
      "2018-05-21  1.85   3       \n",
      "2018-05-20  1.85   3       \n",
      "2018-05-19  2.08   3       \n",
      "2018-05-18  2.40   3       \n",
      "2018-05-16  2.40   3       \n",
      "2018-05-15  2.69   3       \n",
      "2018-05-05  2.69   3       \n",
      "2018-05-04  2.81   3       \n",
      "2018-05-03  3.09   3       \n",
      "2018-05-02  3.42   3       \n",
      "2018-05-01  3.73   3       \n",
      "2018-04-27  3.73   3       \n",
      "2018-04-26  3.83   3       \n",
      "2018-04-25  4.12   3       \n",
      "2018-04-23  4.12   3       \n",
      "2018-04-22  3.94   3       \n",
      "2018-04-21  3.73   3       \n",
      "2018-04-15  3.73   3       \n",
      "2018-04-14  3.39   3       \n",
      "2018-04-13  2.98   3       \n",
      "2018-04-11  2.98   3       \n",
      "2018-04-10  2.95   3       \n",
      "2018-04-09  2.66   3       \n",
      "2018-04-07  2.66   3       \n",
      "2018-04-06  2.78   3       \n",
      "2018-04-05  2.95   3       \n",
      "2018-04-02  2.95   3       \n",
      "2018-04-01  3.13   3       \n",
      "2018-03-31  3.37   3       \n",
      "2018-03-30  2.95   3       \n",
      "2018-03-29  2.71   3       \n",
      "2018-03-28  2.63   3       \n",
      "2018-03-21  2.63   3       \n",
      "2018-03-20  2.26   3       \n",
      "2018-03-19  2.05   3       \n",
      "2018-03-18  2.05   3       \n",
      "2018-03-17  1.86   3       \n",
      "2018-03-16  1.76   3       \n",
      "2018-03-13  1.76   3       \n",
      "2018-03-12  1.87   3       \n",
      "2018-03-11  2.27   3       \n",
      "2018-03-10  2.27   3       \n",
      "2018-03-09  2.48   3       \n",
      "2018-03-08  2.56   3       \n",
      "2018-03-04  2.56   3       \n",
      "2018-03-03  2.81   3       \n",
      "2018-03-02  2.85   3       \n",
      "2018-03-01  2.85   3       \n",
      "2018-02-28  2.86   3       \n",
      "2018-02-27  3.16   3       \n",
      "2018-02-26  3.53   3       \n",
      "2018-02-24  3.53   3       \n",
      "2018-02-23  2.97   3       \n",
      "2018-02-22  2.89   3       \n",
      "2018-02-21  3.11   3       \n",
      "2018-02-19  3.11   3       \n",
      "2018-02-18  2.72   3       \n",
      "2018-02-17  2.51   3       \n",
      "2018-02-16  2.24   3       \n",
      "2018-02-15  2.40   3       \n",
      "2018-02-14  2.53   3       \n",
      "2018-02-13  2.53   3       \n",
      "2018-02-12  2.76   3       \n",
      "2018-02-11  2.82   3       \n",
      "2018-02-10  2.63   3       \n",
      "2018-02-09  1.99   3       \n",
      "2018-02-08  1.95   3       \n",
      "2018-02-07  2.10   3       \n",
      "2018-02-06  2.24   3       \n",
      "2018-02-05  1.99   3       \n",
      "2018-02-04  1.95   3       \n",
      "2018-02-02  1.95   3       \n",
      "2018-02-01  2.22   3       \n",
      "2018-01-31  2.24   3       \n",
      "2018-01-19  2.24   3       \n",
      "2018-01-18  2.23   3       \n",
      "2018-01-17  2.17   3       \n",
      "2018-01-16  2.24   3       \n",
      "2018-01-15  2.11   3       \n",
      "2018-01-14  2.01   3       \n",
      "2018-01-13  2.24   3       \n",
      "2018-01-12  1.97   3       \n",
      "2018-01-11  1.93   3       \n",
      "2018-01-09  1.93   3       \n",
      "2018-01-08  1.51   3       \n",
      "2018-01-07  1.45   3       \n",
      "2018-01-05  1.45   3       \n",
      "2018-01-04  1.24   3       \n",
      "2018-01-03  1.04   3       \n",
      "2018-01-02  1.08   3       \n",
      "2018-01-01  1.11   3       \n",
      "2017-12-31  0.97   3       \n",
      "2017-12-30  1.04   3       \n",
      "2017-12-29  1.12   3       \n",
      "2017-12-28  1.25   3       \n",
      "2017-12-27  1.34   3       \n",
      "2017-12-26  1.72   3       \n",
      "2017-12-25  1.91   3       \n",
      "2017-12-21  1.91   3       \n",
      "2017-12-20  1.66   3       \n",
      "2017-12-19  1.65   3       \n",
      "2017-12-18  1.82   3       \n",
      "2017-12-17  1.87   3       \n",
      "2017-12-15  1.87   3       \n",
      "2017-12-14  1.58   3       \n",
      "2017-12-13  1.54   3       \n",
      "2017-12-02  1.54   3       \n",
      "2017-12-01  1.61   3       \n",
      "2017-11-30  1.85   3       \n",
      "2017-11-29  1.90   3       \n",
      "2017-11-27  1.90   3       \n",
      "2017-11-26  2.15   3       \n",
      "2017-11-25  2.19   3       \n",
      "2017-11-21  2.19   3       \n",
      "2017-11-20  2.43   3       \n",
      "2017-11-19  2.48   3       \n",
      "2017-11-17  2.48   3       \n",
      "2017-11-16  2.63   3       \n",
      "2017-11-15  2.77   3       \n",
      "2017-11-14  2.82   3       \n",
      "2017-11-13  3.06   3       \n",
      "2017-11-11  3.06   3       \n",
      "2017-11-10  2.94   3       \n",
      "2017-11-09  2.48   3       \n",
      "2017-11-02  2.48   3       \n",
      "2017-11-01  2.29   3       \n",
      "2017-10-31  2.19   3       \n",
      "2017-10-26  2.19   3       \n",
      "2017-10-25  2.15   3       \n",
      "2017-10-24  1.90   3       \n",
      "2017-10-18  1.90   3       \n",
      "2017-10-17  1.91   3       \n",
      "2017-10-16  2.19   3       \n",
      "2017-10-10  2.19   3       \n",
      "2017-10-09  2.47   3       \n",
      "2017-10-08  2.58   3       \n",
      "2017-10-07  2.10   3       \n",
      "2017-10-06  2.23   3       \n",
      "2017-10-05  2.29   3       \n",
      "2017-10-01  2.29   3       \n",
      "2017-09-30  2.57   3       \n",
      "2017-09-29  2.74   3       \n",
      "2017-09-28  3.02   3       \n",
      "2017-09-23  3.02   3       \n",
      "2017-09-22  3.07   3       \n",
      "2017-09-21  3.38   3       \n",
      "2017-09-10  3.38   3       \n",
      "2017-09-09  2.99   3       \n",
      "2017-09-08  3.88   3       \n",
      "2017-09-07  3.96   3       \n",
      "2017-08-11  3.96   3       \n",
      "2017-08-10  4.24   3       \n",
      "2017-08-09  4.35   3       \n",
      "2017-07-28  4.35   3       \n",
      "2017-07-27  4.02   3       \n",
      "2017-07-26  3.96   3       \n",
      "2017-07-25  3.96   3       \n",
      "2017-07-24  3.63   3       \n",
      "2017-07-23  3.57   3       \n",
      "2017-07-22  3.57   3       \n",
      "2017-07-21  2.98   3       \n",
      "2017-07-20  2.86   3       \n",
      "2017-07-09  2.86   3       \n",
      "2017-07-08  2.61   3       \n",
      "2017-07-07  2.57   3       \n",
      "2017-07-05  2.57   3       \n",
      "2017-07-04  2.41   3       \n",
      "2017-07-03  2.05   3       \n",
      "2017-07-02  1.83   3       \n",
      "2017-07-01  2.28   3       \n",
      "2017-06-29  2.53   3       \n",
      "2020-06-28  5.99   4       \n",
      "2020-05-05  5.99   4       \n",
      "2020-05-04  6.54   4       \n",
      "2020-05-03  6.72   4       \n",
      "2020-04-28  6.72   4       \n",
      "2020-04-27  6.39   4       \n",
      "2020-04-26  5.99   4       \n",
      "2020-04-25  5.99   4       \n",
      "2020-04-24  6.19   4       \n",
      "2020-04-23  6.79   4       \n",
      "2020-04-22  6.79   4       \n",
      "2020-04-21  7.89   4       \n",
      "2020-04-20  8.25   4       \n",
      "2020-04-11  8.25   4       \n",
      "2020-04-10  8.83   4       \n",
      "2020-04-09  8.98   4       \n",
      "2020-04-02  8.98   4       \n",
      "2020-04-01  8.45   4       \n",
      "2020-03-31  7.99   4       \n",
      "2020-03-30  7.99   4       \n",
      "2020-03-29  8.08   4       \n",
      "2020-03-28  8.99   4       \n",
      "2020-03-15  8.99   4       \n",
      "2020-03-14  8.95   4       \n",
      "2020-03-13  9.49   4       \n",
      "2020-03-12  9.99   4       \n",
      "2020-01-28  9.99   4       \n",
      "2020-01-27  9.78   4       \n",
      "2020-01-26  8.99   4       \n",
      "2020-01-25  9.12   4       \n",
      "2020-01-24  9.99   4       \n",
      "2020-01-18  9.99   4       \n",
      "2020-01-17  9.86   4       \n",
      "2020-01-16  8.99   4       \n",
      "2020-01-14  8.99   4       \n",
      "2020-01-13  9.57   4       \n",
      "2020-01-12  9.99   4       \n",
      "2019-12-25  9.99   4       \n",
      "2019-12-24  9.81   4       \n",
      "2019-12-23  9.69   4       \n",
      "2019-12-15  9.69   4       \n",
      "2019-12-14  10.61  4       \n",
      "2019-12-13  10.69  4       \n",
      "2019-12-12  10.44  4       \n",
      "2019-12-11  8.71   4       \n",
      "2019-11-30  8.71   4       \n",
      "2019-11-29  8.09   4       \n",
      "2019-11-28  7.72   4       \n",
      "2019-11-25  7.72   4       \n",
      "2019-11-24  8.18   4       \n",
      "2019-11-23  8.45   4       \n",
      "2019-11-22  8.86   4       \n",
      "2019-11-21  9.44   4       \n",
      "2019-11-20  8.85   4       \n",
      "2019-11-19  8.73   4       \n",
      "2019-11-18  8.73   4       \n",
      "2019-11-17  8.15   4       \n",
      "2019-11-16  8.00   4       \n",
      "2019-11-15  8.00   4       \n",
      "2019-11-14  7.42   4       \n",
      "2019-11-13  7.27   4       \n",
      "2019-11-12  7.27   4       \n",
      "2019-11-11  6.69   4       \n",
      "2019-11-10  6.54   4       \n",
      "2019-11-09  6.54   4       \n",
      "2019-11-08  6.07   4       \n",
      "2019-11-07  5.95   4       \n",
      "2019-11-06  5.95   4       \n",
      "2019-11-05  5.59   4       \n",
      "2019-11-04  5.49   4       \n",
      "2019-11-01  5.49   4       \n",
      "2019-10-31  5.12   4       \n",
      "2019-10-30  4.85   4       \n",
      "2019-10-29  4.85   4       \n",
      "2019-10-28  4.64   4       \n",
      "2019-10-27  4.39   4       \n",
      "2019-10-26  4.39   4       \n",
      "2019-10-25  4.02   4       \n",
      "2019-10-24  3.34   4       \n",
      "2019-10-23  3.08   4       \n",
      "2019-10-22  2.93   4       \n",
      "2019-10-21  2.79   4       \n",
      "2019-08-30  2.79   4       \n",
      "2019-08-29  2.73   4       \n",
      "2019-08-28  2.50   4       \n",
      "2019-08-27  2.58   4       \n",
      "2019-08-26  2.79   4       \n",
      "2019-07-01  2.79   4       \n",
      "2019-06-30  2.96   4       \n",
      "2019-06-29  3.06   4       \n",
      "2019-05-14  3.06   4       \n",
      "2019-05-13  2.99   4       \n",
      "2019-05-12  2.77   4       \n",
      "2019-05-11  2.77   4       \n",
      "2019-05-10  2.70   4       \n",
      "2019-05-09  2.48   4       \n",
      "2019-05-07  2.48   4       \n",
      "2019-05-06  2.37   4       \n",
      "2019-05-05  2.19   4       \n",
      "2019-05-04  2.19   4       \n",
      "2019-05-03  2.12   4       \n",
      "2019-05-02  1.97   4       \n",
      "2019-05-01  1.97   4       \n",
      "2019-04-30  1.82   4       \n",
      "2019-04-29  1.53   4       \n",
      "2019-04-17  1.53   4       \n",
      "2019-04-16  1.36   4       \n",
      "2019-04-15  1.31   4       \n",
      "2019-02-25  1.31   4       \n",
      "2019-02-24  1.16   4       \n",
      "2019-02-23  1.16   4       \n",
      "2019-02-22  1.19   4       \n",
      "2019-02-16  1.19   4       \n",
      "2019-02-15  1.43   4       \n",
      "2019-02-14  1.57   4       \n",
      "2019-02-05  1.57   4       \n",
      "2019-02-04  1.79   4       \n",
      "2019-01-11  1.79   4       \n",
      "2019-01-10  1.60   4       \n",
      "2019-01-09  1.57   4       \n",
      "2018-12-30  1.57   4       \n",
      "2018-12-29  1.59   4       \n",
      "2018-12-28  1.79   4       \n",
      "2018-12-22  1.79   4       \n",
      "2018-12-21  2.32   4       \n",
      "2018-12-20  2.58   4       \n",
      "2018-10-24  2.58   4       \n",
      "2018-10-23  2.75   4       \n",
      "2018-10-22  3.16   4       \n",
      "2018-09-26  3.16   4       \n",
      "2018-09-25  2.93   4       \n",
      "2018-09-24  2.80   4       \n",
      "2018-09-05  2.80   4       \n",
      "2018-09-04  2.69   4       \n",
      "2018-09-03  2.51   4       \n",
      "2018-06-14  2.51   4       \n",
      "2018-06-13  2.75   4       \n",
      "2018-06-12  2.80   4       \n",
      "2018-05-17  2.80   4       \n",
      "2018-05-16  2.96   4       \n",
      "2018-05-15  3.09   4       \n",
      "2018-04-10  3.09   4       \n",
      "2018-04-09  2.80   4       \n",
      "2018-04-08  2.73   4       \n",
      "2018-03-23  2.73   4       \n",
      "2018-03-22  2.80   4       \n",
      "2018-03-21  3.02   4       \n",
      "2018-02-21  3.02   4       \n",
      "2018-02-20  3.05   4       \n",
      "2018-02-19  3.41   4       \n",
      "2018-02-15  3.41   4       \n",
      "2018-02-14  3.36   4       \n",
      "2018-02-13  2.87   4       \n",
      "2018-02-12  3.05   4       \n",
      "2018-02-06  3.05   4       \n",
      "2018-02-05  2.93   4       \n",
      "2018-02-04  2.76   4       \n",
      "2018-01-21  2.76   4       \n",
      "2018-01-20  2.64   4       \n",
      "2018-01-19  2.18   4       \n",
      "2018-01-12  2.18   4       \n",
      "2018-01-11  2.06   4       \n",
      "2018-01-10  1.89   4       \n",
      "2018-01-09  1.89   4       \n",
      "2018-01-08  1.90   4       \n",
      "2018-01-07  1.71   4       \n",
      "2018-01-06  1.69   4       \n",
      "2017-12-28  1.69   4       \n",
      "2017-12-27  1.56   4       \n",
      "2017-12-26  1.47   4       \n",
      "2017-12-22  1.47   4       \n",
      "2017-12-21  1.67   4       \n",
      "2017-12-20  1.69   4       \n",
      "2017-12-17  1.69   4       \n",
      "2017-12-16  1.74   4       \n",
      "2017-12-15  1.56   4       \n",
      "2017-12-14  1.51   4       \n",
      "2017-12-13  1.69   4       \n",
      "2017-12-12  1.51   4       \n",
      "2017-12-11  1.47   4       \n",
      "2017-12-07  1.47   4       \n",
      "2017-12-06  1.63   4       \n",
      "2017-12-05  1.66   4       \n",
      "2017-12-04  1.27   4       \n",
      "2017-12-03  1.25   4       \n",
      "2017-12-02  1.25   4       \n",
      "2017-12-01  1.06   4       \n",
      "2017-11-30  1.11   4       \n",
      "2017-11-29  1.21   4       \n",
      "2017-11-28  1.12   4       \n",
      "2017-11-27  1.10   4       \n",
      "2017-11-26  1.00   4       \n",
      "2017-11-25  0.96   4       \n",
      "2017-11-24  0.99   4       \n",
      "2017-11-23  1.08   4       \n",
      "2017-11-22  1.02   4       \n",
      "2017-11-21  1.16   4       \n",
      "2017-11-20  1.31   4       \n",
      "2017-11-19  1.28   4       \n",
      "2017-11-18  1.31   4       \n",
      "2020-06-28  6.79   5       \n",
      "2020-04-25  6.79   5       \n",
      "2020-04-24  6.85   5       \n",
      "2020-04-23  7.04   5       \n",
      "2020-04-02  7.04   5       \n",
      "2020-04-01  6.67   5       \n",
      "2020-03-31  5.58   5       \n",
      "2020-03-30  5.41   5       \n",
      "2020-03-29  5.02   5       \n",
      "2020-03-28  5.53   5       \n",
      "2020-03-27  5.53   5       \n",
      "2020-03-26  5.56   5       \n",
      "2020-03-25  6.26   5       \n",
      "2020-03-24  5.68   5       \n",
      "2020-03-23  5.53   5       \n",
      "2020-03-22  5.53   5       \n",
      "2020-03-21  6.08   5       \n",
      "2020-03-20  6.53   5       \n",
      "2020-03-19  6.99   5       \n",
      "2020-03-13  6.99   5       \n",
      "2020-03-12  7.29   5       \n",
      "2020-03-11  7.72   5       \n",
      "2020-03-10  7.72   5       \n",
      "2020-03-09  8.44   5       \n",
      "2020-03-08  9.44   5       \n",
      "2020-02-22  9.44   5       \n",
      "2020-02-21  10.40  5       \n",
      "2020-02-20  10.44  5       \n",
      "2020-02-02  10.44  5       \n",
      "2020-02-01  11.40  5       \n",
      "2020-01-31  11.44  5       \n",
      "2019-11-25  11.44  5       \n",
      "2019-11-24  10.73  5       \n",
      "2019-11-23  10.44  5       \n",
      "2019-11-22  10.44  5       \n",
      "2019-11-21  9.73   5       \n",
      "2019-11-20  9.44   5       \n",
      "2019-11-19  9.44   5       \n",
      "2019-11-18  8.94   5       \n",
      "2019-11-17  8.73   5       \n",
      "2019-11-16  8.73   5       \n",
      "2019-11-15  8.21   5       \n",
      "2019-11-14  8.00   5       \n",
      "2019-11-13  8.00   5       \n",
      "2019-11-12  7.48   5       \n",
      "2019-11-11  7.27   5       \n",
      "2019-11-10  7.27   5       \n",
      "2019-11-09  6.75   5       \n",
      "2019-11-08  6.54   5       \n",
      "2019-11-07  6.54   5       \n",
      "2019-11-06  6.12   5       \n",
      "2019-11-05  5.95   5       \n",
      "2019-11-04  5.95   5       \n",
      "2019-11-03  5.68   5       \n",
      "2019-11-02  5.49   5       \n",
      "2019-11-01  5.49   5       \n",
      "2019-10-31  5.23   5       \n",
      "2019-10-30  4.85   5       \n",
      "2019-10-29  4.58   5       \n",
      "2019-10-28  4.58   5       \n",
      "2019-10-27  4.41   5       \n",
      "2019-10-26  4.19   5       \n",
      "2019-10-25  4.19   5       \n",
      "2019-10-24  4.27   5       \n",
      "2019-10-23  4.38   5       \n",
      "2019-07-03  4.38   5       \n",
      "2019-07-02  4.02   5       \n",
      "2019-07-01  3.99   5       \n",
      "2018-12-22  3.99   5       \n",
      "2018-12-21  4.14   5       \n",
      "2018-12-20  4.38   5       \n",
      "2018-08-24  4.38   5       \n",
      "2018-08-23  3.73   5       \n",
      "2018-08-22  3.60   5       \n",
      "2018-07-28  3.60   5       \n",
      "2018-07-27  3.63   5       \n",
      "2018-07-26  3.99   5       \n",
      "2018-06-18  3.99   5       \n",
      "2018-06-17  8.45   5       \n",
      "2018-06-16  9.94   5       \n",
      "2018-03-17  9.94   5       \n",
      "2018-03-16  10.19  5       \n",
      "2018-03-15  10.44  5       \n",
      "2018-02-28  10.44  5       \n",
      "2018-02-27  10.11  5       \n",
      "2018-02-26  9.44   5       \n",
      "2018-02-15  9.44   5       \n",
      "2018-02-14  9.38   5       \n",
      "2018-02-13  8.72   5       \n",
      "2017-10-06  8.72   5       \n",
      "2017-10-05  8.57   5       \n",
      "2017-10-04  7.99   5       \n",
      "2017-09-09  7.99   5       \n",
      "2017-09-08  10.74  5       \n",
      "2017-06-29  10.99  5       \n",
      "2018-09-06  1.69   3       \n",
      "2018-08-11  1.69   3       \n",
      "2018-08-10  1.53   3       \n",
      "2018-08-09  1.51   3       \n",
      "2018-08-08  1.71   3       \n",
      "2018-07-30  1.71   3       \n",
      "2018-07-29  1.93   3       \n",
      "2018-07-28  1.83   3       \n",
      "2018-07-27  1.67   3       \n",
      "2018-07-26  1.69   3       \n",
      "2018-07-25  1.89   3       \n",
      "2018-07-22  1.89   3       \n",
      "2018-07-21  1.50   3       \n",
      "2018-07-20  1.45   3       \n",
      "2018-07-16  1.45   3       \n",
      "2018-07-15  1.67   3       \n",
      "2018-07-14  1.67   3       \n",
      "2018-07-13  1.87   3       \n",
      "2018-07-12  2.15   3       \n",
      "2018-07-11  2.15   3       \n",
      "2018-07-10  2.09   3       \n",
      "2020-07-17  3.46   0       \n",
      "2020-06-29  3.58   0       \n",
      "2020-06-29  7.06   1       \n",
      "2020-06-29  6.28   2       \n",
      "2020-06-29  5.28   3       \n",
      "2020-06-29  5.99   4       \n",
      "2020-06-29  6.79   5       \n",
      "2020-07-16  3.61   0       \n",
      "2020-07-15  3.37   0       \n",
      "2020-07-14  3.48   0       \n",
      "2020-07-13  3.01   0       \n",
      "2020-07-12  2.35   0       \n",
      "2020-07-11  2.52   0       \n",
      "2020-07-10  2.66   0       \n",
      "2020-07-09  2.81   0       \n",
      "2020-07-08  2.87   0       \n",
      "2020-07-07  2.92   0       \n",
      "2020-07-06  3.25   0       \n",
      "2020-07-05  3.45   0       \n",
      "2020-07-04  3.20   0       \n",
      "2020-07-03  3.21   0       \n",
      "2020-07-02  3.23   0       \n",
      "2020-07-01  3.35   0       \n",
      "2020-06-30  3.71   0       \n",
      "2020-07-17  5.93   1       \n",
      "2020-07-16  6.11   1       \n",
      "2020-07-13  6.11   1       \n",
      "2020-07-12  6.09   1       \n",
      "2020-07-11  5.82   1       \n",
      "2020-07-10  6.13   1       \n",
      "2020-07-09  6.40   1       \n",
      "2020-07-08  6.41   1       \n",
      "2020-07-07  6.50   1       \n",
      "2020-07-03  6.50   1       \n",
      "2020-07-02  6.64   1       \n",
      "2020-07-01  6.92   1       \n",
      "2020-06-30  7.06   1       \n",
      "2020-07-17  5.59   2       \n",
      "2020-07-16  6.12   2       \n",
      "2020-07-15  6.28   2       \n",
      "2020-07-17  5.28   3       \n",
      "2020-07-17  5.99   4       \n",
      "2020-07-17  6.79   5       \n",
      "2020-08-27  3.91   0       \n",
      "2020-08-26  3.90   0       \n",
      "2020-08-25  4.25   0       \n",
      "2020-08-24  4.07   0       \n",
      "2020-08-23  4.30   0       \n",
      "2020-08-22  4.27   0       \n",
      "2020-08-21  4.27   0       \n",
      "2020-08-20  4.06   0       \n",
      "2020-08-19  3.77   0       \n",
      "2020-08-18  3.96   0       \n",
      "2020-08-17  4.42   0       \n",
      "2020-08-16  4.17   0       \n",
      "2020-08-15  3.67   0       \n",
      "2020-08-14  3.39   0       \n",
      "2020-08-13  3.70   0       \n",
      "2020-08-12  3.77   0       \n",
      "2020-08-11  3.48   0       \n",
      "2020-08-10  3.56   0       \n",
      "2020-08-09  3.40   0       \n",
      "2020-08-08  4.86   0       \n",
      "2020-08-07  4.98   0       \n",
      "2020-08-06  4.82   0       \n",
      "2020-08-05  4.75   0       \n",
      "2020-08-04  4.53   0       \n",
      "2020-08-03  2.81   0       \n",
      "2020-08-02  2.78   0       \n",
      "2020-08-01  2.98   0       \n",
      "2020-07-31  3.26   0       \n",
      "2020-07-30  3.29   0       \n",
      "2020-07-29  3.34   0       \n",
      "2020-07-28  3.21   0       \n",
      "2020-07-27  3.37   0       \n",
      "2020-07-26  3.27   0       \n",
      "2020-07-25  2.97   0       \n",
      "2020-07-24  2.84   0       \n",
      "2020-07-23  2.68   0       \n",
      "2020-07-22  2.78   0       \n",
      "2020-07-21  2.84   0       \n",
      "2020-07-20  3.08   0       \n",
      "2020-07-19  3.13   0       \n",
      "2020-07-18  3.31   0       \n",
      "2020-08-27  6.40   1       \n",
      "2020-08-26  6.36   1       \n",
      "2020-08-25  6.36   1       \n",
      "2020-08-24  6.91   1       \n",
      "2020-08-23  6.57   1       \n",
      "2020-08-22  6.46   1       \n",
      "2020-08-20  6.46   1       \n",
      "2020-08-19  6.48   1       \n",
      "2020-08-18  6.49   1       \n",
      "2020-08-17  6.49   1       \n",
      "2020-08-16  6.41   1       \n",
      "2020-08-15  6.02   1       \n",
      "2020-08-12  6.02   1       \n",
      "2020-08-11  6.25   1       \n",
      "2020-08-10  6.68   1       \n",
      "2020-08-09  6.72   1       \n",
      "2020-08-08  6.78   1       \n",
      "2020-08-07  6.22   1       \n",
      "2020-08-06  6.18   1       \n",
      "2020-08-05  6.18   1       \n",
      "2020-08-04  5.78   1       \n",
      "2020-08-03  5.32   1       \n",
      "2020-08-02  5.30   1       \n",
      "2020-08-01  5.40   1       \n",
      "2020-07-31  5.54   1       \n",
      "2020-07-29  5.54   1       \n",
      "2020-07-28  5.60   1       \n",
      "2020-07-27  5.79   1       \n",
      "2020-07-26  5.42   1       \n",
      "2020-07-25  5.06   1       \n",
      "2020-07-24  5.28   1       \n",
      "2020-07-23  5.34   1       \n",
      "2020-07-22  5.14   1       \n",
      "2020-07-21  5.47   1       \n",
      "2020-07-20  5.76   1       \n",
      "2020-07-19  5.82   1       \n",
      "2020-07-18  5.82   1       \n",
      "2020-08-27  5.74   2       \n",
      "2020-08-26  6.04   2       \n",
      "2020-08-21  6.04   2       \n",
      "2020-08-20  6.05   2       \n",
      "2020-08-19  6.12   2       \n",
      "2020-08-17  6.12   2       \n",
      "2020-08-16  6.06   2       \n",
      "2020-08-15  5.92   2       \n",
      "2020-08-13  5.92   2       \n",
      "2020-08-12  5.83   2       \n",
      "2020-08-10  5.83   2       \n",
      "2020-08-09  5.76   2       \n",
      "2020-08-08  5.66   2       \n",
      "2020-08-07  5.37   2       \n",
      "2020-08-06  5.13   2       \n",
      "2020-08-05  5.09   2       \n",
      "2020-08-04  4.76   2       \n",
      "2020-08-03  3.89   2       \n",
      "2020-08-02  3.97   2       \n",
      "2020-08-01  4.02   2       \n",
      "2020-07-31  4.10   2       \n",
      "2020-07-29  4.10   2       \n",
      "2020-07-28  4.08   2       \n",
      "2020-07-27  4.07   2       \n",
      "2020-07-26  4.06   2       \n",
      "2020-07-25  4.17   2       \n",
      "2020-07-24  4.24   2       \n",
      "2020-07-23  4.28   2       \n",
      "2020-07-22  4.28   2       \n",
      "2020-07-21  4.49   2       \n",
      "2020-07-20  5.00   2       \n",
      "2020-07-19  5.18   2       \n",
      "2020-07-18  5.20   2       \n",
      "2020-08-27  4.45   3       \n",
      "2020-08-26  4.45   3       \n",
      "2020-08-25  4.87   3       \n",
      "2020-08-24  4.91   3       \n",
      "2020-08-10  4.91   3       \n",
      "2020-08-09  4.64   3       \n",
      "2020-08-08  4.45   3       \n",
      "2020-08-01  4.45   3       \n",
      "2020-07-31  4.43   3       \n",
      "2020-07-30  3.99   3       \n",
      "2020-07-24  3.99   3       \n",
      "2020-07-23  4.47   3       \n",
      "2020-07-22  4.82   3       \n",
      "2020-07-21  5.17   3       \n",
      "2020-07-20  5.28   3       \n",
      "2020-08-27  4.87   4       \n",
      "2020-08-02  4.87   4       \n",
      "2020-08-01  5.06   4       \n",
      "2020-07-31  5.33   4       \n",
      "2020-07-29  5.33   4       \n",
      "2020-07-28  5.77   4       \n",
      "2020-07-27  5.99   4       \n",
      "2020-08-27  6.79   5       \n",
      "2020-09-21  2.70   0       \n",
      "2020-09-12  5.39   2       \n",
      "2020-09-11  5.44   2       \n",
      "2020-09-10  5.44   2       \n",
      "2020-09-09  5.59   2       \n",
      "2020-09-08  5.90   2       \n",
      "2020-08-31  5.90   2       \n",
      "2020-08-30  5.89   2       \n",
      "2020-08-29  5.79   2       \n",
      "2020-08-28  5.79   2       \n",
      "2020-09-21  3.99   3       \n",
      "2020-09-03  3.99   3       \n",
      "2020-09-02  4.20   3       \n",
      "2020-09-01  4.45   3       \n",
      "2020-09-21  4.87   4       \n",
      "2020-09-21  6.79   5       \n",
      "2020-09-20  2.68   0       \n",
      "2020-09-19  2.93   0       \n",
      "2020-09-18  2.97   0       \n",
      "2020-09-17  2.84   0       \n",
      "2020-09-16  3.19   0       \n",
      "2020-09-15  3.22   0       \n",
      "2020-09-14  3.16   0       \n",
      "2020-09-13  3.18   0       \n",
      "2020-09-12  3.31   0       \n",
      "2020-09-11  3.09   0       \n",
      "2020-09-10  2.86   0       \n",
      "2020-09-09  2.75   0       \n",
      "2020-09-08  2.95   0       \n",
      "2020-09-07  3.10   0       \n",
      "2020-09-06  3.32   0       \n",
      "2020-09-05  3.08   0       \n",
      "2020-09-04  2.88   0       \n",
      "2020-09-03  2.81   0       \n",
      "2020-09-02  3.14   0       \n",
      "2020-09-01  3.24   0       \n",
      "2020-08-31  3.79   0       \n",
      "2020-08-30  3.97   0       \n",
      "2020-08-29  4.06   0       \n",
      "2020-08-28  4.08   0       \n",
      "2020-09-21  5.39   1       \n",
      "2020-09-20  5.40   1       \n",
      "2020-09-19  5.44   1       \n",
      "2020-09-18  5.52   1       \n",
      "2020-09-17  5.45   1       \n",
      "2020-09-16  5.37   1       \n",
      "2020-09-15  5.35   1       \n",
      "2020-09-14  5.86   1       \n",
      "2020-09-13  5.95   1       \n",
      "2020-09-12  6.10   1       \n",
      "2020-09-11  6.13   1       \n",
      "2020-09-10  6.13   1       \n",
      "2020-09-09  6.17   1       \n",
      "2020-09-08  6.20   1       \n",
      "2020-09-07  6.20   1       \n",
      "2020-09-06  6.38   1       \n",
      "2020-09-03  6.38   1       \n",
      "2020-09-02  6.37   1       \n",
      "2020-09-01  6.36   1       \n",
      "2020-08-31  6.48   1       \n",
      "2020-08-30  6.42   1       \n",
      "2020-08-29  6.46   1       \n",
      "2020-08-28  6.43   1       \n",
      "2020-09-21  5.35   2       \n",
      "2020-09-19  5.35   2       \n",
      "2020-09-18  5.34   2       \n",
      "2020-09-17  5.29   2       \n",
      "2020-09-16  5.37   2       \n",
      "2020-09-15  5.36   2       \n",
      "2020-09-13  5.36   2       \n",
      "2020-09-29  2.80   0       \n",
      "2020-09-28  2.87   0       \n",
      "2020-09-27  3.11   0       \n",
      "2020-09-26  2.98   0       \n",
      "2020-09-25  2.84   0       \n",
      "2020-09-24  2.56   0       \n",
      "2020-09-23  2.60   0       \n",
      "2020-09-22  2.68   0       \n",
      "2020-09-29  5.49   1       \n",
      "2020-09-28  5.50   1       \n",
      "2020-09-27  5.49   1       \n",
      "2020-09-26  5.46   1       \n",
      "2020-09-25  5.43   1       \n",
      "2020-09-23  5.43   1       \n",
      "2020-09-22  5.39   1       \n",
      "2020-09-29  4.93   2       \n",
      "2020-09-28  4.93   2       \n",
      "2020-09-27  4.95   2       \n",
      "2020-09-26  5.43   2       \n",
      "2020-09-25  5.36   2       \n",
      "2020-09-23  5.36   2       \n",
      "2020-09-22  5.35   2       \n",
      "2020-09-29  3.99   3       \n",
      "2020-09-29  4.99   4       \n",
      "2020-09-28  4.87   4       \n",
      "2020-09-29  6.79   5       \n"
     ]
    }
   ],
   "source": [
    "print(goatdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "goatdfarr=[]\n",
    "for ctype in sorted(goatdf.cardtype.unique()):\n",
    "    goatdfarr.append(goatdf[goatdf['cardtype']==ctype])\n",
    "    goatdfarr[ctype]=goatdfarr[ctype].drop(columns='cardtype')\n",
    "    goatdfarr[ctype].columns=['price'+str(ctype)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>cardtype</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carddate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>5.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-02</th>\n",
       "      <td>4.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01</th>\n",
       "      <td>4.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-30</th>\n",
       "      <td>4.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-29</th>\n",
       "      <td>4.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-28</th>\n",
       "      <td>4.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-27</th>\n",
       "      <td>4.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-26</th>\n",
       "      <td>4.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-25</th>\n",
       "      <td>4.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-24</th>\n",
       "      <td>4.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-23</th>\n",
       "      <td>3.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-22</th>\n",
       "      <td>3.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-21</th>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-20</th>\n",
       "      <td>3.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-19</th>\n",
       "      <td>3.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-18</th>\n",
       "      <td>3.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-17</th>\n",
       "      <td>3.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-16</th>\n",
       "      <td>3.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-15</th>\n",
       "      <td>3.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-14</th>\n",
       "      <td>3.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-13</th>\n",
       "      <td>3.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-12</th>\n",
       "      <td>3.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-11</th>\n",
       "      <td>4.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-10</th>\n",
       "      <td>4.91</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-09</th>\n",
       "      <td>4.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-08</th>\n",
       "      <td>4.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-07</th>\n",
       "      <td>5.49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-06</th>\n",
       "      <td>4.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-05</th>\n",
       "      <td>4.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-04</th>\n",
       "      <td>4.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-03</th>\n",
       "      <td>4.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-02</th>\n",
       "      <td>4.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-01</th>\n",
       "      <td>4.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-31</th>\n",
       "      <td>4.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-30</th>\n",
       "      <td>5.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-29</th>\n",
       "      <td>5.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-28</th>\n",
       "      <td>4.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-27</th>\n",
       "      <td>3.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-26</th>\n",
       "      <td>3.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-25</th>\n",
       "      <td>3.93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-24</th>\n",
       "      <td>3.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-23</th>\n",
       "      <td>4.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-22</th>\n",
       "      <td>4.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-21</th>\n",
       "      <td>4.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-20</th>\n",
       "      <td>4.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-19</th>\n",
       "      <td>5.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-18</th>\n",
       "      <td>5.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-17</th>\n",
       "      <td>5.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-16</th>\n",
       "      <td>5.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-15</th>\n",
       "      <td>6.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-14</th>\n",
       "      <td>6.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-13</th>\n",
       "      <td>6.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-12</th>\n",
       "      <td>6.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-11</th>\n",
       "      <td>5.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-10</th>\n",
       "      <td>4.76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-09</th>\n",
       "      <td>4.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-08</th>\n",
       "      <td>4.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-07</th>\n",
       "      <td>4.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-06</th>\n",
       "      <td>4.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-05</th>\n",
       "      <td>4.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-04</th>\n",
       "      <td>4.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-03</th>\n",
       "      <td>4.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-02</th>\n",
       "      <td>4.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01</th>\n",
       "      <td>5.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-29</th>\n",
       "      <td>5.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-28</th>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-27</th>\n",
       "      <td>4.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-26</th>\n",
       "      <td>4.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-25</th>\n",
       "      <td>4.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-24</th>\n",
       "      <td>4.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-23</th>\n",
       "      <td>4.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-22</th>\n",
       "      <td>4.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-21</th>\n",
       "      <td>4.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-20</th>\n",
       "      <td>4.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-19</th>\n",
       "      <td>5.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-18</th>\n",
       "      <td>5.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-17</th>\n",
       "      <td>5.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-16</th>\n",
       "      <td>5.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-15</th>\n",
       "      <td>5.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-14</th>\n",
       "      <td>6.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-13</th>\n",
       "      <td>6.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-12</th>\n",
       "      <td>6.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-11</th>\n",
       "      <td>6.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-10</th>\n",
       "      <td>6.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-09</th>\n",
       "      <td>6.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-08</th>\n",
       "      <td>7.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-07</th>\n",
       "      <td>7.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-06</th>\n",
       "      <td>7.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-05</th>\n",
       "      <td>7.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-04</th>\n",
       "      <td>6.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-03</th>\n",
       "      <td>6.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-02</th>\n",
       "      <td>6.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-01</th>\n",
       "      <td>6.15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-31</th>\n",
       "      <td>6.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-30</th>\n",
       "      <td>7.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-29</th>\n",
       "      <td>7.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-28</th>\n",
       "      <td>8.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-27</th>\n",
       "      <td>9.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-26</th>\n",
       "      <td>10.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-25</th>\n",
       "      <td>10.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-24</th>\n",
       "      <td>9.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-23</th>\n",
       "      <td>8.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-22</th>\n",
       "      <td>8.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-21</th>\n",
       "      <td>8.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-20</th>\n",
       "      <td>7.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-19</th>\n",
       "      <td>6.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-18</th>\n",
       "      <td>7.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-17</th>\n",
       "      <td>7.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-16</th>\n",
       "      <td>7.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-15</th>\n",
       "      <td>8.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-14</th>\n",
       "      <td>8.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-13</th>\n",
       "      <td>9.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-12</th>\n",
       "      <td>9.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-11</th>\n",
       "      <td>9.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-10</th>\n",
       "      <td>10.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-09</th>\n",
       "      <td>9.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-08</th>\n",
       "      <td>7.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-07</th>\n",
       "      <td>6.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-06</th>\n",
       "      <td>5.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-05</th>\n",
       "      <td>5.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-04</th>\n",
       "      <td>5.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-05</th>\n",
       "      <td>1.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-04</th>\n",
       "      <td>1.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-03</th>\n",
       "      <td>1.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-02</th>\n",
       "      <td>1.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-01</th>\n",
       "      <td>1.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-31</th>\n",
       "      <td>1.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-30</th>\n",
       "      <td>1.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-29</th>\n",
       "      <td>1.59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-28</th>\n",
       "      <td>1.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-27</th>\n",
       "      <td>1.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-26</th>\n",
       "      <td>1.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-25</th>\n",
       "      <td>1.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-24</th>\n",
       "      <td>1.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-23</th>\n",
       "      <td>1.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-22</th>\n",
       "      <td>1.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-21</th>\n",
       "      <td>1.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-20</th>\n",
       "      <td>1.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-19</th>\n",
       "      <td>1.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-18</th>\n",
       "      <td>1.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-17</th>\n",
       "      <td>1.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-16</th>\n",
       "      <td>1.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-15</th>\n",
       "      <td>1.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-14</th>\n",
       "      <td>1.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-13</th>\n",
       "      <td>1.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-12</th>\n",
       "      <td>1.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-11</th>\n",
       "      <td>1.14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-10</th>\n",
       "      <td>1.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-09</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-08</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-07</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-06</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-05</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-04</th>\n",
       "      <td>1.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-03</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-02</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-01</th>\n",
       "      <td>1.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>1.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-30</th>\n",
       "      <td>1.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-29</th>\n",
       "      <td>1.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-28</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-27</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-26</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-25</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-24</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-23</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-22</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-21</th>\n",
       "      <td>1.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-20</th>\n",
       "      <td>1.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-19</th>\n",
       "      <td>1.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-18</th>\n",
       "      <td>1.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-17</th>\n",
       "      <td>1.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-16</th>\n",
       "      <td>1.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-15</th>\n",
       "      <td>1.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-14</th>\n",
       "      <td>1.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-13</th>\n",
       "      <td>1.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-12</th>\n",
       "      <td>1.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-11</th>\n",
       "      <td>1.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-10</th>\n",
       "      <td>1.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-09</th>\n",
       "      <td>1.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-08</th>\n",
       "      <td>1.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-07</th>\n",
       "      <td>1.49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-06</th>\n",
       "      <td>1.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-05</th>\n",
       "      <td>1.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-04</th>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-03</th>\n",
       "      <td>1.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-02</th>\n",
       "      <td>1.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01</th>\n",
       "      <td>1.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>1.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-29</th>\n",
       "      <td>1.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-28</th>\n",
       "      <td>1.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-27</th>\n",
       "      <td>1.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-26</th>\n",
       "      <td>1.15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-25</th>\n",
       "      <td>1.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-24</th>\n",
       "      <td>1.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-23</th>\n",
       "      <td>1.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-22</th>\n",
       "      <td>1.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-21</th>\n",
       "      <td>1.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-20</th>\n",
       "      <td>1.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-19</th>\n",
       "      <td>1.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-18</th>\n",
       "      <td>1.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-17</th>\n",
       "      <td>1.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-16</th>\n",
       "      <td>1.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-15</th>\n",
       "      <td>1.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-14</th>\n",
       "      <td>1.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-13</th>\n",
       "      <td>1.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-12</th>\n",
       "      <td>1.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-11</th>\n",
       "      <td>1.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-10</th>\n",
       "      <td>1.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-09</th>\n",
       "      <td>1.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-08</th>\n",
       "      <td>1.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-07</th>\n",
       "      <td>1.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-06</th>\n",
       "      <td>1.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-05</th>\n",
       "      <td>1.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-04</th>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-03</th>\n",
       "      <td>1.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-02</th>\n",
       "      <td>1.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-01</th>\n",
       "      <td>1.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>1.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-30</th>\n",
       "      <td>1.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-29</th>\n",
       "      <td>1.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-28</th>\n",
       "      <td>1.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-27</th>\n",
       "      <td>1.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-26</th>\n",
       "      <td>1.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-25</th>\n",
       "      <td>1.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-24</th>\n",
       "      <td>1.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-23</th>\n",
       "      <td>1.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-22</th>\n",
       "      <td>1.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-21</th>\n",
       "      <td>1.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-20</th>\n",
       "      <td>1.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-19</th>\n",
       "      <td>1.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-18</th>\n",
       "      <td>1.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-17</th>\n",
       "      <td>1.14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-16</th>\n",
       "      <td>1.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-15</th>\n",
       "      <td>1.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-14</th>\n",
       "      <td>1.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-13</th>\n",
       "      <td>1.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-12</th>\n",
       "      <td>1.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-11</th>\n",
       "      <td>1.15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-10</th>\n",
       "      <td>1.13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-09</th>\n",
       "      <td>1.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-08</th>\n",
       "      <td>1.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-07</th>\n",
       "      <td>1.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-06</th>\n",
       "      <td>1.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-05</th>\n",
       "      <td>1.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-04</th>\n",
       "      <td>1.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-03</th>\n",
       "      <td>1.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-02</th>\n",
       "      <td>1.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-29</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-28</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-27</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-26</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-25</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-24</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-23</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-22</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-21</th>\n",
       "      <td>1.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-20</th>\n",
       "      <td>1.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-19</th>\n",
       "      <td>1.13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-18</th>\n",
       "      <td>1.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-17</th>\n",
       "      <td>1.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16</th>\n",
       "      <td>1.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-15</th>\n",
       "      <td>1.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-14</th>\n",
       "      <td>1.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-13</th>\n",
       "      <td>1.13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-12</th>\n",
       "      <td>1.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-11</th>\n",
       "      <td>1.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-10</th>\n",
       "      <td>1.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-09</th>\n",
       "      <td>1.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-08</th>\n",
       "      <td>1.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-07</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-06</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-05</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-04</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-03</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-02</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-01</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-29</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-28</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-27</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-26</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-25</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-24</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-23</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-22</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-21</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-20</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-19</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-18</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-17</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-15</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-14</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-13</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-12</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-11</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-10</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-09</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-08</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-07</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-05</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-04</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-03</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-02</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-01</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-27</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-26</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-25</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-24</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-23</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-22</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-21</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-20</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-19</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-18</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-17</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-16</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-15</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-14</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-13</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-12</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-11</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-10</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-09</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-08</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-07</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-06</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-05</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-04</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-03</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-02</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-29</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-28</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-27</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-26</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-25</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-24</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-23</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-21</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-20</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-19</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-18</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-17</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-16</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-15</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-14</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-13</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-11</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-10</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-09</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-08</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-07</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-06</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-05</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-30</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-29</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-28</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-27</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-26</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-25</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-24</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-23</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-22</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-21</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-20</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-19</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-18</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-17</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-16</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-15</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-14</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-13</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-12</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-11</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-10</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-09</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-08</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-07</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-06</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-05</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-04</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-03</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-02</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-01</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-29</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-28</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-27</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-26</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-25</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-24</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-23</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-22</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-21</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-20</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-19</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-18</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-17</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-16</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-15</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-14</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-13</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-12</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-11</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-10</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-09</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-08</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-07</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-06</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-05</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-04</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-03</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-02</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-01</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-30</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-29</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-28</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-27</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-26</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-25</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-24</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-23</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-22</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-21</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-20</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-19</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-18</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-17</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-16</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-15</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-14</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-13</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-12</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-11</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-10</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-09</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-08</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-07</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-06</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-05</th>\n",
       "      <td>1.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-04</th>\n",
       "      <td>1.14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-03</th>\n",
       "      <td>1.15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-02</th>\n",
       "      <td>1.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-01</th>\n",
       "      <td>1.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>1.09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-29</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-28</th>\n",
       "      <td>1.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-27</th>\n",
       "      <td>1.15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-26</th>\n",
       "      <td>1.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-25</th>\n",
       "      <td>1.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-24</th>\n",
       "      <td>1.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-23</th>\n",
       "      <td>1.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-22</th>\n",
       "      <td>1.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-21</th>\n",
       "      <td>1.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-20</th>\n",
       "      <td>1.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-19</th>\n",
       "      <td>1.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-18</th>\n",
       "      <td>1.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-17</th>\n",
       "      <td>1.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-16</th>\n",
       "      <td>1.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-15</th>\n",
       "      <td>1.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-14</th>\n",
       "      <td>1.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-13</th>\n",
       "      <td>1.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-12</th>\n",
       "      <td>2.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11</th>\n",
       "      <td>2.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-10</th>\n",
       "      <td>2.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-09</th>\n",
       "      <td>2.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-08</th>\n",
       "      <td>1.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-07</th>\n",
       "      <td>1.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-06</th>\n",
       "      <td>2.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-05</th>\n",
       "      <td>2.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-04</th>\n",
       "      <td>1.91</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-03</th>\n",
       "      <td>1.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-02</th>\n",
       "      <td>1.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-01</th>\n",
       "      <td>1.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>1.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-30</th>\n",
       "      <td>1.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-29</th>\n",
       "      <td>1.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-28</th>\n",
       "      <td>1.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-27</th>\n",
       "      <td>1.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-26</th>\n",
       "      <td>1.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-25</th>\n",
       "      <td>1.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-24</th>\n",
       "      <td>1.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-23</th>\n",
       "      <td>1.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-22</th>\n",
       "      <td>1.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-21</th>\n",
       "      <td>1.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-20</th>\n",
       "      <td>1.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-19</th>\n",
       "      <td>1.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-18</th>\n",
       "      <td>1.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-17</th>\n",
       "      <td>1.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-16</th>\n",
       "      <td>1.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-15</th>\n",
       "      <td>1.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-14</th>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-13</th>\n",
       "      <td>1.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-12</th>\n",
       "      <td>1.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-11</th>\n",
       "      <td>1.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-10</th>\n",
       "      <td>1.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-09</th>\n",
       "      <td>1.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-08</th>\n",
       "      <td>1.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-07</th>\n",
       "      <td>1.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-06</th>\n",
       "      <td>1.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-05</th>\n",
       "      <td>1.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-04</th>\n",
       "      <td>1.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-03</th>\n",
       "      <td>1.14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-02</th>\n",
       "      <td>1.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-01</th>\n",
       "      <td>1.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>1.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-30</th>\n",
       "      <td>1.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-29</th>\n",
       "      <td>1.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-28</th>\n",
       "      <td>1.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-27</th>\n",
       "      <td>1.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-26</th>\n",
       "      <td>1.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-25</th>\n",
       "      <td>1.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-24</th>\n",
       "      <td>1.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-23</th>\n",
       "      <td>1.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-22</th>\n",
       "      <td>1.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-21</th>\n",
       "      <td>1.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-20</th>\n",
       "      <td>1.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-19</th>\n",
       "      <td>1.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18</th>\n",
       "      <td>1.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-17</th>\n",
       "      <td>1.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-16</th>\n",
       "      <td>1.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-15</th>\n",
       "      <td>1.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-14</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-13</th>\n",
       "      <td>1.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-12</th>\n",
       "      <td>1.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-11</th>\n",
       "      <td>1.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-10</th>\n",
       "      <td>1.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-09</th>\n",
       "      <td>1.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-08</th>\n",
       "      <td>1.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-07</th>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-06</th>\n",
       "      <td>1.76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-05</th>\n",
       "      <td>1.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-04</th>\n",
       "      <td>1.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-03</th>\n",
       "      <td>1.59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-02</th>\n",
       "      <td>1.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-01</th>\n",
       "      <td>1.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>1.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-29</th>\n",
       "      <td>1.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-28</th>\n",
       "      <td>1.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-27</th>\n",
       "      <td>1.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-26</th>\n",
       "      <td>1.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-25</th>\n",
       "      <td>1.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-24</th>\n",
       "      <td>1.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-23</th>\n",
       "      <td>1.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-22</th>\n",
       "      <td>1.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-21</th>\n",
       "      <td>1.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-20</th>\n",
       "      <td>1.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-19</th>\n",
       "      <td>1.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-18</th>\n",
       "      <td>1.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-17</th>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-16</th>\n",
       "      <td>1.76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-15</th>\n",
       "      <td>1.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-14</th>\n",
       "      <td>1.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-13</th>\n",
       "      <td>1.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-12</th>\n",
       "      <td>1.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-11</th>\n",
       "      <td>1.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-10</th>\n",
       "      <td>1.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-09</th>\n",
       "      <td>1.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-08</th>\n",
       "      <td>1.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-07</th>\n",
       "      <td>1.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-06</th>\n",
       "      <td>1.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-05</th>\n",
       "      <td>1.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-04</th>\n",
       "      <td>1.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-03</th>\n",
       "      <td>2.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-02</th>\n",
       "      <td>2.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-01</th>\n",
       "      <td>2.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>2.13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-30</th>\n",
       "      <td>2.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-29</th>\n",
       "      <td>1.93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-28</th>\n",
       "      <td>1.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-27</th>\n",
       "      <td>1.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-26</th>\n",
       "      <td>1.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-25</th>\n",
       "      <td>1.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-24</th>\n",
       "      <td>1.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-23</th>\n",
       "      <td>1.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-22</th>\n",
       "      <td>1.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-21</th>\n",
       "      <td>1.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-20</th>\n",
       "      <td>2.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-19</th>\n",
       "      <td>1.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-18</th>\n",
       "      <td>2.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-17</th>\n",
       "      <td>2.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-16</th>\n",
       "      <td>1.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-15</th>\n",
       "      <td>1.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-14</th>\n",
       "      <td>1.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-13</th>\n",
       "      <td>1.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-12</th>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-11</th>\n",
       "      <td>2.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-10</th>\n",
       "      <td>1.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-09</th>\n",
       "      <td>1.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-08</th>\n",
       "      <td>1.86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-07</th>\n",
       "      <td>1.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-06</th>\n",
       "      <td>1.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-05</th>\n",
       "      <td>1.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-04</th>\n",
       "      <td>1.49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-03</th>\n",
       "      <td>1.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-02</th>\n",
       "      <td>1.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-01</th>\n",
       "      <td>1.93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>1.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-29</th>\n",
       "      <td>1.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28</th>\n",
       "      <td>1.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-27</th>\n",
       "      <td>1.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-26</th>\n",
       "      <td>2.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-25</th>\n",
       "      <td>1.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-24</th>\n",
       "      <td>1.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-23</th>\n",
       "      <td>1.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-22</th>\n",
       "      <td>2.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-21</th>\n",
       "      <td>2.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-20</th>\n",
       "      <td>2.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-19</th>\n",
       "      <td>2.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-18</th>\n",
       "      <td>2.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-16</th>\n",
       "      <td>2.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-15</th>\n",
       "      <td>2.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-14</th>\n",
       "      <td>2.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-13</th>\n",
       "      <td>2.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-12</th>\n",
       "      <td>2.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-11</th>\n",
       "      <td>2.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-10</th>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-09</th>\n",
       "      <td>2.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-08</th>\n",
       "      <td>2.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-07</th>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-06</th>\n",
       "      <td>2.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-05</th>\n",
       "      <td>2.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-04</th>\n",
       "      <td>2.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-03</th>\n",
       "      <td>2.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-02</th>\n",
       "      <td>2.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01</th>\n",
       "      <td>2.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>2.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-30</th>\n",
       "      <td>2.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-29</th>\n",
       "      <td>2.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28</th>\n",
       "      <td>2.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-27</th>\n",
       "      <td>2.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-26</th>\n",
       "      <td>2.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-25</th>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-24</th>\n",
       "      <td>2.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-23</th>\n",
       "      <td>2.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-22</th>\n",
       "      <td>2.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-21</th>\n",
       "      <td>2.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-20</th>\n",
       "      <td>2.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-19</th>\n",
       "      <td>2.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-18</th>\n",
       "      <td>2.15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-17</th>\n",
       "      <td>1.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-16</th>\n",
       "      <td>1.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-15</th>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-14</th>\n",
       "      <td>1.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-13</th>\n",
       "      <td>1.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-12</th>\n",
       "      <td>1.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-11</th>\n",
       "      <td>1.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-10</th>\n",
       "      <td>1.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-09</th>\n",
       "      <td>2.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-08</th>\n",
       "      <td>2.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-07</th>\n",
       "      <td>2.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-06</th>\n",
       "      <td>1.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-05</th>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-04</th>\n",
       "      <td>1.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-03</th>\n",
       "      <td>1.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-02</th>\n",
       "      <td>1.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01</th>\n",
       "      <td>1.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>1.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-27</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-26</th>\n",
       "      <td>2.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-25</th>\n",
       "      <td>2.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-24</th>\n",
       "      <td>2.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-23</th>\n",
       "      <td>2.09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-22</th>\n",
       "      <td>2.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-21</th>\n",
       "      <td>2.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-20</th>\n",
       "      <td>2.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-19</th>\n",
       "      <td>2.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-18</th>\n",
       "      <td>2.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-17</th>\n",
       "      <td>2.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-16</th>\n",
       "      <td>2.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-15</th>\n",
       "      <td>2.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-14</th>\n",
       "      <td>2.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-13</th>\n",
       "      <td>2.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-12</th>\n",
       "      <td>2.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-11</th>\n",
       "      <td>2.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-10</th>\n",
       "      <td>2.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-09</th>\n",
       "      <td>2.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-08</th>\n",
       "      <td>1.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-07</th>\n",
       "      <td>1.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-06</th>\n",
       "      <td>2.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-05</th>\n",
       "      <td>2.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-04</th>\n",
       "      <td>2.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-03</th>\n",
       "      <td>2.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-02</th>\n",
       "      <td>2.14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>2.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>2.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-30</th>\n",
       "      <td>2.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-29</th>\n",
       "      <td>2.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-28</th>\n",
       "      <td>2.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-27</th>\n",
       "      <td>2.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-26</th>\n",
       "      <td>2.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-25</th>\n",
       "      <td>2.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-24</th>\n",
       "      <td>1.93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-23</th>\n",
       "      <td>2.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-22</th>\n",
       "      <td>2.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-21</th>\n",
       "      <td>2.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-20</th>\n",
       "      <td>2.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-19</th>\n",
       "      <td>2.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-18</th>\n",
       "      <td>2.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-17</th>\n",
       "      <td>2.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-16</th>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-15</th>\n",
       "      <td>2.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-14</th>\n",
       "      <td>2.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-13</th>\n",
       "      <td>2.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-12</th>\n",
       "      <td>2.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-11</th>\n",
       "      <td>2.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>2.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>2.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>2.13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>1.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>1.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>1.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>1.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>1.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>1.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>1.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-30</th>\n",
       "      <td>1.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29</th>\n",
       "      <td>1.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-28</th>\n",
       "      <td>1.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-27</th>\n",
       "      <td>1.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-26</th>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-25</th>\n",
       "      <td>1.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-24</th>\n",
       "      <td>1.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-23</th>\n",
       "      <td>1.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-22</th>\n",
       "      <td>1.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-21</th>\n",
       "      <td>1.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-20</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-19</th>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-18</th>\n",
       "      <td>1.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-17</th>\n",
       "      <td>2.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-15</th>\n",
       "      <td>1.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-14</th>\n",
       "      <td>1.86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-13</th>\n",
       "      <td>2.09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-12</th>\n",
       "      <td>2.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-11</th>\n",
       "      <td>2.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-10</th>\n",
       "      <td>2.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-09</th>\n",
       "      <td>2.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-08</th>\n",
       "      <td>1.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-07</th>\n",
       "      <td>2.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-06</th>\n",
       "      <td>1.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-05</th>\n",
       "      <td>1.49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-04</th>\n",
       "      <td>1.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-03</th>\n",
       "      <td>1.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-02</th>\n",
       "      <td>1.91</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>2.13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>2.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29</th>\n",
       "      <td>1.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28</th>\n",
       "      <td>1.86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27</th>\n",
       "      <td>1.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26</th>\n",
       "      <td>1.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-25</th>\n",
       "      <td>1.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-24</th>\n",
       "      <td>1.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-23</th>\n",
       "      <td>1.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22</th>\n",
       "      <td>1.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21</th>\n",
       "      <td>1.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20</th>\n",
       "      <td>1.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19</th>\n",
       "      <td>1.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18</th>\n",
       "      <td>1.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17</th>\n",
       "      <td>1.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16</th>\n",
       "      <td>1.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15</th>\n",
       "      <td>2.09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14</th>\n",
       "      <td>2.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-13</th>\n",
       "      <td>2.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-12</th>\n",
       "      <td>2.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-11</th>\n",
       "      <td>3.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-10</th>\n",
       "      <td>3.15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-09</th>\n",
       "      <td>3.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-08</th>\n",
       "      <td>3.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06</th>\n",
       "      <td>2.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05</th>\n",
       "      <td>2.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04</th>\n",
       "      <td>2.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03</th>\n",
       "      <td>2.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02</th>\n",
       "      <td>2.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01</th>\n",
       "      <td>2.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-31</th>\n",
       "      <td>2.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-30</th>\n",
       "      <td>2.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-29</th>\n",
       "      <td>2.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-28</th>\n",
       "      <td>1.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-27</th>\n",
       "      <td>1.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-26</th>\n",
       "      <td>2.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-25</th>\n",
       "      <td>2.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-24</th>\n",
       "      <td>1.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-23</th>\n",
       "      <td>1.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-22</th>\n",
       "      <td>1.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-21</th>\n",
       "      <td>1.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-20</th>\n",
       "      <td>1.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-19</th>\n",
       "      <td>1.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-18</th>\n",
       "      <td>1.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-17</th>\n",
       "      <td>1.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-16</th>\n",
       "      <td>1.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-15</th>\n",
       "      <td>1.59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-14</th>\n",
       "      <td>1.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-13</th>\n",
       "      <td>1.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-12</th>\n",
       "      <td>1.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-11</th>\n",
       "      <td>1.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-10</th>\n",
       "      <td>1.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-09</th>\n",
       "      <td>1.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-08</th>\n",
       "      <td>1.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-07</th>\n",
       "      <td>1.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-06</th>\n",
       "      <td>1.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-05</th>\n",
       "      <td>1.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-04</th>\n",
       "      <td>1.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-03</th>\n",
       "      <td>2.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-02</th>\n",
       "      <td>2.13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-01</th>\n",
       "      <td>1.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>1.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-29</th>\n",
       "      <td>1.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-28</th>\n",
       "      <td>1.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-27</th>\n",
       "      <td>1.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26</th>\n",
       "      <td>1.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-25</th>\n",
       "      <td>2.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-24</th>\n",
       "      <td>2.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-23</th>\n",
       "      <td>2.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-22</th>\n",
       "      <td>2.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-21</th>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-20</th>\n",
       "      <td>1.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-19</th>\n",
       "      <td>1.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-18</th>\n",
       "      <td>2.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-17</th>\n",
       "      <td>2.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-16</th>\n",
       "      <td>2.14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-15</th>\n",
       "      <td>2.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-14</th>\n",
       "      <td>2.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-13</th>\n",
       "      <td>2.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-12</th>\n",
       "      <td>2.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-11</th>\n",
       "      <td>2.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-10</th>\n",
       "      <td>2.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-09</th>\n",
       "      <td>2.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-08</th>\n",
       "      <td>3.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-07</th>\n",
       "      <td>3.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-06</th>\n",
       "      <td>3.59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-05</th>\n",
       "      <td>4.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-04</th>\n",
       "      <td>4.15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-03</th>\n",
       "      <td>3.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-02</th>\n",
       "      <td>3.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-01</th>\n",
       "      <td>3.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-31</th>\n",
       "      <td>2.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-30</th>\n",
       "      <td>2.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-29</th>\n",
       "      <td>2.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-28</th>\n",
       "      <td>2.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-27</th>\n",
       "      <td>2.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-26</th>\n",
       "      <td>2.49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-25</th>\n",
       "      <td>2.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-24</th>\n",
       "      <td>2.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-23</th>\n",
       "      <td>2.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-22</th>\n",
       "      <td>2.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-21</th>\n",
       "      <td>2.93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-20</th>\n",
       "      <td>3.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-19</th>\n",
       "      <td>3.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-18</th>\n",
       "      <td>3.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-17</th>\n",
       "      <td>3.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-16</th>\n",
       "      <td>3.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-15</th>\n",
       "      <td>3.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-14</th>\n",
       "      <td>3.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-13</th>\n",
       "      <td>4.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-12</th>\n",
       "      <td>3.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-11</th>\n",
       "      <td>3.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-10</th>\n",
       "      <td>3.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-09</th>\n",
       "      <td>3.59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-08</th>\n",
       "      <td>3.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-07</th>\n",
       "      <td>3.49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-06</th>\n",
       "      <td>3.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-05</th>\n",
       "      <td>3.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-04</th>\n",
       "      <td>3.76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-03</th>\n",
       "      <td>3.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-02</th>\n",
       "      <td>3.09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-01</th>\n",
       "      <td>3.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>3.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-30</th>\n",
       "      <td>3.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-29</th>\n",
       "      <td>3.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-28</th>\n",
       "      <td>3.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-27</th>\n",
       "      <td>3.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-26</th>\n",
       "      <td>3.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-25</th>\n",
       "      <td>3.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-24</th>\n",
       "      <td>4.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-23</th>\n",
       "      <td>3.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-22</th>\n",
       "      <td>3.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-21</th>\n",
       "      <td>3.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-20</th>\n",
       "      <td>3.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-19</th>\n",
       "      <td>3.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-18</th>\n",
       "      <td>3.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-17</th>\n",
       "      <td>3.59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-16</th>\n",
       "      <td>3.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-15</th>\n",
       "      <td>3.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-14</th>\n",
       "      <td>3.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-13</th>\n",
       "      <td>3.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-12</th>\n",
       "      <td>3.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-11</th>\n",
       "      <td>3.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-10</th>\n",
       "      <td>3.91</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-09</th>\n",
       "      <td>4.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-08</th>\n",
       "      <td>3.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-07</th>\n",
       "      <td>3.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-06</th>\n",
       "      <td>3.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-05</th>\n",
       "      <td>3.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-04</th>\n",
       "      <td>3.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-03</th>\n",
       "      <td>3.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-02</th>\n",
       "      <td>2.86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-01</th>\n",
       "      <td>2.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>2.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-29</th>\n",
       "      <td>2.14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-28</th>\n",
       "      <td>7.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-27</th>\n",
       "      <td>7.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-26</th>\n",
       "      <td>7.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-25</th>\n",
       "      <td>7.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-24</th>\n",
       "      <td>7.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-22</th>\n",
       "      <td>7.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-21</th>\n",
       "      <td>7.08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-20</th>\n",
       "      <td>7.15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-19</th>\n",
       "      <td>7.17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-18</th>\n",
       "      <td>7.18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-17</th>\n",
       "      <td>7.17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-16</th>\n",
       "      <td>7.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-15</th>\n",
       "      <td>7.43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-14</th>\n",
       "      <td>7.43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-13</th>\n",
       "      <td>7.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-10</th>\n",
       "      <td>7.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-09</th>\n",
       "      <td>7.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-08</th>\n",
       "      <td>7.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-07</th>\n",
       "      <td>7.47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-06</th>\n",
       "      <td>7.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-04</th>\n",
       "      <td>7.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-03</th>\n",
       "      <td>7.44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-02</th>\n",
       "      <td>7.44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-01</th>\n",
       "      <td>7.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31</th>\n",
       "      <td>7.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-28</th>\n",
       "      <td>7.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-27</th>\n",
       "      <td>7.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-26</th>\n",
       "      <td>7.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-25</th>\n",
       "      <td>7.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-24</th>\n",
       "      <td>7.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-23</th>\n",
       "      <td>7.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-22</th>\n",
       "      <td>6.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-21</th>\n",
       "      <td>6.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-20</th>\n",
       "      <td>7.08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-19</th>\n",
       "      <td>7.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-18</th>\n",
       "      <td>7.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-17</th>\n",
       "      <td>7.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-16</th>\n",
       "      <td>7.15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-15</th>\n",
       "      <td>7.14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-14</th>\n",
       "      <td>7.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-13</th>\n",
       "      <td>7.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-12</th>\n",
       "      <td>7.13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-11</th>\n",
       "      <td>6.96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-10</th>\n",
       "      <td>6.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-09</th>\n",
       "      <td>6.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-08</th>\n",
       "      <td>6.36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-07</th>\n",
       "      <td>6.08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-06</th>\n",
       "      <td>5.97</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-05</th>\n",
       "      <td>6.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-04</th>\n",
       "      <td>6.27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-03</th>\n",
       "      <td>6.28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-02</th>\n",
       "      <td>6.13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01</th>\n",
       "      <td>5.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-30</th>\n",
       "      <td>5.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-29</th>\n",
       "      <td>5.83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-28</th>\n",
       "      <td>5.73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-27</th>\n",
       "      <td>5.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-26</th>\n",
       "      <td>6.33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-25</th>\n",
       "      <td>5.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-24</th>\n",
       "      <td>5.86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-23</th>\n",
       "      <td>5.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-22</th>\n",
       "      <td>5.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-21</th>\n",
       "      <td>5.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-20</th>\n",
       "      <td>5.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-19</th>\n",
       "      <td>5.64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-18</th>\n",
       "      <td>5.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-17</th>\n",
       "      <td>5.82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-16</th>\n",
       "      <td>5.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-15</th>\n",
       "      <td>5.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-14</th>\n",
       "      <td>5.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-13</th>\n",
       "      <td>5.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-12</th>\n",
       "      <td>5.94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-11</th>\n",
       "      <td>6.16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-10</th>\n",
       "      <td>6.33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-09</th>\n",
       "      <td>6.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-08</th>\n",
       "      <td>6.34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-07</th>\n",
       "      <td>6.61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-06</th>\n",
       "      <td>6.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-05</th>\n",
       "      <td>6.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-04</th>\n",
       "      <td>7.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-31</th>\n",
       "      <td>7.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-30</th>\n",
       "      <td>7.06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-29</th>\n",
       "      <td>7.19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-28</th>\n",
       "      <td>7.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-27</th>\n",
       "      <td>7.63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-26</th>\n",
       "      <td>7.63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-25</th>\n",
       "      <td>7.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-24</th>\n",
       "      <td>7.97</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-23</th>\n",
       "      <td>7.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-22</th>\n",
       "      <td>7.79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-21</th>\n",
       "      <td>7.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-17</th>\n",
       "      <td>7.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-16</th>\n",
       "      <td>7.91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-15</th>\n",
       "      <td>8.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-14</th>\n",
       "      <td>8.64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-13</th>\n",
       "      <td>9.09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-12</th>\n",
       "      <td>9.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-11</th>\n",
       "      <td>9.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-10</th>\n",
       "      <td>9.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-09</th>\n",
       "      <td>9.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-08</th>\n",
       "      <td>9.79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-05</th>\n",
       "      <td>9.79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-04</th>\n",
       "      <td>9.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-03</th>\n",
       "      <td>9.96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-02</th>\n",
       "      <td>10.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01</th>\n",
       "      <td>10.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-29</th>\n",
       "      <td>10.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-28</th>\n",
       "      <td>11.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-27</th>\n",
       "      <td>11.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-25</th>\n",
       "      <td>11.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-24</th>\n",
       "      <td>11.04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-23</th>\n",
       "      <td>11.35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-22</th>\n",
       "      <td>11.42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-20</th>\n",
       "      <td>11.42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-19</th>\n",
       "      <td>11.13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-18</th>\n",
       "      <td>11.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-17</th>\n",
       "      <td>11.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-16</th>\n",
       "      <td>10.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-15</th>\n",
       "      <td>10.72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-14</th>\n",
       "      <td>10.73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-13</th>\n",
       "      <td>10.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-12</th>\n",
       "      <td>10.73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-11</th>\n",
       "      <td>10.72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-10</th>\n",
       "      <td>10.72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-09</th>\n",
       "      <td>10.06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-08</th>\n",
       "      <td>9.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-07</th>\n",
       "      <td>9.94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-06</th>\n",
       "      <td>9.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-05</th>\n",
       "      <td>9.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-04</th>\n",
       "      <td>10.04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-03</th>\n",
       "      <td>11.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-02</th>\n",
       "      <td>11.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-01</th>\n",
       "      <td>11.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-31</th>\n",
       "      <td>11.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-30</th>\n",
       "      <td>11.78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-29</th>\n",
       "      <td>11.82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-28</th>\n",
       "      <td>11.95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-27</th>\n",
       "      <td>12.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-26</th>\n",
       "      <td>12.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-25</th>\n",
       "      <td>12.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-24</th>\n",
       "      <td>12.06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-23</th>\n",
       "      <td>11.86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-22</th>\n",
       "      <td>12.22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-21</th>\n",
       "      <td>11.98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-20</th>\n",
       "      <td>12.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-19</th>\n",
       "      <td>12.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-18</th>\n",
       "      <td>12.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-17</th>\n",
       "      <td>12.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-16</th>\n",
       "      <td>11.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-15</th>\n",
       "      <td>11.98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-14</th>\n",
       "      <td>12.09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-13</th>\n",
       "      <td>11.97</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-12</th>\n",
       "      <td>11.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-11</th>\n",
       "      <td>11.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-10</th>\n",
       "      <td>11.94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-09</th>\n",
       "      <td>12.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-08</th>\n",
       "      <td>11.96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-07</th>\n",
       "      <td>12.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-06</th>\n",
       "      <td>12.72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-05</th>\n",
       "      <td>13.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-04</th>\n",
       "      <td>13.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>13.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02</th>\n",
       "      <td>13.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>13.12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>13.13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>13.14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-29</th>\n",
       "      <td>13.22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-28</th>\n",
       "      <td>13.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>13.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-26</th>\n",
       "      <td>13.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-25</th>\n",
       "      <td>13.09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-24</th>\n",
       "      <td>12.59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-23</th>\n",
       "      <td>12.44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-22</th>\n",
       "      <td>11.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-21</th>\n",
       "      <td>11.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-20</th>\n",
       "      <td>11.47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-19</th>\n",
       "      <td>11.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-18</th>\n",
       "      <td>11.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-17</th>\n",
       "      <td>11.49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-16</th>\n",
       "      <td>11.27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-15</th>\n",
       "      <td>10.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-14</th>\n",
       "      <td>10.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-13</th>\n",
       "      <td>10.96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12</th>\n",
       "      <td>10.86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-11</th>\n",
       "      <td>10.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-10</th>\n",
       "      <td>11.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-09</th>\n",
       "      <td>11.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-08</th>\n",
       "      <td>11.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-07</th>\n",
       "      <td>11.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-06</th>\n",
       "      <td>11.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-05</th>\n",
       "      <td>12.35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-04</th>\n",
       "      <td>12.12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-03</th>\n",
       "      <td>12.19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-02</th>\n",
       "      <td>12.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>12.17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>12.17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-29</th>\n",
       "      <td>12.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-28</th>\n",
       "      <td>12.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-27</th>\n",
       "      <td>12.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-26</th>\n",
       "      <td>12.94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-25</th>\n",
       "      <td>13.12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-24</th>\n",
       "      <td>12.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-23</th>\n",
       "      <td>12.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-22</th>\n",
       "      <td>12.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-21</th>\n",
       "      <td>12.16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-20</th>\n",
       "      <td>12.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-19</th>\n",
       "      <td>12.06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-18</th>\n",
       "      <td>11.94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-17</th>\n",
       "      <td>11.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-16</th>\n",
       "      <td>12.04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-15</th>\n",
       "      <td>13.36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-14</th>\n",
       "      <td>13.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-13</th>\n",
       "      <td>12.86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-12</th>\n",
       "      <td>12.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-11</th>\n",
       "      <td>13.09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-10</th>\n",
       "      <td>11.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-09</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-08</th>\n",
       "      <td>10.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-07</th>\n",
       "      <td>10.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-06</th>\n",
       "      <td>9.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-05</th>\n",
       "      <td>9.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-04</th>\n",
       "      <td>8.91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-03</th>\n",
       "      <td>8.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-02</th>\n",
       "      <td>8.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>9.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>9.51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-30</th>\n",
       "      <td>7.97</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-29</th>\n",
       "      <td>7.61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-28</th>\n",
       "      <td>8.22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-27</th>\n",
       "      <td>8.17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-26</th>\n",
       "      <td>7.97</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-25</th>\n",
       "      <td>6.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-24</th>\n",
       "      <td>4.95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-23</th>\n",
       "      <td>3.95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-22</th>\n",
       "      <td>3.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-21</th>\n",
       "      <td>3.13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-20</th>\n",
       "      <td>3.12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-19</th>\n",
       "      <td>2.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-18</th>\n",
       "      <td>2.83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-17</th>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-16</th>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-15</th>\n",
       "      <td>2.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-14</th>\n",
       "      <td>2.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-13</th>\n",
       "      <td>2.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-12</th>\n",
       "      <td>2.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-11</th>\n",
       "      <td>2.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-10</th>\n",
       "      <td>2.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-09</th>\n",
       "      <td>2.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-08</th>\n",
       "      <td>2.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07</th>\n",
       "      <td>2.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-06</th>\n",
       "      <td>2.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02</th>\n",
       "      <td>5.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>5.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>5.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>5.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-29</th>\n",
       "      <td>6.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-28</th>\n",
       "      <td>7.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>7.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-26</th>\n",
       "      <td>7.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-25</th>\n",
       "      <td>7.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-24</th>\n",
       "      <td>8.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-23</th>\n",
       "      <td>10.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-22</th>\n",
       "      <td>11.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-21</th>\n",
       "      <td>10.14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-20</th>\n",
       "      <td>9.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-19</th>\n",
       "      <td>8.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-18</th>\n",
       "      <td>9.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-17</th>\n",
       "      <td>9.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-16</th>\n",
       "      <td>7.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-15</th>\n",
       "      <td>7.93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-14</th>\n",
       "      <td>8.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-13</th>\n",
       "      <td>8.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12</th>\n",
       "      <td>8.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-11</th>\n",
       "      <td>8.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-10</th>\n",
       "      <td>7.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-09</th>\n",
       "      <td>8.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-08</th>\n",
       "      <td>8.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-07</th>\n",
       "      <td>8.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-06</th>\n",
       "      <td>8.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-05</th>\n",
       "      <td>8.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-04</th>\n",
       "      <td>8.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-03</th>\n",
       "      <td>7.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-02</th>\n",
       "      <td>6.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>6.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-28</th>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-27</th>\n",
       "      <td>3.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-26</th>\n",
       "      <td>3.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-25</th>\n",
       "      <td>3.59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-24</th>\n",
       "      <td>3.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-23</th>\n",
       "      <td>3.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-22</th>\n",
       "      <td>3.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-21</th>\n",
       "      <td>3.15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-20</th>\n",
       "      <td>3.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-19</th>\n",
       "      <td>3.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-18</th>\n",
       "      <td>4.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-17</th>\n",
       "      <td>4.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-16</th>\n",
       "      <td>4.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-15</th>\n",
       "      <td>4.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-14</th>\n",
       "      <td>4.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-13</th>\n",
       "      <td>4.49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-12</th>\n",
       "      <td>5.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-11</th>\n",
       "      <td>4.59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-10</th>\n",
       "      <td>4.13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-09</th>\n",
       "      <td>4.09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-08</th>\n",
       "      <td>4.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-07</th>\n",
       "      <td>4.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-06</th>\n",
       "      <td>4.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-05</th>\n",
       "      <td>4.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-04</th>\n",
       "      <td>4.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-03</th>\n",
       "      <td>3.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-02</th>\n",
       "      <td>3.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-01</th>\n",
       "      <td>3.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31</th>\n",
       "      <td>3.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-30</th>\n",
       "      <td>4.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-29</th>\n",
       "      <td>4.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-28</th>\n",
       "      <td>4.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-27</th>\n",
       "      <td>4.86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-26</th>\n",
       "      <td>5.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-25</th>\n",
       "      <td>5.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-24</th>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-23</th>\n",
       "      <td>5.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-22</th>\n",
       "      <td>4.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-21</th>\n",
       "      <td>5.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-20</th>\n",
       "      <td>5.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-19</th>\n",
       "      <td>5.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-18</th>\n",
       "      <td>5.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-17</th>\n",
       "      <td>5.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-16</th>\n",
       "      <td>5.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-15</th>\n",
       "      <td>5.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-14</th>\n",
       "      <td>5.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-13</th>\n",
       "      <td>4.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-12</th>\n",
       "      <td>4.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-11</th>\n",
       "      <td>4.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-10</th>\n",
       "      <td>5.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-09</th>\n",
       "      <td>6.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-08</th>\n",
       "      <td>5.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-07</th>\n",
       "      <td>5.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-06</th>\n",
       "      <td>5.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-05</th>\n",
       "      <td>5.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-04</th>\n",
       "      <td>5.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-03</th>\n",
       "      <td>5.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-03</th>\n",
       "      <td>2.98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-02</th>\n",
       "      <td>2.98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-01</th>\n",
       "      <td>2.95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-31</th>\n",
       "      <td>2.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-30</th>\n",
       "      <td>2.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-29</th>\n",
       "      <td>2.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-28</th>\n",
       "      <td>2.61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-27</th>\n",
       "      <td>2.61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-26</th>\n",
       "      <td>2.63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-25</th>\n",
       "      <td>2.64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-23</th>\n",
       "      <td>2.64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-22</th>\n",
       "      <td>2.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-21</th>\n",
       "      <td>2.79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-19</th>\n",
       "      <td>2.79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-18</th>\n",
       "      <td>2.78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-17</th>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-16</th>\n",
       "      <td>2.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-15</th>\n",
       "      <td>2.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-14</th>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-13</th>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-12</th>\n",
       "      <td>2.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-11</th>\n",
       "      <td>2.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-10</th>\n",
       "      <td>2.72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-09</th>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-08</th>\n",
       "      <td>2.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-07</th>\n",
       "      <td>2.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-06</th>\n",
       "      <td>2.73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-05</th>\n",
       "      <td>2.79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-04</th>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-01</th>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>3.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-30</th>\n",
       "      <td>3.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-29</th>\n",
       "      <td>3.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-28</th>\n",
       "      <td>3.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-27</th>\n",
       "      <td>3.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-26</th>\n",
       "      <td>3.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-25</th>\n",
       "      <td>3.55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-24</th>\n",
       "      <td>3.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-23</th>\n",
       "      <td>3.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-22</th>\n",
       "      <td>3.51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-21</th>\n",
       "      <td>3.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-20</th>\n",
       "      <td>3.24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-19</th>\n",
       "      <td>3.24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-18</th>\n",
       "      <td>3.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-17</th>\n",
       "      <td>3.24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-16</th>\n",
       "      <td>3.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-15</th>\n",
       "      <td>3.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-14</th>\n",
       "      <td>3.61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-13</th>\n",
       "      <td>3.63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-11</th>\n",
       "      <td>3.63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-10</th>\n",
       "      <td>3.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-09</th>\n",
       "      <td>3.63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-08</th>\n",
       "      <td>3.64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-07</th>\n",
       "      <td>3.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-06</th>\n",
       "      <td>3.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-05</th>\n",
       "      <td>3.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-04</th>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-03</th>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-02</th>\n",
       "      <td>3.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01</th>\n",
       "      <td>4.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>3.95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-29</th>\n",
       "      <td>3.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-28</th>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-27</th>\n",
       "      <td>3.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-26</th>\n",
       "      <td>3.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-25</th>\n",
       "      <td>3.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-24</th>\n",
       "      <td>3.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-23</th>\n",
       "      <td>3.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-22</th>\n",
       "      <td>3.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-21</th>\n",
       "      <td>3.36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-20</th>\n",
       "      <td>3.51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-19</th>\n",
       "      <td>3.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-18</th>\n",
       "      <td>3.47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-17</th>\n",
       "      <td>3.34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-16</th>\n",
       "      <td>3.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-15</th>\n",
       "      <td>3.12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-14</th>\n",
       "      <td>3.06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-13</th>\n",
       "      <td>3.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-12</th>\n",
       "      <td>2.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-11</th>\n",
       "      <td>2.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-10</th>\n",
       "      <td>2.63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-09</th>\n",
       "      <td>2.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-08</th>\n",
       "      <td>2.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-07</th>\n",
       "      <td>2.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-06</th>\n",
       "      <td>2.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-05</th>\n",
       "      <td>2.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-04</th>\n",
       "      <td>2.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-03</th>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-02</th>\n",
       "      <td>2.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-01</th>\n",
       "      <td>2.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>2.78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-30</th>\n",
       "      <td>2.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-29</th>\n",
       "      <td>2.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-28</th>\n",
       "      <td>2.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-27</th>\n",
       "      <td>2.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-26</th>\n",
       "      <td>2.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-25</th>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-24</th>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-23</th>\n",
       "      <td>2.72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-22</th>\n",
       "      <td>2.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-21</th>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-20</th>\n",
       "      <td>2.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-19</th>\n",
       "      <td>2.83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-18</th>\n",
       "      <td>2.86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-17</th>\n",
       "      <td>3.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-16</th>\n",
       "      <td>3.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-15</th>\n",
       "      <td>2.86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-14</th>\n",
       "      <td>2.79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-13</th>\n",
       "      <td>2.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-12</th>\n",
       "      <td>2.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-11</th>\n",
       "      <td>2.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-10</th>\n",
       "      <td>2.49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-09</th>\n",
       "      <td>2.43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-08</th>\n",
       "      <td>2.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-07</th>\n",
       "      <td>2.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-06</th>\n",
       "      <td>2.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-05</th>\n",
       "      <td>2.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-04</th>\n",
       "      <td>2.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-03</th>\n",
       "      <td>2.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-02</th>\n",
       "      <td>2.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01</th>\n",
       "      <td>1.98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>1.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-29</th>\n",
       "      <td>1.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-28</th>\n",
       "      <td>2.04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-27</th>\n",
       "      <td>2.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-26</th>\n",
       "      <td>2.04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-25</th>\n",
       "      <td>2.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-24</th>\n",
       "      <td>2.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-23</th>\n",
       "      <td>2.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-22</th>\n",
       "      <td>2.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-21</th>\n",
       "      <td>2.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-20</th>\n",
       "      <td>1.96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-19</th>\n",
       "      <td>1.91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-18</th>\n",
       "      <td>1.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-17</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-15</th>\n",
       "      <td>1.59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-14</th>\n",
       "      <td>1.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-13</th>\n",
       "      <td>1.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-12</th>\n",
       "      <td>1.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-11</th>\n",
       "      <td>1.44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-10</th>\n",
       "      <td>1.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-09</th>\n",
       "      <td>1.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-08</th>\n",
       "      <td>1.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-07</th>\n",
       "      <td>1.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-06</th>\n",
       "      <td>1.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-05</th>\n",
       "      <td>1.35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-04</th>\n",
       "      <td>1.35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-03</th>\n",
       "      <td>1.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-02</th>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-01</th>\n",
       "      <td>1.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>1.28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30</th>\n",
       "      <td>1.34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-29</th>\n",
       "      <td>1.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-28</th>\n",
       "      <td>1.59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-27</th>\n",
       "      <td>1.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-26</th>\n",
       "      <td>1.35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-25</th>\n",
       "      <td>1.27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-24</th>\n",
       "      <td>1.41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-23</th>\n",
       "      <td>1.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-22</th>\n",
       "      <td>1.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-21</th>\n",
       "      <td>1.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-20</th>\n",
       "      <td>1.33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-19</th>\n",
       "      <td>1.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-18</th>\n",
       "      <td>1.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-17</th>\n",
       "      <td>1.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16</th>\n",
       "      <td>1.09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-15</th>\n",
       "      <td>1.08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-14</th>\n",
       "      <td>1.08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-13</th>\n",
       "      <td>1.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-12</th>\n",
       "      <td>1.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-11</th>\n",
       "      <td>1.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-10</th>\n",
       "      <td>1.12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-09</th>\n",
       "      <td>1.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-08</th>\n",
       "      <td>1.28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-07</th>\n",
       "      <td>1.17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06</th>\n",
       "      <td>1.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-05</th>\n",
       "      <td>0.98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-04</th>\n",
       "      <td>0.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-03</th>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-02</th>\n",
       "      <td>0.96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-01</th>\n",
       "      <td>0.91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>0.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-27</th>\n",
       "      <td>0.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-26</th>\n",
       "      <td>0.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-25</th>\n",
       "      <td>0.83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-24</th>\n",
       "      <td>0.79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-23</th>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-22</th>\n",
       "      <td>0.78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-21</th>\n",
       "      <td>0.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-20</th>\n",
       "      <td>0.72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-19</th>\n",
       "      <td>0.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-18</th>\n",
       "      <td>0.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-17</th>\n",
       "      <td>0.72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-16</th>\n",
       "      <td>0.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-15</th>\n",
       "      <td>0.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-14</th>\n",
       "      <td>0.78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-13</th>\n",
       "      <td>0.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-12</th>\n",
       "      <td>0.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-11</th>\n",
       "      <td>0.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-09</th>\n",
       "      <td>0.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-08</th>\n",
       "      <td>0.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-07</th>\n",
       "      <td>0.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-05</th>\n",
       "      <td>0.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-04</th>\n",
       "      <td>0.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-03</th>\n",
       "      <td>0.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-02</th>\n",
       "      <td>0.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01</th>\n",
       "      <td>0.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>0.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30</th>\n",
       "      <td>0.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-29</th>\n",
       "      <td>0.96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-28</th>\n",
       "      <td>1.03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-27</th>\n",
       "      <td>1.03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-26</th>\n",
       "      <td>1.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-25</th>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-24</th>\n",
       "      <td>0.83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-23</th>\n",
       "      <td>0.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22</th>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-21</th>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-20</th>\n",
       "      <td>0.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-19</th>\n",
       "      <td>0.91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-18</th>\n",
       "      <td>0.91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-17</th>\n",
       "      <td>0.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-16</th>\n",
       "      <td>1.17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-15</th>\n",
       "      <td>1.18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-14</th>\n",
       "      <td>1.18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-13</th>\n",
       "      <td>1.15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12</th>\n",
       "      <td>1.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-11</th>\n",
       "      <td>1.24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-10</th>\n",
       "      <td>1.24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-09</th>\n",
       "      <td>1.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-06</th>\n",
       "      <td>1.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-05</th>\n",
       "      <td>1.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04</th>\n",
       "      <td>1.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>1.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <td>1.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>1.06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>1.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-30</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-28</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-27</th>\n",
       "      <td>1.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-26</th>\n",
       "      <td>1.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-25</th>\n",
       "      <td>1.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-23</th>\n",
       "      <td>1.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-22</th>\n",
       "      <td>1.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-21</th>\n",
       "      <td>1.03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-20</th>\n",
       "      <td>1.03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-19</th>\n",
       "      <td>1.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-18</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-16</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-15</th>\n",
       "      <td>0.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-14</th>\n",
       "      <td>0.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-13</th>\n",
       "      <td>1.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-12</th>\n",
       "      <td>1.13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-10</th>\n",
       "      <td>1.13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-09</th>\n",
       "      <td>1.15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-08</th>\n",
       "      <td>1.15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-07</th>\n",
       "      <td>1.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-06</th>\n",
       "      <td>1.22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-05</th>\n",
       "      <td>1.19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-04</th>\n",
       "      <td>1.19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-03</th>\n",
       "      <td>1.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-02</th>\n",
       "      <td>1.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-01</th>\n",
       "      <td>1.18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>1.17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-28</th>\n",
       "      <td>1.17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-27</th>\n",
       "      <td>1.15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-25</th>\n",
       "      <td>1.15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-24</th>\n",
       "      <td>1.14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-23</th>\n",
       "      <td>1.12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-22</th>\n",
       "      <td>1.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-19</th>\n",
       "      <td>1.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-18</th>\n",
       "      <td>1.03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-17</th>\n",
       "      <td>0.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-16</th>\n",
       "      <td>0.97</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-15</th>\n",
       "      <td>0.97</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-14</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-13</th>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-12</th>\n",
       "      <td>0.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-11</th>\n",
       "      <td>0.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-10</th>\n",
       "      <td>0.86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-09</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-08</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-07</th>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-06</th>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-05</th>\n",
       "      <td>0.98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-04</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-03</th>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-02</th>\n",
       "      <td>0.96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-01</th>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>1.03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-30</th>\n",
       "      <td>1.22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-29</th>\n",
       "      <td>1.27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-28</th>\n",
       "      <td>1.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-27</th>\n",
       "      <td>1.47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-26</th>\n",
       "      <td>1.51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-25</th>\n",
       "      <td>1.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-24</th>\n",
       "      <td>1.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-23</th>\n",
       "      <td>1.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-22</th>\n",
       "      <td>1.94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-21</th>\n",
       "      <td>1.96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-20</th>\n",
       "      <td>2.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-19</th>\n",
       "      <td>2.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-18</th>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-17</th>\n",
       "      <td>2.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-16</th>\n",
       "      <td>2.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-15</th>\n",
       "      <td>2.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-14</th>\n",
       "      <td>2.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-13</th>\n",
       "      <td>2.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-12</th>\n",
       "      <td>2.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-11</th>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-10</th>\n",
       "      <td>2.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-09</th>\n",
       "      <td>2.78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-08</th>\n",
       "      <td>2.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-07</th>\n",
       "      <td>2.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-06</th>\n",
       "      <td>2.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-05</th>\n",
       "      <td>3.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-04</th>\n",
       "      <td>3.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-03</th>\n",
       "      <td>3.14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>3.14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-29</th>\n",
       "      <td>3.30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-28</th>\n",
       "      <td>3.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-27</th>\n",
       "      <td>3.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-26</th>\n",
       "      <td>3.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-25</th>\n",
       "      <td>3.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-24</th>\n",
       "      <td>3.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-23</th>\n",
       "      <td>3.51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-22</th>\n",
       "      <td>3.43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-21</th>\n",
       "      <td>3.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-19</th>\n",
       "      <td>3.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-18</th>\n",
       "      <td>3.35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-17</th>\n",
       "      <td>3.35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-16</th>\n",
       "      <td>3.33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-15</th>\n",
       "      <td>3.33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-14</th>\n",
       "      <td>3.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-13</th>\n",
       "      <td>3.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-12</th>\n",
       "      <td>3.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11</th>\n",
       "      <td>3.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-10</th>\n",
       "      <td>3.09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-09</th>\n",
       "      <td>3.09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-08</th>\n",
       "      <td>3.12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-07</th>\n",
       "      <td>3.12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-06</th>\n",
       "      <td>3.13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-05</th>\n",
       "      <td>3.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-04</th>\n",
       "      <td>3.09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-03</th>\n",
       "      <td>2.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-02</th>\n",
       "      <td>2.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-01</th>\n",
       "      <td>2.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>2.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-30</th>\n",
       "      <td>2.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-29</th>\n",
       "      <td>2.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-28</th>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-27</th>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-26</th>\n",
       "      <td>2.79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-25</th>\n",
       "      <td>2.78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-24</th>\n",
       "      <td>2.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-22</th>\n",
       "      <td>2.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-21</th>\n",
       "      <td>2.73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-20</th>\n",
       "      <td>2.73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-19</th>\n",
       "      <td>2.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-18</th>\n",
       "      <td>2.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-17</th>\n",
       "      <td>2.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-16</th>\n",
       "      <td>2.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-15</th>\n",
       "      <td>3.06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-14</th>\n",
       "      <td>3.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-13</th>\n",
       "      <td>2.91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-12</th>\n",
       "      <td>2.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-11</th>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-10</th>\n",
       "      <td>2.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-09</th>\n",
       "      <td>2.83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-08</th>\n",
       "      <td>2.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-07</th>\n",
       "      <td>2.55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-06</th>\n",
       "      <td>2.59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-05</th>\n",
       "      <td>2.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-04</th>\n",
       "      <td>2.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-03</th>\n",
       "      <td>2.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-02</th>\n",
       "      <td>2.72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-01</th>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>2.78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-30</th>\n",
       "      <td>2.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-29</th>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-28</th>\n",
       "      <td>2.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-27</th>\n",
       "      <td>2.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-26</th>\n",
       "      <td>2.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-25</th>\n",
       "      <td>2.86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-24</th>\n",
       "      <td>2.91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-23</th>\n",
       "      <td>2.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-22</th>\n",
       "      <td>2.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-21</th>\n",
       "      <td>2.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-20</th>\n",
       "      <td>2.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-19</th>\n",
       "      <td>2.82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18</th>\n",
       "      <td>2.83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-17</th>\n",
       "      <td>2.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-16</th>\n",
       "      <td>2.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-12</th>\n",
       "      <td>2.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-11</th>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-10</th>\n",
       "      <td>2.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-09</th>\n",
       "      <td>2.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-08</th>\n",
       "      <td>2.79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-07</th>\n",
       "      <td>2.96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-06</th>\n",
       "      <td>2.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-05</th>\n",
       "      <td>3.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-04</th>\n",
       "      <td>2.96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-03</th>\n",
       "      <td>2.95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-02</th>\n",
       "      <td>2.94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-01</th>\n",
       "      <td>2.91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>2.78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-29</th>\n",
       "      <td>2.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-27</th>\n",
       "      <td>2.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-26</th>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-25</th>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-24</th>\n",
       "      <td>2.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-22</th>\n",
       "      <td>2.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-21</th>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-20</th>\n",
       "      <td>2.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-19</th>\n",
       "      <td>2.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-18</th>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-17</th>\n",
       "      <td>2.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-16</th>\n",
       "      <td>2.55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-15</th>\n",
       "      <td>2.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-14</th>\n",
       "      <td>2.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-13</th>\n",
       "      <td>2.59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-12</th>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-11</th>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-10</th>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-09</th>\n",
       "      <td>2.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-08</th>\n",
       "      <td>2.63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-07</th>\n",
       "      <td>2.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-06</th>\n",
       "      <td>2.64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-05</th>\n",
       "      <td>2.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-04</th>\n",
       "      <td>2.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-03</th>\n",
       "      <td>2.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-02</th>\n",
       "      <td>2.64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-01</th>\n",
       "      <td>2.64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>2.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-30</th>\n",
       "      <td>2.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-29</th>\n",
       "      <td>2.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-28</th>\n",
       "      <td>2.51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-27</th>\n",
       "      <td>2.61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-26</th>\n",
       "      <td>2.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-25</th>\n",
       "      <td>2.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-24</th>\n",
       "      <td>2.78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-23</th>\n",
       "      <td>2.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-22</th>\n",
       "      <td>2.36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-21</th>\n",
       "      <td>2.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-20</th>\n",
       "      <td>2.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-19</th>\n",
       "      <td>2.28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-18</th>\n",
       "      <td>2.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-17</th>\n",
       "      <td>2.47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-16</th>\n",
       "      <td>2.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-15</th>\n",
       "      <td>2.34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-14</th>\n",
       "      <td>2.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-13</th>\n",
       "      <td>2.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-12</th>\n",
       "      <td>2.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-11</th>\n",
       "      <td>2.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-10</th>\n",
       "      <td>2.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-09</th>\n",
       "      <td>2.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-08</th>\n",
       "      <td>2.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-07</th>\n",
       "      <td>3.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-06</th>\n",
       "      <td>3.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-05</th>\n",
       "      <td>3.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-04</th>\n",
       "      <td>3.47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-03</th>\n",
       "      <td>3.44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-02</th>\n",
       "      <td>3.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-01</th>\n",
       "      <td>3.24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>3.28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-29</th>\n",
       "      <td>3.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28</th>\n",
       "      <td>3.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-26</th>\n",
       "      <td>3.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-25</th>\n",
       "      <td>3.49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-24</th>\n",
       "      <td>3.42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-23</th>\n",
       "      <td>3.41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-22</th>\n",
       "      <td>3.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-21</th>\n",
       "      <td>3.34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-20</th>\n",
       "      <td>3.33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-19</th>\n",
       "      <td>3.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-18</th>\n",
       "      <td>3.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-17</th>\n",
       "      <td>3.44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-16</th>\n",
       "      <td>3.46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-15</th>\n",
       "      <td>3.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-14</th>\n",
       "      <td>3.72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-13</th>\n",
       "      <td>3.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-12</th>\n",
       "      <td>3.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-11</th>\n",
       "      <td>3.95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-10</th>\n",
       "      <td>3.78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-09</th>\n",
       "      <td>3.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-08</th>\n",
       "      <td>3.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-07</th>\n",
       "      <td>3.47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-06</th>\n",
       "      <td>3.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-05</th>\n",
       "      <td>3.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-04</th>\n",
       "      <td>3.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-03</th>\n",
       "      <td>3.49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-02</th>\n",
       "      <td>3.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01</th>\n",
       "      <td>3.08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>2.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-30</th>\n",
       "      <td>2.83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-29</th>\n",
       "      <td>2.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28</th>\n",
       "      <td>2.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-27</th>\n",
       "      <td>2.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-26</th>\n",
       "      <td>2.72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-25</th>\n",
       "      <td>2.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-24</th>\n",
       "      <td>2.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-23</th>\n",
       "      <td>2.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-22</th>\n",
       "      <td>2.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-21</th>\n",
       "      <td>2.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-20</th>\n",
       "      <td>2.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-19</th>\n",
       "      <td>2.42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-18</th>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-17</th>\n",
       "      <td>2.22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-16</th>\n",
       "      <td>2.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-15</th>\n",
       "      <td>2.27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-14</th>\n",
       "      <td>2.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-13</th>\n",
       "      <td>2.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-12</th>\n",
       "      <td>2.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-11</th>\n",
       "      <td>2.24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-10</th>\n",
       "      <td>2.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-09</th>\n",
       "      <td>2.24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-08</th>\n",
       "      <td>2.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-07</th>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-06</th>\n",
       "      <td>2.34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-05</th>\n",
       "      <td>2.36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-04</th>\n",
       "      <td>2.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-03</th>\n",
       "      <td>2.22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-02</th>\n",
       "      <td>2.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01</th>\n",
       "      <td>2.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>2.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-27</th>\n",
       "      <td>2.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-26</th>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-25</th>\n",
       "      <td>2.38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-24</th>\n",
       "      <td>2.41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-23</th>\n",
       "      <td>2.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-22</th>\n",
       "      <td>2.49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-21</th>\n",
       "      <td>2.51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-20</th>\n",
       "      <td>2.55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-19</th>\n",
       "      <td>2.73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-18</th>\n",
       "      <td>2.86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-17</th>\n",
       "      <td>2.82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-16</th>\n",
       "      <td>2.95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-15</th>\n",
       "      <td>2.91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-14</th>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-13</th>\n",
       "      <td>2.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-12</th>\n",
       "      <td>2.73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-11</th>\n",
       "      <td>2.34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-10</th>\n",
       "      <td>2.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-09</th>\n",
       "      <td>2.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-08</th>\n",
       "      <td>2.27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-07</th>\n",
       "      <td>2.30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-06</th>\n",
       "      <td>2.51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-05</th>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-04</th>\n",
       "      <td>2.24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-03</th>\n",
       "      <td>2.30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-02</th>\n",
       "      <td>2.38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>2.47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>2.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-30</th>\n",
       "      <td>2.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-29</th>\n",
       "      <td>2.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-28</th>\n",
       "      <td>2.51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-27</th>\n",
       "      <td>2.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-26</th>\n",
       "      <td>2.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-25</th>\n",
       "      <td>2.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-24</th>\n",
       "      <td>2.63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-23</th>\n",
       "      <td>2.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-22</th>\n",
       "      <td>2.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-21</th>\n",
       "      <td>2.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-20</th>\n",
       "      <td>2.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-19</th>\n",
       "      <td>2.86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-18</th>\n",
       "      <td>2.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-17</th>\n",
       "      <td>2.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-16</th>\n",
       "      <td>2.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-15</th>\n",
       "      <td>2.78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-14</th>\n",
       "      <td>2.61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-13</th>\n",
       "      <td>2.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-12</th>\n",
       "      <td>2.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-11</th>\n",
       "      <td>2.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>2.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>2.22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>2.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>1.91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>1.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>1.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>1.82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>1.73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>1.63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>1.63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-30</th>\n",
       "      <td>1.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29</th>\n",
       "      <td>1.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-26</th>\n",
       "      <td>1.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-25</th>\n",
       "      <td>1.91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-24</th>\n",
       "      <td>1.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-22</th>\n",
       "      <td>1.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-21</th>\n",
       "      <td>2.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-20</th>\n",
       "      <td>2.12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-19</th>\n",
       "      <td>2.08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-15</th>\n",
       "      <td>2.08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-14</th>\n",
       "      <td>1.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-13</th>\n",
       "      <td>1.97</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-12</th>\n",
       "      <td>1.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-11</th>\n",
       "      <td>1.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-10</th>\n",
       "      <td>1.83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-09</th>\n",
       "      <td>1.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-08</th>\n",
       "      <td>1.97</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-07</th>\n",
       "      <td>1.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-06</th>\n",
       "      <td>1.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-05</th>\n",
       "      <td>1.61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-04</th>\n",
       "      <td>1.49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-03</th>\n",
       "      <td>1.41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-02</th>\n",
       "      <td>1.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>1.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>1.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29</th>\n",
       "      <td>1.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28</th>\n",
       "      <td>1.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27</th>\n",
       "      <td>1.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26</th>\n",
       "      <td>1.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-25</th>\n",
       "      <td>1.33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-24</th>\n",
       "      <td>1.35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-23</th>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22</th>\n",
       "      <td>1.22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21</th>\n",
       "      <td>1.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20</th>\n",
       "      <td>1.27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19</th>\n",
       "      <td>1.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18</th>\n",
       "      <td>1.63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17</th>\n",
       "      <td>1.49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-28</th>\n",
       "      <td>6.28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-13</th>\n",
       "      <td>6.28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-12</th>\n",
       "      <td>6.49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-11</th>\n",
       "      <td>6.57</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-10</th>\n",
       "      <td>6.29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-09</th>\n",
       "      <td>6.28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-11</th>\n",
       "      <td>6.28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-10</th>\n",
       "      <td>6.32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-09</th>\n",
       "      <td>6.11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-08</th>\n",
       "      <td>5.99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-07</th>\n",
       "      <td>6.03</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-06</th>\n",
       "      <td>6.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-05</th>\n",
       "      <td>6.57</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-26</th>\n",
       "      <td>6.57</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-25</th>\n",
       "      <td>6.51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-24</th>\n",
       "      <td>6.28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-23</th>\n",
       "      <td>6.28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-22</th>\n",
       "      <td>6.22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-21</th>\n",
       "      <td>5.99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-30</th>\n",
       "      <td>5.99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-29</th>\n",
       "      <td>5.98</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-28</th>\n",
       "      <td>4.82</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-27</th>\n",
       "      <td>4.29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-26</th>\n",
       "      <td>4.30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-25</th>\n",
       "      <td>4.64</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-24</th>\n",
       "      <td>4.77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-23</th>\n",
       "      <td>4.99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-22</th>\n",
       "      <td>6.31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-21</th>\n",
       "      <td>8.01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-20</th>\n",
       "      <td>8.09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-19</th>\n",
       "      <td>8.47</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-18</th>\n",
       "      <td>8.56</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-17</th>\n",
       "      <td>8.56</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-16</th>\n",
       "      <td>8.29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-15</th>\n",
       "      <td>8.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-14</th>\n",
       "      <td>8.45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-13</th>\n",
       "      <td>8.96</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-12</th>\n",
       "      <td>9.47</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-11</th>\n",
       "      <td>9.49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-10</th>\n",
       "      <td>9.49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-09</th>\n",
       "      <td>9.60</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-08</th>\n",
       "      <td>9.64</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-07</th>\n",
       "      <td>9.64</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-06</th>\n",
       "      <td>9.70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-05</th>\n",
       "      <td>9.72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-04</th>\n",
       "      <td>9.72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-03</th>\n",
       "      <td>9.94</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-02</th>\n",
       "      <td>10.03</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-29</th>\n",
       "      <td>10.03</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-28</th>\n",
       "      <td>10.04</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-27</th>\n",
       "      <td>10.11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-23</th>\n",
       "      <td>10.11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-22</th>\n",
       "      <td>10.15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-21</th>\n",
       "      <td>10.19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-10</th>\n",
       "      <td>10.19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-09</th>\n",
       "      <td>10.21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-08</th>\n",
       "      <td>10.21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-07</th>\n",
       "      <td>10.19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-03</th>\n",
       "      <td>10.19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-02</th>\n",
       "      <td>10.78</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-01</th>\n",
       "      <td>10.91</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-30</th>\n",
       "      <td>10.91</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-29</th>\n",
       "      <td>10.97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-28</th>\n",
       "      <td>10.96</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-27</th>\n",
       "      <td>10.88</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-26</th>\n",
       "      <td>10.93</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-25</th>\n",
       "      <td>11.20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-24</th>\n",
       "      <td>11.23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-23</th>\n",
       "      <td>11.23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-22</th>\n",
       "      <td>11.27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-21</th>\n",
       "      <td>11.45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-20</th>\n",
       "      <td>11.47</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-19</th>\n",
       "      <td>11.52</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-18</th>\n",
       "      <td>11.22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-17</th>\n",
       "      <td>11.22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-16</th>\n",
       "      <td>11.14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-15</th>\n",
       "      <td>11.22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-14</th>\n",
       "      <td>11.25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-13</th>\n",
       "      <td>11.26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-12</th>\n",
       "      <td>11.15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-11</th>\n",
       "      <td>10.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-10</th>\n",
       "      <td>10.34</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-09</th>\n",
       "      <td>10.40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-08</th>\n",
       "      <td>10.89</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-07</th>\n",
       "      <td>10.90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-06</th>\n",
       "      <td>10.91</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-05</th>\n",
       "      <td>10.91</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-04</th>\n",
       "      <td>10.93</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>10.93</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02</th>\n",
       "      <td>10.97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>10.94</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>10.94</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>10.96</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>10.96</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-26</th>\n",
       "      <td>10.95</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-25</th>\n",
       "      <td>10.90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-24</th>\n",
       "      <td>11.08</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-23</th>\n",
       "      <td>10.80</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-22</th>\n",
       "      <td>10.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-21</th>\n",
       "      <td>10.63</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-20</th>\n",
       "      <td>10.69</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-19</th>\n",
       "      <td>10.63</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-18</th>\n",
       "      <td>10.63</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-17</th>\n",
       "      <td>10.60</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-16</th>\n",
       "      <td>10.58</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-15</th>\n",
       "      <td>10.58</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-14</th>\n",
       "      <td>10.68</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-13</th>\n",
       "      <td>10.86</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12</th>\n",
       "      <td>11.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-11</th>\n",
       "      <td>11.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-10</th>\n",
       "      <td>11.39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-09</th>\n",
       "      <td>11.40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-08</th>\n",
       "      <td>11.40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-07</th>\n",
       "      <td>11.41</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-06</th>\n",
       "      <td>11.44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-05</th>\n",
       "      <td>11.44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-04</th>\n",
       "      <td>11.45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-03</th>\n",
       "      <td>11.40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-02</th>\n",
       "      <td>11.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-27</th>\n",
       "      <td>11.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-26</th>\n",
       "      <td>11.45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-25</th>\n",
       "      <td>11.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-24</th>\n",
       "      <td>11.29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-23</th>\n",
       "      <td>11.49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-22</th>\n",
       "      <td>11.72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-21</th>\n",
       "      <td>11.48</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-20</th>\n",
       "      <td>11.41</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-19</th>\n",
       "      <td>11.48</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-18</th>\n",
       "      <td>11.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-17</th>\n",
       "      <td>11.49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-16</th>\n",
       "      <td>11.88</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-15</th>\n",
       "      <td>12.29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-14</th>\n",
       "      <td>12.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-13</th>\n",
       "      <td>11.81</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-12</th>\n",
       "      <td>10.60</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-11</th>\n",
       "      <td>10.29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-10</th>\n",
       "      <td>9.77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-09</th>\n",
       "      <td>9.65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-08</th>\n",
       "      <td>9.14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-07</th>\n",
       "      <td>9.18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-06</th>\n",
       "      <td>8.73</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-05</th>\n",
       "      <td>8.03</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-04</th>\n",
       "      <td>7.96</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-03</th>\n",
       "      <td>7.83</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-02</th>\n",
       "      <td>7.93</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>7.82</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>7.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-30</th>\n",
       "      <td>6.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-29</th>\n",
       "      <td>6.17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-28</th>\n",
       "      <td>6.12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-27</th>\n",
       "      <td>5.79</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-26</th>\n",
       "      <td>5.75</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-25</th>\n",
       "      <td>5.49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-24</th>\n",
       "      <td>4.78</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-23</th>\n",
       "      <td>3.37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-22</th>\n",
       "      <td>2.77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-21</th>\n",
       "      <td>2.42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-20</th>\n",
       "      <td>2.37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-16</th>\n",
       "      <td>2.37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-15</th>\n",
       "      <td>2.40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-14</th>\n",
       "      <td>2.44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-13</th>\n",
       "      <td>2.45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-12</th>\n",
       "      <td>2.45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-11</th>\n",
       "      <td>2.42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-10</th>\n",
       "      <td>2.41</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-08</th>\n",
       "      <td>2.41</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07</th>\n",
       "      <td>2.43</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-06</th>\n",
       "      <td>2.44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-03</th>\n",
       "      <td>2.44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-02</th>\n",
       "      <td>2.60</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>2.79</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-26</th>\n",
       "      <td>2.79</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-25</th>\n",
       "      <td>2.99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-24</th>\n",
       "      <td>3.09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-23</th>\n",
       "      <td>3.09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-22</th>\n",
       "      <td>3.02</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-21</th>\n",
       "      <td>2.96</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-14</th>\n",
       "      <td>2.96</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-13</th>\n",
       "      <td>2.97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-12</th>\n",
       "      <td>2.96</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-11</th>\n",
       "      <td>2.96</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-10</th>\n",
       "      <td>2.97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-09</th>\n",
       "      <td>3.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-08</th>\n",
       "      <td>2.99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-06</th>\n",
       "      <td>2.99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-05</th>\n",
       "      <td>2.94</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-04</th>\n",
       "      <td>2.95</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-03</th>\n",
       "      <td>2.95</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-02</th>\n",
       "      <td>3.04</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-01</th>\n",
       "      <td>3.12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-31</th>\n",
       "      <td>3.12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-30</th>\n",
       "      <td>3.01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-29</th>\n",
       "      <td>2.97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-28</th>\n",
       "      <td>2.96</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-27</th>\n",
       "      <td>2.92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-26</th>\n",
       "      <td>2.92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-25</th>\n",
       "      <td>2.91</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-23</th>\n",
       "      <td>2.91</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-22</th>\n",
       "      <td>2.89</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-21</th>\n",
       "      <td>2.88</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-19</th>\n",
       "      <td>2.88</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-18</th>\n",
       "      <td>2.90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-16</th>\n",
       "      <td>2.90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-15</th>\n",
       "      <td>2.93</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-11</th>\n",
       "      <td>2.93</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-10</th>\n",
       "      <td>2.89</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-09</th>\n",
       "      <td>2.87</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-08</th>\n",
       "      <td>2.87</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-07</th>\n",
       "      <td>2.88</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-06</th>\n",
       "      <td>2.98</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-05</th>\n",
       "      <td>3.08</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-04</th>\n",
       "      <td>3.09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-03</th>\n",
       "      <td>3.19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-01</th>\n",
       "      <td>3.19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>3.20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-30</th>\n",
       "      <td>3.20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-29</th>\n",
       "      <td>3.21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-28</th>\n",
       "      <td>3.21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-27</th>\n",
       "      <td>3.46</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-26</th>\n",
       "      <td>3.54</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-25</th>\n",
       "      <td>3.49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-21</th>\n",
       "      <td>3.49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-20</th>\n",
       "      <td>3.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-17</th>\n",
       "      <td>3.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-16</th>\n",
       "      <td>3.97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-15</th>\n",
       "      <td>4.22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-14</th>\n",
       "      <td>4.19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-12</th>\n",
       "      <td>4.19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-11</th>\n",
       "      <td>4.21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-10</th>\n",
       "      <td>4.19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-09</th>\n",
       "      <td>4.18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-08</th>\n",
       "      <td>4.17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-07</th>\n",
       "      <td>4.14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-06</th>\n",
       "      <td>4.07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-05</th>\n",
       "      <td>4.08</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-04</th>\n",
       "      <td>4.03</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-03</th>\n",
       "      <td>4.03</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-02</th>\n",
       "      <td>3.99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01</th>\n",
       "      <td>3.99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>3.56</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-29</th>\n",
       "      <td>3.30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-28</th>\n",
       "      <td>3.30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-27</th>\n",
       "      <td>3.37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-26</th>\n",
       "      <td>3.52</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-25</th>\n",
       "      <td>3.55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-24</th>\n",
       "      <td>3.54</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-23</th>\n",
       "      <td>3.49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-22</th>\n",
       "      <td>3.33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-21</th>\n",
       "      <td>3.29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-20</th>\n",
       "      <td>3.23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-19</th>\n",
       "      <td>3.07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-18</th>\n",
       "      <td>3.07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-17</th>\n",
       "      <td>3.03</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-16</th>\n",
       "      <td>2.82</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-15</th>\n",
       "      <td>2.82</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-14</th>\n",
       "      <td>2.73</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-13</th>\n",
       "      <td>2.67</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-12</th>\n",
       "      <td>2.62</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-11</th>\n",
       "      <td>2.75</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-10</th>\n",
       "      <td>2.79</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-09</th>\n",
       "      <td>2.78</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-08</th>\n",
       "      <td>2.73</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-07</th>\n",
       "      <td>2.72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-06</th>\n",
       "      <td>2.71</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-05</th>\n",
       "      <td>2.68</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-04</th>\n",
       "      <td>2.69</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-03</th>\n",
       "      <td>2.70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-02</th>\n",
       "      <td>2.69</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-01</th>\n",
       "      <td>2.71</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-30</th>\n",
       "      <td>2.71</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-29</th>\n",
       "      <td>2.72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-28</th>\n",
       "      <td>2.72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-27</th>\n",
       "      <td>2.67</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-26</th>\n",
       "      <td>2.62</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-25</th>\n",
       "      <td>2.61</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-24</th>\n",
       "      <td>2.59</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-23</th>\n",
       "      <td>2.55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-19</th>\n",
       "      <td>2.55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-18</th>\n",
       "      <td>2.56</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-16</th>\n",
       "      <td>2.56</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-15</th>\n",
       "      <td>2.55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-14</th>\n",
       "      <td>2.54</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-13</th>\n",
       "      <td>2.51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-12</th>\n",
       "      <td>2.41</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-11</th>\n",
       "      <td>2.28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-10</th>\n",
       "      <td>2.20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-07</th>\n",
       "      <td>2.20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-06</th>\n",
       "      <td>2.19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-05</th>\n",
       "      <td>2.19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-04</th>\n",
       "      <td>2.17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-03</th>\n",
       "      <td>2.19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-02</th>\n",
       "      <td>2.43</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01</th>\n",
       "      <td>2.42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>2.44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-29</th>\n",
       "      <td>2.40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-28</th>\n",
       "      <td>2.05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-27</th>\n",
       "      <td>1.83</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-26</th>\n",
       "      <td>1.80</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-25</th>\n",
       "      <td>1.76</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-24</th>\n",
       "      <td>1.76</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-23</th>\n",
       "      <td>1.77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-22</th>\n",
       "      <td>1.77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-21</th>\n",
       "      <td>1.74</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-20</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-19</th>\n",
       "      <td>1.74</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-15</th>\n",
       "      <td>1.74</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-14</th>\n",
       "      <td>1.72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-13</th>\n",
       "      <td>1.64</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-12</th>\n",
       "      <td>1.55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-11</th>\n",
       "      <td>1.55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-10</th>\n",
       "      <td>1.59</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-09</th>\n",
       "      <td>1.76</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-08</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-07</th>\n",
       "      <td>1.56</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-06</th>\n",
       "      <td>1.51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-05</th>\n",
       "      <td>1.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-04</th>\n",
       "      <td>1.49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-03</th>\n",
       "      <td>1.48</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-02</th>\n",
       "      <td>1.47</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-01</th>\n",
       "      <td>1.46</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>1.43</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30</th>\n",
       "      <td>1.42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-29</th>\n",
       "      <td>1.43</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-28</th>\n",
       "      <td>1.39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-27</th>\n",
       "      <td>1.40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-26</th>\n",
       "      <td>1.34</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-25</th>\n",
       "      <td>1.32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-24</th>\n",
       "      <td>1.29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-23</th>\n",
       "      <td>1.28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-22</th>\n",
       "      <td>1.27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-21</th>\n",
       "      <td>1.25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-20</th>\n",
       "      <td>1.25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-19</th>\n",
       "      <td>1.21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-18</th>\n",
       "      <td>1.12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-17</th>\n",
       "      <td>1.11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16</th>\n",
       "      <td>1.05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-15</th>\n",
       "      <td>1.04</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-14</th>\n",
       "      <td>1.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-13</th>\n",
       "      <td>0.93</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-12</th>\n",
       "      <td>0.92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-11</th>\n",
       "      <td>0.93</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-10</th>\n",
       "      <td>0.91</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-09</th>\n",
       "      <td>0.90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-08</th>\n",
       "      <td>0.85</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-07</th>\n",
       "      <td>0.96</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06</th>\n",
       "      <td>0.92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-05</th>\n",
       "      <td>0.83</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-04</th>\n",
       "      <td>0.83</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-03</th>\n",
       "      <td>0.81</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-02</th>\n",
       "      <td>0.72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-01</th>\n",
       "      <td>0.74</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>0.81</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-27</th>\n",
       "      <td>0.73</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-26</th>\n",
       "      <td>0.72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-25</th>\n",
       "      <td>0.71</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-24</th>\n",
       "      <td>0.68</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-23</th>\n",
       "      <td>0.62</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-22</th>\n",
       "      <td>0.55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-21</th>\n",
       "      <td>0.41</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-20</th>\n",
       "      <td>0.43</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-16</th>\n",
       "      <td>0.43</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-15</th>\n",
       "      <td>0.67</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-14</th>\n",
       "      <td>0.85</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-13</th>\n",
       "      <td>0.93</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-12</th>\n",
       "      <td>0.88</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-11</th>\n",
       "      <td>0.86</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-10</th>\n",
       "      <td>0.85</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-09</th>\n",
       "      <td>0.86</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-08</th>\n",
       "      <td>0.82</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-07</th>\n",
       "      <td>0.80</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-03</th>\n",
       "      <td>0.80</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-02</th>\n",
       "      <td>0.83</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01</th>\n",
       "      <td>0.90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>0.91</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-27</th>\n",
       "      <td>0.91</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-26</th>\n",
       "      <td>1.11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-25</th>\n",
       "      <td>1.18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-24</th>\n",
       "      <td>0.99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-23</th>\n",
       "      <td>1.30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22</th>\n",
       "      <td>1.44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-21</th>\n",
       "      <td>1.46</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-20</th>\n",
       "      <td>1.51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-19</th>\n",
       "      <td>1.51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-18</th>\n",
       "      <td>1.52</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-17</th>\n",
       "      <td>1.55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-16</th>\n",
       "      <td>1.55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-15</th>\n",
       "      <td>1.56</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-14</th>\n",
       "      <td>1.54</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-13</th>\n",
       "      <td>1.41</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12</th>\n",
       "      <td>1.39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-11</th>\n",
       "      <td>1.29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-10</th>\n",
       "      <td>1.29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-09</th>\n",
       "      <td>1.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-08</th>\n",
       "      <td>1.29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-07</th>\n",
       "      <td>1.20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-06</th>\n",
       "      <td>1.26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>1.26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <td>1.31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>1.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>1.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-30</th>\n",
       "      <td>1.40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-29</th>\n",
       "      <td>1.48</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-27</th>\n",
       "      <td>1.48</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-26</th>\n",
       "      <td>1.49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-24</th>\n",
       "      <td>1.49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-23</th>\n",
       "      <td>1.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-22</th>\n",
       "      <td>1.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-21</th>\n",
       "      <td>2.03</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-20</th>\n",
       "      <td>2.29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-19</th>\n",
       "      <td>2.29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-18</th>\n",
       "      <td>2.28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-17</th>\n",
       "      <td>2.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-16</th>\n",
       "      <td>2.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-15</th>\n",
       "      <td>2.41</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-14</th>\n",
       "      <td>2.52</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-13</th>\n",
       "      <td>2.52</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-12</th>\n",
       "      <td>2.53</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-11</th>\n",
       "      <td>2.62</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-08</th>\n",
       "      <td>2.62</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-07</th>\n",
       "      <td>2.64</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-06</th>\n",
       "      <td>2.65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-04</th>\n",
       "      <td>2.65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-03</th>\n",
       "      <td>2.61</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-02</th>\n",
       "      <td>2.51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-01</th>\n",
       "      <td>2.51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>2.52</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-25</th>\n",
       "      <td>2.52</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-24</th>\n",
       "      <td>2.64</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-23</th>\n",
       "      <td>2.66</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-22</th>\n",
       "      <td>2.64</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-21</th>\n",
       "      <td>2.62</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-16</th>\n",
       "      <td>2.62</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-15</th>\n",
       "      <td>2.69</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-14</th>\n",
       "      <td>2.76</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-13</th>\n",
       "      <td>2.78</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-12</th>\n",
       "      <td>2.80</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-10</th>\n",
       "      <td>2.80</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-09</th>\n",
       "      <td>2.77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-08</th>\n",
       "      <td>2.86</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-07</th>\n",
       "      <td>2.89</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-06</th>\n",
       "      <td>2.72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-02</th>\n",
       "      <td>2.72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-01</th>\n",
       "      <td>2.77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>2.80</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-30</th>\n",
       "      <td>2.80</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-29</th>\n",
       "      <td>2.89</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-28</th>\n",
       "      <td>2.93</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-27</th>\n",
       "      <td>3.10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-26</th>\n",
       "      <td>3.11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-25</th>\n",
       "      <td>3.11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-24</th>\n",
       "      <td>3.30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-23</th>\n",
       "      <td>3.31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-22</th>\n",
       "      <td>3.31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-21</th>\n",
       "      <td>3.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-19</th>\n",
       "      <td>3.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-18</th>\n",
       "      <td>3.32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-17</th>\n",
       "      <td>3.31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-13</th>\n",
       "      <td>3.31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-12</th>\n",
       "      <td>3.32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-11</th>\n",
       "      <td>3.34</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-08</th>\n",
       "      <td>3.34</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-07</th>\n",
       "      <td>3.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-06</th>\n",
       "      <td>3.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-05</th>\n",
       "      <td>3.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-04</th>\n",
       "      <td>3.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-02</th>\n",
       "      <td>3.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-01</th>\n",
       "      <td>3.48</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-28</th>\n",
       "      <td>3.48</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-27</th>\n",
       "      <td>3.49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-26</th>\n",
       "      <td>3.52</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-25</th>\n",
       "      <td>3.53</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-24</th>\n",
       "      <td>3.55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-23</th>\n",
       "      <td>3.55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-22</th>\n",
       "      <td>3.36</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-21</th>\n",
       "      <td>3.34</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-14</th>\n",
       "      <td>3.34</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-13</th>\n",
       "      <td>3.33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-12</th>\n",
       "      <td>3.33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11</th>\n",
       "      <td>3.30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-10</th>\n",
       "      <td>3.29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-09</th>\n",
       "      <td>3.37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-08</th>\n",
       "      <td>3.43</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-07</th>\n",
       "      <td>3.40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-03</th>\n",
       "      <td>3.40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-02</th>\n",
       "      <td>3.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-01</th>\n",
       "      <td>3.37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>3.37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-30</th>\n",
       "      <td>3.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-29</th>\n",
       "      <td>3.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>7.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-29</th>\n",
       "      <td>7.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-28</th>\n",
       "      <td>8.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-27</th>\n",
       "      <td>7.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-26</th>\n",
       "      <td>7.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-25</th>\n",
       "      <td>7.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-24</th>\n",
       "      <td>7.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-23</th>\n",
       "      <td>7.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-22</th>\n",
       "      <td>7.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-21</th>\n",
       "      <td>6.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-20</th>\n",
       "      <td>6.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-19</th>\n",
       "      <td>6.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-18</th>\n",
       "      <td>7.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-17</th>\n",
       "      <td>8.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-16</th>\n",
       "      <td>9.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-15</th>\n",
       "      <td>11.14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-14</th>\n",
       "      <td>12.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-13</th>\n",
       "      <td>11.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-12</th>\n",
       "      <td>11.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-11</th>\n",
       "      <td>9.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-10</th>\n",
       "      <td>9.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-09</th>\n",
       "      <td>9.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-08</th>\n",
       "      <td>8.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-07</th>\n",
       "      <td>6.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-06</th>\n",
       "      <td>7.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-05</th>\n",
       "      <td>5.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-04</th>\n",
       "      <td>5.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-03</th>\n",
       "      <td>6.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-02</th>\n",
       "      <td>7.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>7.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>7.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-30</th>\n",
       "      <td>6.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-29</th>\n",
       "      <td>6.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-28</th>\n",
       "      <td>5.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-27</th>\n",
       "      <td>5.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-26</th>\n",
       "      <td>4.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-25</th>\n",
       "      <td>3.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-24</th>\n",
       "      <td>4.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-23</th>\n",
       "      <td>3.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-22</th>\n",
       "      <td>2.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-21</th>\n",
       "      <td>1.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-20</th>\n",
       "      <td>1.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-19</th>\n",
       "      <td>1.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-18</th>\n",
       "      <td>1.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-17</th>\n",
       "      <td>1.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-16</th>\n",
       "      <td>1.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-15</th>\n",
       "      <td>1.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-14</th>\n",
       "      <td>1.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-13</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-12</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-11</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-10</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-09</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-08</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-06</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-05</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-04</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-03</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-02</th>\n",
       "      <td>1.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>1.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-29</th>\n",
       "      <td>1.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-28</th>\n",
       "      <td>1.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-27</th>\n",
       "      <td>1.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-26</th>\n",
       "      <td>1.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-25</th>\n",
       "      <td>1.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-24</th>\n",
       "      <td>1.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-23</th>\n",
       "      <td>1.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-22</th>\n",
       "      <td>1.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-21</th>\n",
       "      <td>1.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-20</th>\n",
       "      <td>1.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-19</th>\n",
       "      <td>1.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-18</th>\n",
       "      <td>1.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-17</th>\n",
       "      <td>1.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-16</th>\n",
       "      <td>1.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-15</th>\n",
       "      <td>1.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-14</th>\n",
       "      <td>1.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-13</th>\n",
       "      <td>1.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-12</th>\n",
       "      <td>1.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-11</th>\n",
       "      <td>1.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-10</th>\n",
       "      <td>1.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-09</th>\n",
       "      <td>1.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-08</th>\n",
       "      <td>1.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-07</th>\n",
       "      <td>1.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-06</th>\n",
       "      <td>1.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-13</th>\n",
       "      <td>4.05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-12</th>\n",
       "      <td>4.02</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-11</th>\n",
       "      <td>3.99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-10</th>\n",
       "      <td>3.98</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-09</th>\n",
       "      <td>3.87</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-08</th>\n",
       "      <td>3.84</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-07</th>\n",
       "      <td>3.84</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-06</th>\n",
       "      <td>3.73</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-05</th>\n",
       "      <td>3.67</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-04</th>\n",
       "      <td>3.37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-03</th>\n",
       "      <td>3.33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-02</th>\n",
       "      <td>3.29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01</th>\n",
       "      <td>3.27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>3.16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-30</th>\n",
       "      <td>3.11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-29</th>\n",
       "      <td>2.97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28</th>\n",
       "      <td>2.91</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-27</th>\n",
       "      <td>2.92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-26</th>\n",
       "      <td>2.95</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-25</th>\n",
       "      <td>2.95</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-24</th>\n",
       "      <td>2.94</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-23</th>\n",
       "      <td>2.92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-22</th>\n",
       "      <td>2.89</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-21</th>\n",
       "      <td>2.91</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-20</th>\n",
       "      <td>2.90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-19</th>\n",
       "      <td>2.92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-18</th>\n",
       "      <td>2.89</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-17</th>\n",
       "      <td>2.88</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-16</th>\n",
       "      <td>2.86</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-14</th>\n",
       "      <td>2.86</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-13</th>\n",
       "      <td>2.97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-12</th>\n",
       "      <td>2.99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-11</th>\n",
       "      <td>3.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-10</th>\n",
       "      <td>2.99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-09</th>\n",
       "      <td>2.97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-05</th>\n",
       "      <td>2.97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-04</th>\n",
       "      <td>2.99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-03</th>\n",
       "      <td>3.01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-02</th>\n",
       "      <td>3.12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01</th>\n",
       "      <td>3.20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>3.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-26</th>\n",
       "      <td>3.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-25</th>\n",
       "      <td>3.39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-21</th>\n",
       "      <td>3.39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-20</th>\n",
       "      <td>3.40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-19</th>\n",
       "      <td>3.41</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-18</th>\n",
       "      <td>3.37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-17</th>\n",
       "      <td>3.30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-16</th>\n",
       "      <td>2.97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-15</th>\n",
       "      <td>2.76</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-13</th>\n",
       "      <td>2.76</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-12</th>\n",
       "      <td>2.73</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-11</th>\n",
       "      <td>2.72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-06</th>\n",
       "      <td>2.72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-05</th>\n",
       "      <td>2.92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-04</th>\n",
       "      <td>2.94</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-03</th>\n",
       "      <td>2.96</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>2.96</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>3.02</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-30</th>\n",
       "      <td>3.03</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-29</th>\n",
       "      <td>3.03</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-28</th>\n",
       "      <td>3.17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-27</th>\n",
       "      <td>3.21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-26</th>\n",
       "      <td>3.21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-25</th>\n",
       "      <td>3.18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-24</th>\n",
       "      <td>3.37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-23</th>\n",
       "      <td>3.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-22</th>\n",
       "      <td>3.83</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-21</th>\n",
       "      <td>3.90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-20</th>\n",
       "      <td>3.91</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-19</th>\n",
       "      <td>4.07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-18</th>\n",
       "      <td>4.14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-17</th>\n",
       "      <td>4.14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-16</th>\n",
       "      <td>4.29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-15</th>\n",
       "      <td>4.33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-14</th>\n",
       "      <td>4.32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-13</th>\n",
       "      <td>4.32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-12</th>\n",
       "      <td>4.45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-11</th>\n",
       "      <td>4.53</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>4.68</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>4.69</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>4.69</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>4.67</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>4.66</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29</th>\n",
       "      <td>4.66</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-28</th>\n",
       "      <td>4.65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-24</th>\n",
       "      <td>4.65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-23</th>\n",
       "      <td>4.62</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-22</th>\n",
       "      <td>4.61</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-21</th>\n",
       "      <td>4.67</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-20</th>\n",
       "      <td>4.83</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-17</th>\n",
       "      <td>4.83</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16</th>\n",
       "      <td>4.85</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-15</th>\n",
       "      <td>4.87</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-14</th>\n",
       "      <td>4.90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-12</th>\n",
       "      <td>4.90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-11</th>\n",
       "      <td>4.70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-10</th>\n",
       "      <td>4.66</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-07</th>\n",
       "      <td>4.66</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-06</th>\n",
       "      <td>4.44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-05</th>\n",
       "      <td>4.44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-04</th>\n",
       "      <td>4.45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-02</th>\n",
       "      <td>4.45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>4.46</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29</th>\n",
       "      <td>4.46</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28</th>\n",
       "      <td>4.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27</th>\n",
       "      <td>4.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26</th>\n",
       "      <td>4.51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-25</th>\n",
       "      <td>4.49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-24</th>\n",
       "      <td>4.48</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-23</th>\n",
       "      <td>4.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18</th>\n",
       "      <td>4.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17</th>\n",
       "      <td>4.47</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15</th>\n",
       "      <td>4.47</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14</th>\n",
       "      <td>4.48</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-13</th>\n",
       "      <td>4.49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-12</th>\n",
       "      <td>4.49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-11</th>\n",
       "      <td>4.48</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-08</th>\n",
       "      <td>4.48</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07</th>\n",
       "      <td>4.47</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06</th>\n",
       "      <td>4.44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05</th>\n",
       "      <td>4.46</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04</th>\n",
       "      <td>4.47</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03</th>\n",
       "      <td>4.47</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02</th>\n",
       "      <td>4.46</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01</th>\n",
       "      <td>4.43</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-30</th>\n",
       "      <td>4.43</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-29</th>\n",
       "      <td>4.42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-28</th>\n",
       "      <td>4.53</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-27</th>\n",
       "      <td>4.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-26</th>\n",
       "      <td>4.32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-22</th>\n",
       "      <td>4.32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-21</th>\n",
       "      <td>4.28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-10</th>\n",
       "      <td>4.28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-09</th>\n",
       "      <td>4.32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-08</th>\n",
       "      <td>4.30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-07</th>\n",
       "      <td>4.28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-06</th>\n",
       "      <td>4.30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-04</th>\n",
       "      <td>4.30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-03</th>\n",
       "      <td>4.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-02</th>\n",
       "      <td>4.66</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-01</th>\n",
       "      <td>4.69</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>4.69</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-29</th>\n",
       "      <td>4.70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-28</th>\n",
       "      <td>4.75</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-27</th>\n",
       "      <td>4.78</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26</th>\n",
       "      <td>4.72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-25</th>\n",
       "      <td>4.72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-24</th>\n",
       "      <td>4.60</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-23</th>\n",
       "      <td>4.54</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-22</th>\n",
       "      <td>4.34</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-21</th>\n",
       "      <td>4.33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-20</th>\n",
       "      <td>4.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-19</th>\n",
       "      <td>4.32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-18</th>\n",
       "      <td>4.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-17</th>\n",
       "      <td>4.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-16</th>\n",
       "      <td>4.36</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-14</th>\n",
       "      <td>4.36</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-13</th>\n",
       "      <td>4.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-12</th>\n",
       "      <td>4.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-11</th>\n",
       "      <td>4.34</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-10</th>\n",
       "      <td>4.34</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-09</th>\n",
       "      <td>4.19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-08</th>\n",
       "      <td>4.23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-07</th>\n",
       "      <td>4.27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-06</th>\n",
       "      <td>4.26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-03</th>\n",
       "      <td>4.26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-02</th>\n",
       "      <td>4.25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-30</th>\n",
       "      <td>4.25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-29</th>\n",
       "      <td>4.26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-28</th>\n",
       "      <td>4.25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-26</th>\n",
       "      <td>4.25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-25</th>\n",
       "      <td>4.24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-22</th>\n",
       "      <td>4.24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-21</th>\n",
       "      <td>4.23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-20</th>\n",
       "      <td>4.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-18</th>\n",
       "      <td>4.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-17</th>\n",
       "      <td>4.39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-15</th>\n",
       "      <td>4.39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-14</th>\n",
       "      <td>4.37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-13</th>\n",
       "      <td>4.53</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-11</th>\n",
       "      <td>4.53</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-10</th>\n",
       "      <td>4.51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-09</th>\n",
       "      <td>4.46</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-08</th>\n",
       "      <td>4.45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-07</th>\n",
       "      <td>4.45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-06</th>\n",
       "      <td>4.44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-05</th>\n",
       "      <td>4.43</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-04</th>\n",
       "      <td>4.57</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-03</th>\n",
       "      <td>4.41</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-02</th>\n",
       "      <td>4.37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-01</th>\n",
       "      <td>4.37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>4.25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-30</th>\n",
       "      <td>4.24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-29</th>\n",
       "      <td>4.22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-28</th>\n",
       "      <td>4.10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-27</th>\n",
       "      <td>4.04</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-26</th>\n",
       "      <td>4.06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-25</th>\n",
       "      <td>4.24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-24</th>\n",
       "      <td>4.18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-23</th>\n",
       "      <td>4.08</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-22</th>\n",
       "      <td>3.83</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-21</th>\n",
       "      <td>3.62</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-20</th>\n",
       "      <td>3.72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-19</th>\n",
       "      <td>3.70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-18</th>\n",
       "      <td>3.74</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-17</th>\n",
       "      <td>3.71</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-16</th>\n",
       "      <td>3.73</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-15</th>\n",
       "      <td>3.53</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-14</th>\n",
       "      <td>3.52</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-13</th>\n",
       "      <td>3.51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-12</th>\n",
       "      <td>3.51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-11</th>\n",
       "      <td>3.52</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-10</th>\n",
       "      <td>3.55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-09</th>\n",
       "      <td>3.49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-08</th>\n",
       "      <td>3.56</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-07</th>\n",
       "      <td>3.61</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-06</th>\n",
       "      <td>3.77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-05</th>\n",
       "      <td>3.70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-04</th>\n",
       "      <td>3.41</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-03</th>\n",
       "      <td>3.28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-02</th>\n",
       "      <td>3.26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-01</th>\n",
       "      <td>3.26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>3.10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-29</th>\n",
       "      <td>3.05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-28</th>\n",
       "      <td>5.28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-27</th>\n",
       "      <td>5.40</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-26</th>\n",
       "      <td>5.87</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-03</th>\n",
       "      <td>5.87</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-02</th>\n",
       "      <td>5.51</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-01</th>\n",
       "      <td>5.21</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-23</th>\n",
       "      <td>5.21</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-22</th>\n",
       "      <td>5.36</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-21</th>\n",
       "      <td>5.73</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-20</th>\n",
       "      <td>5.73</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-19</th>\n",
       "      <td>5.37</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-18</th>\n",
       "      <td>5.15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-17</th>\n",
       "      <td>5.53</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-15</th>\n",
       "      <td>5.53</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-14</th>\n",
       "      <td>6.17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-13</th>\n",
       "      <td>6.26</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-12</th>\n",
       "      <td>6.84</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-11</th>\n",
       "      <td>6.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-10</th>\n",
       "      <td>6.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-09</th>\n",
       "      <td>6.35</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-08</th>\n",
       "      <td>6.26</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-12</th>\n",
       "      <td>6.26</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-11</th>\n",
       "      <td>6.78</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-10</th>\n",
       "      <td>6.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-30</th>\n",
       "      <td>6.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-29</th>\n",
       "      <td>6.97</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-28</th>\n",
       "      <td>6.72</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-26</th>\n",
       "      <td>6.72</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-25</th>\n",
       "      <td>6.69</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-24</th>\n",
       "      <td>5.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-14</th>\n",
       "      <td>5.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-13</th>\n",
       "      <td>5.69</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-12</th>\n",
       "      <td>5.39</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-10</th>\n",
       "      <td>5.39</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-09</th>\n",
       "      <td>6.67</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-08</th>\n",
       "      <td>8.05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-11</th>\n",
       "      <td>8.05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-10</th>\n",
       "      <td>8.13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-09</th>\n",
       "      <td>8.88</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-08</th>\n",
       "      <td>9.04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-28</th>\n",
       "      <td>9.04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-27</th>\n",
       "      <td>8.83</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-26</th>\n",
       "      <td>8.05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-22</th>\n",
       "      <td>8.05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-21</th>\n",
       "      <td>8.13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-20</th>\n",
       "      <td>9.04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-18</th>\n",
       "      <td>9.04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-17</th>\n",
       "      <td>9.50</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-16</th>\n",
       "      <td>10.04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-13</th>\n",
       "      <td>10.04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-12</th>\n",
       "      <td>9.87</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-11</th>\n",
       "      <td>8.05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-05</th>\n",
       "      <td>8.05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-04</th>\n",
       "      <td>7.81</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>7.32</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>7.32</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>7.75</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>9.04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-29</th>\n",
       "      <td>9.04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-28</th>\n",
       "      <td>9.92</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>10.04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-25</th>\n",
       "      <td>10.04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-24</th>\n",
       "      <td>10.46</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-23</th>\n",
       "      <td>11.04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-22</th>\n",
       "      <td>9.68</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-21</th>\n",
       "      <td>8.55</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-20</th>\n",
       "      <td>7.77</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-18</th>\n",
       "      <td>7.77</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-17</th>\n",
       "      <td>7.56</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-16</th>\n",
       "      <td>7.04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-15</th>\n",
       "      <td>7.04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-14</th>\n",
       "      <td>7.25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-13</th>\n",
       "      <td>7.77</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-08</th>\n",
       "      <td>7.77</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-07</th>\n",
       "      <td>8.32</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-06</th>\n",
       "      <td>8.50</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-04</th>\n",
       "      <td>8.50</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-03</th>\n",
       "      <td>8.54</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-02</th>\n",
       "      <td>9.49</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>9.49</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>9.21</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-29</th>\n",
       "      <td>7.79</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-25</th>\n",
       "      <td>7.79</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-24</th>\n",
       "      <td>8.52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-23</th>\n",
       "      <td>8.44</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-22</th>\n",
       "      <td>7.96</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-21</th>\n",
       "      <td>6.47</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-20</th>\n",
       "      <td>6.22</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-19</th>\n",
       "      <td>6.80</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-18</th>\n",
       "      <td>6.80</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-17</th>\n",
       "      <td>7.32</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-16</th>\n",
       "      <td>7.53</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-11</th>\n",
       "      <td>7.53</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-10</th>\n",
       "      <td>7.44</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-09</th>\n",
       "      <td>6.74</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-08</th>\n",
       "      <td>6.74</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-07</th>\n",
       "      <td>6.72</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-06</th>\n",
       "      <td>6.14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-05</th>\n",
       "      <td>6.14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-04</th>\n",
       "      <td>6.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-03</th>\n",
       "      <td>5.03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-02</th>\n",
       "      <td>4.57</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>4.57</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>4.79</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-30</th>\n",
       "      <td>4.10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-29</th>\n",
       "      <td>4.35</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-28</th>\n",
       "      <td>4.12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-27</th>\n",
       "      <td>3.80</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-26</th>\n",
       "      <td>3.86</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-25</th>\n",
       "      <td>3.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-24</th>\n",
       "      <td>3.44</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-23</th>\n",
       "      <td>2.48</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-22</th>\n",
       "      <td>2.25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-21</th>\n",
       "      <td>1.83</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-20</th>\n",
       "      <td>1.77</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-19</th>\n",
       "      <td>1.77</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-18</th>\n",
       "      <td>1.62</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-17</th>\n",
       "      <td>1.55</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-08</th>\n",
       "      <td>1.55</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-07</th>\n",
       "      <td>1.77</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-03</th>\n",
       "      <td>1.77</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-02</th>\n",
       "      <td>1.83</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-01</th>\n",
       "      <td>1.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-24</th>\n",
       "      <td>1.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-23</th>\n",
       "      <td>2.05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-22</th>\n",
       "      <td>2.28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-21</th>\n",
       "      <td>2.28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-20</th>\n",
       "      <td>2.20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-19</th>\n",
       "      <td>1.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01</th>\n",
       "      <td>1.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>1.85</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-29</th>\n",
       "      <td>1.77</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-27</th>\n",
       "      <td>1.77</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-26</th>\n",
       "      <td>1.82</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-25</th>\n",
       "      <td>1.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-23</th>\n",
       "      <td>1.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-22</th>\n",
       "      <td>1.62</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-21</th>\n",
       "      <td>1.55</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-09</th>\n",
       "      <td>1.55</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-08</th>\n",
       "      <td>1.64</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-07</th>\n",
       "      <td>1.77</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-06</th>\n",
       "      <td>1.65</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-05</th>\n",
       "      <td>1.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>1.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-30</th>\n",
       "      <td>1.15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-29</th>\n",
       "      <td>1.11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-26</th>\n",
       "      <td>1.11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-25</th>\n",
       "      <td>1.23</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-24</th>\n",
       "      <td>1.30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-29</th>\n",
       "      <td>1.30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-28</th>\n",
       "      <td>1.48</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-27</th>\n",
       "      <td>1.52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-24</th>\n",
       "      <td>1.52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-23</th>\n",
       "      <td>1.57</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-22</th>\n",
       "      <td>1.74</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-20</th>\n",
       "      <td>1.74</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-19</th>\n",
       "      <td>1.52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-14</th>\n",
       "      <td>1.52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-13</th>\n",
       "      <td>1.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-08</th>\n",
       "      <td>1.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-07</th>\n",
       "      <td>1.14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-05</th>\n",
       "      <td>1.14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-04</th>\n",
       "      <td>0.98</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30</th>\n",
       "      <td>0.98</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-29</th>\n",
       "      <td>0.84</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-24</th>\n",
       "      <td>0.84</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-23</th>\n",
       "      <td>0.72</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-20</th>\n",
       "      <td>0.72</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-19</th>\n",
       "      <td>0.63</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-18</th>\n",
       "      <td>0.60</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-17</th>\n",
       "      <td>0.62</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16</th>\n",
       "      <td>0.72</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-12</th>\n",
       "      <td>0.72</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-11</th>\n",
       "      <td>0.71</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-10</th>\n",
       "      <td>0.60</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-09</th>\n",
       "      <td>0.46</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-08</th>\n",
       "      <td>0.40</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-03</th>\n",
       "      <td>0.40</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-02</th>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-01</th>\n",
       "      <td>0.31</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>0.21</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-27</th>\n",
       "      <td>0.23</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-26</th>\n",
       "      <td>0.23</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-25</th>\n",
       "      <td>0.25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-24</th>\n",
       "      <td>0.36</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-13</th>\n",
       "      <td>0.36</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-12</th>\n",
       "      <td>0.45</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-10</th>\n",
       "      <td>0.45</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-09</th>\n",
       "      <td>0.54</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01</th>\n",
       "      <td>0.54</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>0.62</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30</th>\n",
       "      <td>0.63</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22</th>\n",
       "      <td>0.63</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-21</th>\n",
       "      <td>0.57</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-20</th>\n",
       "      <td>0.51</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-11</th>\n",
       "      <td>0.51</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-10</th>\n",
       "      <td>0.60</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>0.60</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-30</th>\n",
       "      <td>0.51</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-29</th>\n",
       "      <td>0.49</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-25</th>\n",
       "      <td>0.49</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-24</th>\n",
       "      <td>0.44</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-23</th>\n",
       "      <td>0.49</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-16</th>\n",
       "      <td>0.49</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-15</th>\n",
       "      <td>0.57</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-14</th>\n",
       "      <td>0.58</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-13</th>\n",
       "      <td>0.68</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-12</th>\n",
       "      <td>0.70</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-08</th>\n",
       "      <td>0.70</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-07</th>\n",
       "      <td>0.78</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-06</th>\n",
       "      <td>0.82</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-05</th>\n",
       "      <td>0.89</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-04</th>\n",
       "      <td>1.01</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-16</th>\n",
       "      <td>1.01</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-15</th>\n",
       "      <td>1.19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-14</th>\n",
       "      <td>1.20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-09</th>\n",
       "      <td>1.20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-08</th>\n",
       "      <td>1.31</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-07</th>\n",
       "      <td>1.39</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-03</th>\n",
       "      <td>1.39</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-02</th>\n",
       "      <td>1.48</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-01</th>\n",
       "      <td>1.64</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>1.73</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-23</th>\n",
       "      <td>1.73</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-22</th>\n",
       "      <td>1.82</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-21</th>\n",
       "      <td>1.95</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-14</th>\n",
       "      <td>1.95</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-13</th>\n",
       "      <td>2.12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-12</th>\n",
       "      <td>2.24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11</th>\n",
       "      <td>2.24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-10</th>\n",
       "      <td>2.18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-09</th>\n",
       "      <td>1.95</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-08</th>\n",
       "      <td>1.95</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-07</th>\n",
       "      <td>1.88</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-28</th>\n",
       "      <td>3.39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-27</th>\n",
       "      <td>3.41</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-26</th>\n",
       "      <td>3.41</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-25</th>\n",
       "      <td>3.44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-24</th>\n",
       "      <td>3.60</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-23</th>\n",
       "      <td>3.63</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-22</th>\n",
       "      <td>3.63</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-21</th>\n",
       "      <td>3.62</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-20</th>\n",
       "      <td>3.67</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-19</th>\n",
       "      <td>3.67</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-18</th>\n",
       "      <td>3.84</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-17</th>\n",
       "      <td>3.85</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-16</th>\n",
       "      <td>3.85</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-15</th>\n",
       "      <td>3.82</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-14</th>\n",
       "      <td>3.66</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-12</th>\n",
       "      <td>3.66</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-11</th>\n",
       "      <td>3.68</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-10</th>\n",
       "      <td>3.66</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-09</th>\n",
       "      <td>3.65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-08</th>\n",
       "      <td>3.65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-07</th>\n",
       "      <td>3.67</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-03</th>\n",
       "      <td>3.67</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-02</th>\n",
       "      <td>3.73</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-01</th>\n",
       "      <td>3.64</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>3.75</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-30</th>\n",
       "      <td>3.69</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-29</th>\n",
       "      <td>3.65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-28</th>\n",
       "      <td>3.65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-27</th>\n",
       "      <td>3.69</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-26</th>\n",
       "      <td>3.69</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-25</th>\n",
       "      <td>3.68</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-24</th>\n",
       "      <td>3.68</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-23</th>\n",
       "      <td>3.56</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-22</th>\n",
       "      <td>3.66</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-21</th>\n",
       "      <td>3.69</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-20</th>\n",
       "      <td>3.56</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-19</th>\n",
       "      <td>3.51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-09</th>\n",
       "      <td>3.51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-08</th>\n",
       "      <td>3.49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-07</th>\n",
       "      <td>3.47</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-06</th>\n",
       "      <td>3.30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-05</th>\n",
       "      <td>3.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-04</th>\n",
       "      <td>3.45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-03</th>\n",
       "      <td>3.46</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-02</th>\n",
       "      <td>3.64</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-21</th>\n",
       "      <td>3.64</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-20</th>\n",
       "      <td>3.65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-13</th>\n",
       "      <td>3.65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-12</th>\n",
       "      <td>3.67</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-11</th>\n",
       "      <td>3.80</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-10</th>\n",
       "      <td>3.86</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-05</th>\n",
       "      <td>3.86</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-04</th>\n",
       "      <td>3.87</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-03</th>\n",
       "      <td>3.89</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-02</th>\n",
       "      <td>3.87</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>3.87</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-30</th>\n",
       "      <td>3.90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-29</th>\n",
       "      <td>3.92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-28</th>\n",
       "      <td>3.93</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-27</th>\n",
       "      <td>4.01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-26</th>\n",
       "      <td>4.04</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-23</th>\n",
       "      <td>4.04</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-22</th>\n",
       "      <td>4.03</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-16</th>\n",
       "      <td>4.03</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-15</th>\n",
       "      <td>3.99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-14</th>\n",
       "      <td>3.97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-12</th>\n",
       "      <td>3.97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-11</th>\n",
       "      <td>3.99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-10</th>\n",
       "      <td>4.24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-09</th>\n",
       "      <td>4.25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-08</th>\n",
       "      <td>4.25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-07</th>\n",
       "      <td>4.24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-06</th>\n",
       "      <td>4.23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-05</th>\n",
       "      <td>4.24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-29</th>\n",
       "      <td>4.24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28</th>\n",
       "      <td>4.25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-27</th>\n",
       "      <td>4.27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-25</th>\n",
       "      <td>4.27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-24</th>\n",
       "      <td>4.26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-23</th>\n",
       "      <td>4.25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-22</th>\n",
       "      <td>4.22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-20</th>\n",
       "      <td>4.22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-19</th>\n",
       "      <td>4.25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-18</th>\n",
       "      <td>4.21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-17</th>\n",
       "      <td>4.19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-16</th>\n",
       "      <td>4.19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-15</th>\n",
       "      <td>4.07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-14</th>\n",
       "      <td>4.04</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-05</th>\n",
       "      <td>2.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-04</th>\n",
       "      <td>2.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-03</th>\n",
       "      <td>2.63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-02</th>\n",
       "      <td>2.64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>2.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>2.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-29</th>\n",
       "      <td>2.63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-28</th>\n",
       "      <td>2.64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-27</th>\n",
       "      <td>2.64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-26</th>\n",
       "      <td>2.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-25</th>\n",
       "      <td>2.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-24</th>\n",
       "      <td>2.82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-22</th>\n",
       "      <td>2.82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-21</th>\n",
       "      <td>2.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-20</th>\n",
       "      <td>2.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-19</th>\n",
       "      <td>2.82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-18</th>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-17</th>\n",
       "      <td>2.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-16</th>\n",
       "      <td>2.82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-15</th>\n",
       "      <td>2.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-14</th>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-13</th>\n",
       "      <td>2.79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-12</th>\n",
       "      <td>2.79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-11</th>\n",
       "      <td>2.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-10</th>\n",
       "      <td>2.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-09</th>\n",
       "      <td>2.78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-08</th>\n",
       "      <td>3.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-07</th>\n",
       "      <td>3.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-06</th>\n",
       "      <td>2.95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-05</th>\n",
       "      <td>2.95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-04</th>\n",
       "      <td>2.96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-09</th>\n",
       "      <td>1.67</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-08</th>\n",
       "      <td>1.59</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-07</th>\n",
       "      <td>1.45</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-06</th>\n",
       "      <td>1.38</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-05</th>\n",
       "      <td>1.42</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-04</th>\n",
       "      <td>1.42</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-03</th>\n",
       "      <td>1.28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-02</th>\n",
       "      <td>1.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-01</th>\n",
       "      <td>1.39</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-26</th>\n",
       "      <td>1.39</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-25</th>\n",
       "      <td>2.24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-24</th>\n",
       "      <td>2.28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-21</th>\n",
       "      <td>2.28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-20</th>\n",
       "      <td>2.06</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-19</th>\n",
       "      <td>1.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-13</th>\n",
       "      <td>1.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-12</th>\n",
       "      <td>2.14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-11</th>\n",
       "      <td>2.28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-10</th>\n",
       "      <td>2.05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-09</th>\n",
       "      <td>1.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-07</th>\n",
       "      <td>1.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-06</th>\n",
       "      <td>2.45</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-05</th>\n",
       "      <td>2.47</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-02</th>\n",
       "      <td>2.47</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-01</th>\n",
       "      <td>2.25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>2.18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-24</th>\n",
       "      <td>2.18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-23</th>\n",
       "      <td>1.69</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-22</th>\n",
       "      <td>1.72</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-21</th>\n",
       "      <td>1.85</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-20</th>\n",
       "      <td>1.85</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-19</th>\n",
       "      <td>2.08</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-18</th>\n",
       "      <td>2.40</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-16</th>\n",
       "      <td>2.40</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-15</th>\n",
       "      <td>2.69</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-05</th>\n",
       "      <td>2.69</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-04</th>\n",
       "      <td>2.81</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-03</th>\n",
       "      <td>3.09</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-02</th>\n",
       "      <td>3.42</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-01</th>\n",
       "      <td>3.73</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-27</th>\n",
       "      <td>3.73</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-26</th>\n",
       "      <td>3.83</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-25</th>\n",
       "      <td>4.12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-23</th>\n",
       "      <td>4.12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-22</th>\n",
       "      <td>3.94</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-21</th>\n",
       "      <td>3.73</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-15</th>\n",
       "      <td>3.73</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-14</th>\n",
       "      <td>3.39</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-13</th>\n",
       "      <td>2.98</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-11</th>\n",
       "      <td>2.98</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-10</th>\n",
       "      <td>2.95</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-09</th>\n",
       "      <td>2.66</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-07</th>\n",
       "      <td>2.66</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-06</th>\n",
       "      <td>2.78</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-05</th>\n",
       "      <td>2.95</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-02</th>\n",
       "      <td>2.95</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01</th>\n",
       "      <td>3.13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>3.37</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-30</th>\n",
       "      <td>2.95</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-29</th>\n",
       "      <td>2.71</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28</th>\n",
       "      <td>2.63</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-21</th>\n",
       "      <td>2.63</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-20</th>\n",
       "      <td>2.26</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-19</th>\n",
       "      <td>2.05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-18</th>\n",
       "      <td>2.05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-17</th>\n",
       "      <td>1.86</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-16</th>\n",
       "      <td>1.76</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-13</th>\n",
       "      <td>1.76</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-12</th>\n",
       "      <td>1.87</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-11</th>\n",
       "      <td>2.27</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-10</th>\n",
       "      <td>2.27</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-09</th>\n",
       "      <td>2.48</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-08</th>\n",
       "      <td>2.56</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-04</th>\n",
       "      <td>2.56</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-03</th>\n",
       "      <td>2.81</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-02</th>\n",
       "      <td>2.85</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01</th>\n",
       "      <td>2.85</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>2.86</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-27</th>\n",
       "      <td>3.16</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-26</th>\n",
       "      <td>3.53</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-24</th>\n",
       "      <td>3.53</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-23</th>\n",
       "      <td>2.97</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-22</th>\n",
       "      <td>2.89</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-21</th>\n",
       "      <td>3.11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-19</th>\n",
       "      <td>3.11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-18</th>\n",
       "      <td>2.72</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-17</th>\n",
       "      <td>2.51</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-16</th>\n",
       "      <td>2.24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-15</th>\n",
       "      <td>2.40</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-14</th>\n",
       "      <td>2.53</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-13</th>\n",
       "      <td>2.53</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-12</th>\n",
       "      <td>2.76</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-11</th>\n",
       "      <td>2.82</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-10</th>\n",
       "      <td>2.63</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-09</th>\n",
       "      <td>1.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-08</th>\n",
       "      <td>1.95</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-07</th>\n",
       "      <td>2.10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-06</th>\n",
       "      <td>2.24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-05</th>\n",
       "      <td>1.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-04</th>\n",
       "      <td>1.95</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-02</th>\n",
       "      <td>1.95</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>2.22</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>2.24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-19</th>\n",
       "      <td>2.24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-18</th>\n",
       "      <td>2.23</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-17</th>\n",
       "      <td>2.17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-16</th>\n",
       "      <td>2.24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-15</th>\n",
       "      <td>2.11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-14</th>\n",
       "      <td>2.01</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-13</th>\n",
       "      <td>2.24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-12</th>\n",
       "      <td>1.97</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-11</th>\n",
       "      <td>1.93</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>1.93</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>1.51</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>1.45</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>1.45</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>1.24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>1.04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>1.08</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>1.11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>0.97</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-30</th>\n",
       "      <td>1.04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29</th>\n",
       "      <td>1.12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-28</th>\n",
       "      <td>1.25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-27</th>\n",
       "      <td>1.34</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-26</th>\n",
       "      <td>1.72</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-25</th>\n",
       "      <td>1.91</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-21</th>\n",
       "      <td>1.91</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-20</th>\n",
       "      <td>1.66</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-19</th>\n",
       "      <td>1.65</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-18</th>\n",
       "      <td>1.82</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-17</th>\n",
       "      <td>1.87</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-15</th>\n",
       "      <td>1.87</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-14</th>\n",
       "      <td>1.58</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-13</th>\n",
       "      <td>1.54</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-02</th>\n",
       "      <td>1.54</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>1.61</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>1.85</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29</th>\n",
       "      <td>1.90</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27</th>\n",
       "      <td>1.90</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26</th>\n",
       "      <td>2.15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-25</th>\n",
       "      <td>2.19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21</th>\n",
       "      <td>2.19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20</th>\n",
       "      <td>2.43</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19</th>\n",
       "      <td>2.48</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17</th>\n",
       "      <td>2.48</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16</th>\n",
       "      <td>2.63</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15</th>\n",
       "      <td>2.77</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14</th>\n",
       "      <td>2.82</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-13</th>\n",
       "      <td>3.06</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-11</th>\n",
       "      <td>3.06</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-10</th>\n",
       "      <td>2.94</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-09</th>\n",
       "      <td>2.48</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02</th>\n",
       "      <td>2.48</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01</th>\n",
       "      <td>2.29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-31</th>\n",
       "      <td>2.19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-26</th>\n",
       "      <td>2.19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-25</th>\n",
       "      <td>2.15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-24</th>\n",
       "      <td>1.90</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-18</th>\n",
       "      <td>1.90</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-17</th>\n",
       "      <td>1.91</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-16</th>\n",
       "      <td>2.19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-10</th>\n",
       "      <td>2.19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-09</th>\n",
       "      <td>2.47</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-08</th>\n",
       "      <td>2.58</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-07</th>\n",
       "      <td>2.10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-06</th>\n",
       "      <td>2.23</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-05</th>\n",
       "      <td>2.29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-01</th>\n",
       "      <td>2.29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>2.57</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-29</th>\n",
       "      <td>2.74</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-28</th>\n",
       "      <td>3.02</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-23</th>\n",
       "      <td>3.02</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-22</th>\n",
       "      <td>3.07</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-21</th>\n",
       "      <td>3.38</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-10</th>\n",
       "      <td>3.38</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-09</th>\n",
       "      <td>2.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-08</th>\n",
       "      <td>3.88</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-07</th>\n",
       "      <td>3.96</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-11</th>\n",
       "      <td>3.96</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-10</th>\n",
       "      <td>4.24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-09</th>\n",
       "      <td>4.35</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-28</th>\n",
       "      <td>4.35</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-27</th>\n",
       "      <td>4.02</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-26</th>\n",
       "      <td>3.96</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-25</th>\n",
       "      <td>3.96</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-24</th>\n",
       "      <td>3.63</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-23</th>\n",
       "      <td>3.57</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-22</th>\n",
       "      <td>3.57</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-21</th>\n",
       "      <td>2.98</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-20</th>\n",
       "      <td>2.86</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-09</th>\n",
       "      <td>2.86</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-08</th>\n",
       "      <td>2.61</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-07</th>\n",
       "      <td>2.57</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-05</th>\n",
       "      <td>2.57</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-04</th>\n",
       "      <td>2.41</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-03</th>\n",
       "      <td>2.05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-02</th>\n",
       "      <td>1.83</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-01</th>\n",
       "      <td>2.28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-29</th>\n",
       "      <td>2.53</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-28</th>\n",
       "      <td>5.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-05</th>\n",
       "      <td>5.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-04</th>\n",
       "      <td>6.54</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-03</th>\n",
       "      <td>6.72</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-28</th>\n",
       "      <td>6.72</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-27</th>\n",
       "      <td>6.39</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-26</th>\n",
       "      <td>5.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-25</th>\n",
       "      <td>5.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-24</th>\n",
       "      <td>6.19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-23</th>\n",
       "      <td>6.79</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-22</th>\n",
       "      <td>6.79</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-21</th>\n",
       "      <td>7.89</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-20</th>\n",
       "      <td>8.25</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-11</th>\n",
       "      <td>8.25</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-10</th>\n",
       "      <td>8.83</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-09</th>\n",
       "      <td>8.98</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-02</th>\n",
       "      <td>8.98</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-01</th>\n",
       "      <td>8.45</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-31</th>\n",
       "      <td>7.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-30</th>\n",
       "      <td>7.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-29</th>\n",
       "      <td>8.08</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-28</th>\n",
       "      <td>8.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-15</th>\n",
       "      <td>8.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-14</th>\n",
       "      <td>8.95</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-13</th>\n",
       "      <td>9.49</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-12</th>\n",
       "      <td>9.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-28</th>\n",
       "      <td>9.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-27</th>\n",
       "      <td>9.78</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-26</th>\n",
       "      <td>8.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-25</th>\n",
       "      <td>9.12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-24</th>\n",
       "      <td>9.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-18</th>\n",
       "      <td>9.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-17</th>\n",
       "      <td>9.86</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-16</th>\n",
       "      <td>8.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-14</th>\n",
       "      <td>8.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-13</th>\n",
       "      <td>9.57</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-12</th>\n",
       "      <td>9.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-25</th>\n",
       "      <td>9.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-24</th>\n",
       "      <td>9.81</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-23</th>\n",
       "      <td>9.69</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-15</th>\n",
       "      <td>9.69</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-14</th>\n",
       "      <td>10.61</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-13</th>\n",
       "      <td>10.69</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12</th>\n",
       "      <td>10.44</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-11</th>\n",
       "      <td>8.71</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>8.71</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-29</th>\n",
       "      <td>8.09</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-28</th>\n",
       "      <td>7.72</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-25</th>\n",
       "      <td>7.72</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-24</th>\n",
       "      <td>8.18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-23</th>\n",
       "      <td>8.45</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-22</th>\n",
       "      <td>8.86</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-21</th>\n",
       "      <td>9.44</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-20</th>\n",
       "      <td>8.85</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-19</th>\n",
       "      <td>8.73</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-18</th>\n",
       "      <td>8.73</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-17</th>\n",
       "      <td>8.15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-16</th>\n",
       "      <td>8.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-15</th>\n",
       "      <td>8.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-14</th>\n",
       "      <td>7.42</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-13</th>\n",
       "      <td>7.27</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-12</th>\n",
       "      <td>7.27</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-11</th>\n",
       "      <td>6.69</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-10</th>\n",
       "      <td>6.54</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-09</th>\n",
       "      <td>6.54</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-08</th>\n",
       "      <td>6.07</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-07</th>\n",
       "      <td>5.95</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-06</th>\n",
       "      <td>5.95</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-05</th>\n",
       "      <td>5.59</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-04</th>\n",
       "      <td>5.49</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>5.49</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>5.12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-30</th>\n",
       "      <td>4.85</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-29</th>\n",
       "      <td>4.85</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-28</th>\n",
       "      <td>4.64</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-27</th>\n",
       "      <td>4.39</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-26</th>\n",
       "      <td>4.39</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-25</th>\n",
       "      <td>4.02</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-24</th>\n",
       "      <td>3.34</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-23</th>\n",
       "      <td>3.08</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-22</th>\n",
       "      <td>2.93</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-21</th>\n",
       "      <td>2.79</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-30</th>\n",
       "      <td>2.79</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-29</th>\n",
       "      <td>2.73</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-28</th>\n",
       "      <td>2.50</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-27</th>\n",
       "      <td>2.58</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-26</th>\n",
       "      <td>2.79</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01</th>\n",
       "      <td>2.79</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>2.96</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-29</th>\n",
       "      <td>3.06</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-14</th>\n",
       "      <td>3.06</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-13</th>\n",
       "      <td>2.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-12</th>\n",
       "      <td>2.77</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-11</th>\n",
       "      <td>2.77</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-10</th>\n",
       "      <td>2.70</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-09</th>\n",
       "      <td>2.48</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-07</th>\n",
       "      <td>2.48</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-06</th>\n",
       "      <td>2.37</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-05</th>\n",
       "      <td>2.19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-04</th>\n",
       "      <td>2.19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-03</th>\n",
       "      <td>2.12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-02</th>\n",
       "      <td>1.97</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01</th>\n",
       "      <td>1.97</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>1.82</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-29</th>\n",
       "      <td>1.53</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-17</th>\n",
       "      <td>1.53</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16</th>\n",
       "      <td>1.36</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-15</th>\n",
       "      <td>1.31</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-25</th>\n",
       "      <td>1.31</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-24</th>\n",
       "      <td>1.16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-23</th>\n",
       "      <td>1.16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-22</th>\n",
       "      <td>1.19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-16</th>\n",
       "      <td>1.19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-15</th>\n",
       "      <td>1.43</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-14</th>\n",
       "      <td>1.57</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-05</th>\n",
       "      <td>1.57</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-04</th>\n",
       "      <td>1.79</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-11</th>\n",
       "      <td>1.79</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-10</th>\n",
       "      <td>1.60</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-09</th>\n",
       "      <td>1.57</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-30</th>\n",
       "      <td>1.57</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-29</th>\n",
       "      <td>1.59</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-28</th>\n",
       "      <td>1.79</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-22</th>\n",
       "      <td>1.79</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-21</th>\n",
       "      <td>2.32</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-20</th>\n",
       "      <td>2.58</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-24</th>\n",
       "      <td>2.58</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-23</th>\n",
       "      <td>2.75</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-22</th>\n",
       "      <td>3.16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-26</th>\n",
       "      <td>3.16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-25</th>\n",
       "      <td>2.93</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-24</th>\n",
       "      <td>2.80</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-05</th>\n",
       "      <td>2.80</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-04</th>\n",
       "      <td>2.69</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-03</th>\n",
       "      <td>2.51</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-14</th>\n",
       "      <td>2.51</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-13</th>\n",
       "      <td>2.75</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-12</th>\n",
       "      <td>2.80</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-17</th>\n",
       "      <td>2.80</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-16</th>\n",
       "      <td>2.96</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-15</th>\n",
       "      <td>3.09</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-10</th>\n",
       "      <td>3.09</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-09</th>\n",
       "      <td>2.80</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-08</th>\n",
       "      <td>2.73</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-23</th>\n",
       "      <td>2.73</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-22</th>\n",
       "      <td>2.80</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-21</th>\n",
       "      <td>3.02</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-21</th>\n",
       "      <td>3.02</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-20</th>\n",
       "      <td>3.05</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-19</th>\n",
       "      <td>3.41</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-15</th>\n",
       "      <td>3.41</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-14</th>\n",
       "      <td>3.36</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-13</th>\n",
       "      <td>2.87</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-12</th>\n",
       "      <td>3.05</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-06</th>\n",
       "      <td>3.05</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-05</th>\n",
       "      <td>2.93</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-04</th>\n",
       "      <td>2.76</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-21</th>\n",
       "      <td>2.76</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-20</th>\n",
       "      <td>2.64</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-19</th>\n",
       "      <td>2.18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-12</th>\n",
       "      <td>2.18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-11</th>\n",
       "      <td>2.06</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>1.89</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>1.89</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>1.90</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>1.71</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>1.69</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-28</th>\n",
       "      <td>1.69</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-27</th>\n",
       "      <td>1.56</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-26</th>\n",
       "      <td>1.47</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-22</th>\n",
       "      <td>1.47</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-21</th>\n",
       "      <td>1.67</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-20</th>\n",
       "      <td>1.69</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-17</th>\n",
       "      <td>1.69</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16</th>\n",
       "      <td>1.74</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-15</th>\n",
       "      <td>1.56</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-14</th>\n",
       "      <td>1.51</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-13</th>\n",
       "      <td>1.69</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-12</th>\n",
       "      <td>1.51</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-11</th>\n",
       "      <td>1.47</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-07</th>\n",
       "      <td>1.47</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-06</th>\n",
       "      <td>1.63</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-05</th>\n",
       "      <td>1.66</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-04</th>\n",
       "      <td>1.27</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-03</th>\n",
       "      <td>1.25</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-02</th>\n",
       "      <td>1.25</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>1.06</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>1.11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29</th>\n",
       "      <td>1.21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28</th>\n",
       "      <td>1.12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27</th>\n",
       "      <td>1.10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26</th>\n",
       "      <td>1.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-25</th>\n",
       "      <td>0.96</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-24</th>\n",
       "      <td>0.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-23</th>\n",
       "      <td>1.08</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22</th>\n",
       "      <td>1.02</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21</th>\n",
       "      <td>1.16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20</th>\n",
       "      <td>1.31</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19</th>\n",
       "      <td>1.28</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18</th>\n",
       "      <td>1.31</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-28</th>\n",
       "      <td>6.79</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-25</th>\n",
       "      <td>6.79</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-24</th>\n",
       "      <td>6.85</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-23</th>\n",
       "      <td>7.04</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-02</th>\n",
       "      <td>7.04</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-01</th>\n",
       "      <td>6.67</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-31</th>\n",
       "      <td>5.58</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-30</th>\n",
       "      <td>5.41</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-29</th>\n",
       "      <td>5.02</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-28</th>\n",
       "      <td>5.53</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-27</th>\n",
       "      <td>5.53</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-26</th>\n",
       "      <td>5.56</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-25</th>\n",
       "      <td>6.26</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-24</th>\n",
       "      <td>5.68</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-23</th>\n",
       "      <td>5.53</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-22</th>\n",
       "      <td>5.53</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-21</th>\n",
       "      <td>6.08</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-20</th>\n",
       "      <td>6.53</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-19</th>\n",
       "      <td>6.99</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-13</th>\n",
       "      <td>6.99</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-12</th>\n",
       "      <td>7.29</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-11</th>\n",
       "      <td>7.72</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-10</th>\n",
       "      <td>7.72</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-09</th>\n",
       "      <td>8.44</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-08</th>\n",
       "      <td>9.44</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-22</th>\n",
       "      <td>9.44</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-21</th>\n",
       "      <td>10.40</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-20</th>\n",
       "      <td>10.44</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-02</th>\n",
       "      <td>10.44</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-01</th>\n",
       "      <td>11.40</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-31</th>\n",
       "      <td>11.44</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-25</th>\n",
       "      <td>11.44</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-24</th>\n",
       "      <td>10.73</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-23</th>\n",
       "      <td>10.44</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-22</th>\n",
       "      <td>10.44</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-21</th>\n",
       "      <td>9.73</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-20</th>\n",
       "      <td>9.44</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-19</th>\n",
       "      <td>9.44</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-18</th>\n",
       "      <td>8.94</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-17</th>\n",
       "      <td>8.73</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-16</th>\n",
       "      <td>8.73</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-15</th>\n",
       "      <td>8.21</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-14</th>\n",
       "      <td>8.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-13</th>\n",
       "      <td>8.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-12</th>\n",
       "      <td>7.48</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-11</th>\n",
       "      <td>7.27</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-10</th>\n",
       "      <td>7.27</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-09</th>\n",
       "      <td>6.75</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-08</th>\n",
       "      <td>6.54</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-07</th>\n",
       "      <td>6.54</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-06</th>\n",
       "      <td>6.12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-05</th>\n",
       "      <td>5.95</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-04</th>\n",
       "      <td>5.95</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-03</th>\n",
       "      <td>5.68</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-02</th>\n",
       "      <td>5.49</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>5.49</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>5.23</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-30</th>\n",
       "      <td>4.85</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-29</th>\n",
       "      <td>4.58</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-28</th>\n",
       "      <td>4.58</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-27</th>\n",
       "      <td>4.41</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-26</th>\n",
       "      <td>4.19</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-25</th>\n",
       "      <td>4.19</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-24</th>\n",
       "      <td>4.27</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-23</th>\n",
       "      <td>4.38</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-03</th>\n",
       "      <td>4.38</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-02</th>\n",
       "      <td>4.02</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01</th>\n",
       "      <td>3.99</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-22</th>\n",
       "      <td>3.99</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-21</th>\n",
       "      <td>4.14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-20</th>\n",
       "      <td>4.38</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-24</th>\n",
       "      <td>4.38</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-23</th>\n",
       "      <td>3.73</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-22</th>\n",
       "      <td>3.60</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-28</th>\n",
       "      <td>3.60</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-27</th>\n",
       "      <td>3.63</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-26</th>\n",
       "      <td>3.99</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-18</th>\n",
       "      <td>3.99</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-17</th>\n",
       "      <td>8.45</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-16</th>\n",
       "      <td>9.94</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-17</th>\n",
       "      <td>9.94</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-16</th>\n",
       "      <td>10.19</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-15</th>\n",
       "      <td>10.44</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>10.44</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-27</th>\n",
       "      <td>10.11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-26</th>\n",
       "      <td>9.44</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-15</th>\n",
       "      <td>9.44</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-14</th>\n",
       "      <td>9.38</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-13</th>\n",
       "      <td>8.72</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-06</th>\n",
       "      <td>8.72</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-05</th>\n",
       "      <td>8.57</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-04</th>\n",
       "      <td>7.99</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-09</th>\n",
       "      <td>7.99</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-08</th>\n",
       "      <td>10.74</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-29</th>\n",
       "      <td>10.99</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-06</th>\n",
       "      <td>1.69</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-11</th>\n",
       "      <td>1.69</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-10</th>\n",
       "      <td>1.53</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-09</th>\n",
       "      <td>1.51</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-08</th>\n",
       "      <td>1.71</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-30</th>\n",
       "      <td>1.71</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-29</th>\n",
       "      <td>1.93</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-28</th>\n",
       "      <td>1.83</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-27</th>\n",
       "      <td>1.67</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-26</th>\n",
       "      <td>1.69</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-25</th>\n",
       "      <td>1.89</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-22</th>\n",
       "      <td>1.89</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-21</th>\n",
       "      <td>1.50</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-20</th>\n",
       "      <td>1.45</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-16</th>\n",
       "      <td>1.45</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-15</th>\n",
       "      <td>1.67</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-14</th>\n",
       "      <td>1.67</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-13</th>\n",
       "      <td>1.87</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-12</th>\n",
       "      <td>2.15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-11</th>\n",
       "      <td>2.15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-10</th>\n",
       "      <td>2.09</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-17</th>\n",
       "      <td>3.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-29</th>\n",
       "      <td>3.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-29</th>\n",
       "      <td>7.06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-29</th>\n",
       "      <td>6.28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-29</th>\n",
       "      <td>5.28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-29</th>\n",
       "      <td>5.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-29</th>\n",
       "      <td>6.79</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-16</th>\n",
       "      <td>3.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-15</th>\n",
       "      <td>3.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-14</th>\n",
       "      <td>3.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-13</th>\n",
       "      <td>3.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-12</th>\n",
       "      <td>2.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11</th>\n",
       "      <td>2.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-10</th>\n",
       "      <td>2.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-09</th>\n",
       "      <td>2.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-08</th>\n",
       "      <td>2.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-07</th>\n",
       "      <td>2.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-06</th>\n",
       "      <td>3.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-05</th>\n",
       "      <td>3.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-04</th>\n",
       "      <td>3.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-03</th>\n",
       "      <td>3.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-02</th>\n",
       "      <td>3.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-01</th>\n",
       "      <td>3.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-30</th>\n",
       "      <td>3.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-17</th>\n",
       "      <td>5.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-16</th>\n",
       "      <td>6.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-13</th>\n",
       "      <td>6.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-12</th>\n",
       "      <td>6.09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11</th>\n",
       "      <td>5.82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-10</th>\n",
       "      <td>6.13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-09</th>\n",
       "      <td>6.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-08</th>\n",
       "      <td>6.41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-07</th>\n",
       "      <td>6.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-03</th>\n",
       "      <td>6.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-02</th>\n",
       "      <td>6.64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-01</th>\n",
       "      <td>6.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-30</th>\n",
       "      <td>7.06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-17</th>\n",
       "      <td>5.59</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-16</th>\n",
       "      <td>6.12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-15</th>\n",
       "      <td>6.28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-17</th>\n",
       "      <td>5.28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-17</th>\n",
       "      <td>5.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-17</th>\n",
       "      <td>6.79</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-27</th>\n",
       "      <td>3.91</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-26</th>\n",
       "      <td>3.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-25</th>\n",
       "      <td>4.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-24</th>\n",
       "      <td>4.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-23</th>\n",
       "      <td>4.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-22</th>\n",
       "      <td>4.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-21</th>\n",
       "      <td>4.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-20</th>\n",
       "      <td>4.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-19</th>\n",
       "      <td>3.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-18</th>\n",
       "      <td>3.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-17</th>\n",
       "      <td>4.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-16</th>\n",
       "      <td>4.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-15</th>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-14</th>\n",
       "      <td>3.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-13</th>\n",
       "      <td>3.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-12</th>\n",
       "      <td>3.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-11</th>\n",
       "      <td>3.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-10</th>\n",
       "      <td>3.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-09</th>\n",
       "      <td>3.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-08</th>\n",
       "      <td>4.86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-07</th>\n",
       "      <td>4.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-06</th>\n",
       "      <td>4.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-05</th>\n",
       "      <td>4.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-04</th>\n",
       "      <td>4.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-03</th>\n",
       "      <td>2.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-02</th>\n",
       "      <td>2.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-01</th>\n",
       "      <td>2.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31</th>\n",
       "      <td>3.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-30</th>\n",
       "      <td>3.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-29</th>\n",
       "      <td>3.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-28</th>\n",
       "      <td>3.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-27</th>\n",
       "      <td>3.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-26</th>\n",
       "      <td>3.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-25</th>\n",
       "      <td>2.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-24</th>\n",
       "      <td>2.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-23</th>\n",
       "      <td>2.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-22</th>\n",
       "      <td>2.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-21</th>\n",
       "      <td>2.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-20</th>\n",
       "      <td>3.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-19</th>\n",
       "      <td>3.13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-18</th>\n",
       "      <td>3.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-27</th>\n",
       "      <td>6.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-26</th>\n",
       "      <td>6.36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-25</th>\n",
       "      <td>6.36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-24</th>\n",
       "      <td>6.91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-23</th>\n",
       "      <td>6.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-22</th>\n",
       "      <td>6.46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-20</th>\n",
       "      <td>6.46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-19</th>\n",
       "      <td>6.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-18</th>\n",
       "      <td>6.49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-17</th>\n",
       "      <td>6.49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-16</th>\n",
       "      <td>6.41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-15</th>\n",
       "      <td>6.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-12</th>\n",
       "      <td>6.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-11</th>\n",
       "      <td>6.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-10</th>\n",
       "      <td>6.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-09</th>\n",
       "      <td>6.72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-08</th>\n",
       "      <td>6.78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-07</th>\n",
       "      <td>6.22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-06</th>\n",
       "      <td>6.18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-05</th>\n",
       "      <td>6.18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-04</th>\n",
       "      <td>5.78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-03</th>\n",
       "      <td>5.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-02</th>\n",
       "      <td>5.30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-01</th>\n",
       "      <td>5.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31</th>\n",
       "      <td>5.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-29</th>\n",
       "      <td>5.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-28</th>\n",
       "      <td>5.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-27</th>\n",
       "      <td>5.79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-26</th>\n",
       "      <td>5.42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-25</th>\n",
       "      <td>5.06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-24</th>\n",
       "      <td>5.28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-23</th>\n",
       "      <td>5.34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-22</th>\n",
       "      <td>5.14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-21</th>\n",
       "      <td>5.47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-20</th>\n",
       "      <td>5.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-19</th>\n",
       "      <td>5.82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-18</th>\n",
       "      <td>5.82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-27</th>\n",
       "      <td>5.74</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-26</th>\n",
       "      <td>6.04</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-21</th>\n",
       "      <td>6.04</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-20</th>\n",
       "      <td>6.05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-19</th>\n",
       "      <td>6.12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-17</th>\n",
       "      <td>6.12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-16</th>\n",
       "      <td>6.06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-15</th>\n",
       "      <td>5.92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-13</th>\n",
       "      <td>5.92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-12</th>\n",
       "      <td>5.83</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-10</th>\n",
       "      <td>5.83</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-09</th>\n",
       "      <td>5.76</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-08</th>\n",
       "      <td>5.66</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-07</th>\n",
       "      <td>5.37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-06</th>\n",
       "      <td>5.13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-05</th>\n",
       "      <td>5.09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-04</th>\n",
       "      <td>4.76</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-03</th>\n",
       "      <td>3.89</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-02</th>\n",
       "      <td>3.97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-01</th>\n",
       "      <td>4.02</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31</th>\n",
       "      <td>4.10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-29</th>\n",
       "      <td>4.10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-28</th>\n",
       "      <td>4.08</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-27</th>\n",
       "      <td>4.07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-26</th>\n",
       "      <td>4.06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-25</th>\n",
       "      <td>4.17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-24</th>\n",
       "      <td>4.24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-23</th>\n",
       "      <td>4.28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-22</th>\n",
       "      <td>4.28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-21</th>\n",
       "      <td>4.49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-20</th>\n",
       "      <td>5.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-19</th>\n",
       "      <td>5.18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-18</th>\n",
       "      <td>5.20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-27</th>\n",
       "      <td>4.45</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-26</th>\n",
       "      <td>4.45</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-25</th>\n",
       "      <td>4.87</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-24</th>\n",
       "      <td>4.91</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-10</th>\n",
       "      <td>4.91</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-09</th>\n",
       "      <td>4.64</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-08</th>\n",
       "      <td>4.45</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-01</th>\n",
       "      <td>4.45</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31</th>\n",
       "      <td>4.43</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-30</th>\n",
       "      <td>3.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-24</th>\n",
       "      <td>3.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-23</th>\n",
       "      <td>4.47</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-22</th>\n",
       "      <td>4.82</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-21</th>\n",
       "      <td>5.17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-20</th>\n",
       "      <td>5.28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-27</th>\n",
       "      <td>4.87</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-02</th>\n",
       "      <td>4.87</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-01</th>\n",
       "      <td>5.06</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31</th>\n",
       "      <td>5.33</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-29</th>\n",
       "      <td>5.33</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-28</th>\n",
       "      <td>5.77</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-27</th>\n",
       "      <td>5.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-27</th>\n",
       "      <td>6.79</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-21</th>\n",
       "      <td>2.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-12</th>\n",
       "      <td>5.39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-11</th>\n",
       "      <td>5.44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-10</th>\n",
       "      <td>5.44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-09</th>\n",
       "      <td>5.59</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-08</th>\n",
       "      <td>5.90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-31</th>\n",
       "      <td>5.90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-30</th>\n",
       "      <td>5.89</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-29</th>\n",
       "      <td>5.79</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-28</th>\n",
       "      <td>5.79</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-21</th>\n",
       "      <td>3.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-03</th>\n",
       "      <td>3.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-02</th>\n",
       "      <td>4.20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-01</th>\n",
       "      <td>4.45</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-21</th>\n",
       "      <td>4.87</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-21</th>\n",
       "      <td>6.79</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-20</th>\n",
       "      <td>2.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-19</th>\n",
       "      <td>2.93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-18</th>\n",
       "      <td>2.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-17</th>\n",
       "      <td>2.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-16</th>\n",
       "      <td>3.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-15</th>\n",
       "      <td>3.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-14</th>\n",
       "      <td>3.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-13</th>\n",
       "      <td>3.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-12</th>\n",
       "      <td>3.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-11</th>\n",
       "      <td>3.09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-10</th>\n",
       "      <td>2.86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-09</th>\n",
       "      <td>2.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-08</th>\n",
       "      <td>2.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-07</th>\n",
       "      <td>3.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-06</th>\n",
       "      <td>3.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-05</th>\n",
       "      <td>3.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-04</th>\n",
       "      <td>2.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-03</th>\n",
       "      <td>2.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-02</th>\n",
       "      <td>3.14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-01</th>\n",
       "      <td>3.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-31</th>\n",
       "      <td>3.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-30</th>\n",
       "      <td>3.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-29</th>\n",
       "      <td>4.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-28</th>\n",
       "      <td>4.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-21</th>\n",
       "      <td>5.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-20</th>\n",
       "      <td>5.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-19</th>\n",
       "      <td>5.44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-18</th>\n",
       "      <td>5.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-17</th>\n",
       "      <td>5.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-16</th>\n",
       "      <td>5.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-15</th>\n",
       "      <td>5.35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-14</th>\n",
       "      <td>5.86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-13</th>\n",
       "      <td>5.95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-12</th>\n",
       "      <td>6.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-11</th>\n",
       "      <td>6.13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-10</th>\n",
       "      <td>6.13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-09</th>\n",
       "      <td>6.17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-08</th>\n",
       "      <td>6.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-07</th>\n",
       "      <td>6.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-06</th>\n",
       "      <td>6.38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-03</th>\n",
       "      <td>6.38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-02</th>\n",
       "      <td>6.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-01</th>\n",
       "      <td>6.36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-31</th>\n",
       "      <td>6.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-30</th>\n",
       "      <td>6.42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-29</th>\n",
       "      <td>6.46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-28</th>\n",
       "      <td>6.43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-21</th>\n",
       "      <td>5.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-19</th>\n",
       "      <td>5.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-18</th>\n",
       "      <td>5.34</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-17</th>\n",
       "      <td>5.29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-16</th>\n",
       "      <td>5.37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-15</th>\n",
       "      <td>5.36</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-13</th>\n",
       "      <td>5.36</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-29</th>\n",
       "      <td>2.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-28</th>\n",
       "      <td>2.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-27</th>\n",
       "      <td>3.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-26</th>\n",
       "      <td>2.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-25</th>\n",
       "      <td>2.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-24</th>\n",
       "      <td>2.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-23</th>\n",
       "      <td>2.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-22</th>\n",
       "      <td>2.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-29</th>\n",
       "      <td>5.49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-28</th>\n",
       "      <td>5.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-27</th>\n",
       "      <td>5.49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-26</th>\n",
       "      <td>5.46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-25</th>\n",
       "      <td>5.43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-23</th>\n",
       "      <td>5.43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-22</th>\n",
       "      <td>5.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-29</th>\n",
       "      <td>4.93</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-28</th>\n",
       "      <td>4.93</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-27</th>\n",
       "      <td>4.95</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-26</th>\n",
       "      <td>5.43</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-25</th>\n",
       "      <td>5.36</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-23</th>\n",
       "      <td>5.36</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-22</th>\n",
       "      <td>5.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-29</th>\n",
       "      <td>3.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-29</th>\n",
       "      <td>4.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-28</th>\n",
       "      <td>4.87</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-29</th>\n",
       "      <td>6.79</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            price  cardtype\n",
       "carddate                   \n",
       "2020-01-03  5.20   0       \n",
       "2020-05-02  4.84   0       \n",
       "2020-05-01  4.82   0       \n",
       "2020-04-30  4.87   0       \n",
       "2020-04-29  4.73   0       \n",
       "2020-04-28  4.64   0       \n",
       "2020-04-27  4.70   0       \n",
       "2020-04-26  4.61   0       \n",
       "2020-04-25  4.36   0       \n",
       "2020-04-24  4.02   0       \n",
       "2020-04-23  3.90   0       \n",
       "2020-04-22  3.88   0       \n",
       "2020-04-21  3.67   0       \n",
       "2020-04-20  3.83   0       \n",
       "2020-04-19  3.83   0       \n",
       "2020-04-18  3.92   0       \n",
       "2020-04-17  3.80   0       \n",
       "2020-04-16  3.75   0       \n",
       "2020-04-15  3.60   0       \n",
       "2020-04-14  3.66   0       \n",
       "2020-04-13  3.68   0       \n",
       "2020-04-12  3.85   0       \n",
       "2020-04-11  4.53   0       \n",
       "2020-04-10  4.91   0       \n",
       "2020-04-09  4.77   0       \n",
       "2020-04-08  4.85   0       \n",
       "2020-04-07  5.49   0       \n",
       "2020-04-06  4.79   0       \n",
       "2020-04-05  4.29   0       \n",
       "2020-04-04  4.53   0       \n",
       "2020-04-03  4.52   0       \n",
       "2020-04-02  4.42   0       \n",
       "2020-04-01  4.63   0       \n",
       "2020-03-31  4.70   0       \n",
       "2020-03-30  5.32   0       \n",
       "2020-03-29  5.07   0       \n",
       "2020-03-28  4.51   0       \n",
       "2020-03-27  3.73   0       \n",
       "2020-03-26  3.54   0       \n",
       "2020-03-25  3.93   0       \n",
       "2020-03-24  3.96   0       \n",
       "2020-03-23  4.22   0       \n",
       "2020-03-22  4.28   0       \n",
       "2020-03-21  4.53   0       \n",
       "2020-03-20  4.50   0       \n",
       "2020-03-19  5.12   0       \n",
       "2020-03-18  5.06   0       \n",
       "2020-03-17  5.43   0       \n",
       "2020-03-16  5.75   0       \n",
       "2020-03-15  6.46   0       \n",
       "2020-03-14  6.67   0       \n",
       "2020-03-13  6.47   0       \n",
       "2020-03-12  6.37   0       \n",
       "2020-03-11  5.71   0       \n",
       "2020-03-10  4.76   0       \n",
       "2020-03-09  4.61   0       \n",
       "2020-03-08  4.92   0       \n",
       "2020-03-07  4.50   0       \n",
       "2020-03-06  4.17   0       \n",
       "2020-03-05  4.34   0       \n",
       "2020-03-04  4.51   0       \n",
       "2020-03-03  4.88   0       \n",
       "2020-03-02  4.30   0       \n",
       "2020-03-01  5.20   0       \n",
       "2020-02-29  5.39   0       \n",
       "2020-02-28  4.99   0       \n",
       "2020-02-27  4.87   0       \n",
       "2020-02-26  4.75   0       \n",
       "2020-02-25  4.31   0       \n",
       "2020-02-24  4.35   0       \n",
       "2020-02-23  4.27   0       \n",
       "2020-02-22  4.23   0       \n",
       "2020-02-21  4.37   0       \n",
       "2020-02-20  4.94   0       \n",
       "2020-02-19  5.07   0       \n",
       "2020-02-18  5.16   0       \n",
       "2020-02-17  5.18   0       \n",
       "2020-02-16  5.20   0       \n",
       "2020-02-15  5.26   0       \n",
       "2020-02-14  6.07   0       \n",
       "2020-02-13  6.41   0       \n",
       "2020-02-12  6.44   0       \n",
       "2020-02-11  6.29   0       \n",
       "2020-02-10  6.44   0       \n",
       "2020-02-09  6.90   0       \n",
       "2020-02-08  7.33   0       \n",
       "2020-02-07  7.08   0       \n",
       "2020-02-06  7.63   0       \n",
       "2020-02-05  7.25   0       \n",
       "2020-02-04  6.45   0       \n",
       "2020-02-03  6.36   0       \n",
       "2020-02-02  6.84   0       \n",
       "2020-02-01  6.15   0       \n",
       "2020-01-31  6.77   0       \n",
       "2020-01-30  7.34   0       \n",
       "2020-01-29  7.88   0       \n",
       "2020-01-28  8.30   0       \n",
       "2020-01-27  9.39   0       \n",
       "2020-01-26  10.63  0       \n",
       "2020-01-25  10.42  0       \n",
       "2020-01-24  9.95   0       \n",
       "2020-01-23  8.64   0       \n",
       "2020-01-22  8.20   0       \n",
       "2020-01-21  8.20   0       \n",
       "2020-01-20  7.03   0       \n",
       "2020-01-19  6.74   0       \n",
       "2020-01-18  7.50   0       \n",
       "2020-01-17  7.97   0       \n",
       "2020-01-16  7.79   0       \n",
       "2020-01-15  8.48   0       \n",
       "2020-01-14  8.53   0       \n",
       "2020-01-13  9.20   0       \n",
       "2020-01-12  9.64   0       \n",
       "2020-01-11  9.61   0       \n",
       "2020-01-10  10.45  0       \n",
       "2020-01-09  9.28   0       \n",
       "2020-01-08  7.43   0       \n",
       "2020-01-07  6.02   0       \n",
       "2020-01-06  5.69   0       \n",
       "2020-01-05  5.58   0       \n",
       "2020-01-04  5.62   0       \n",
       "2019-09-05  1.43   0       \n",
       "2019-09-04  1.27   0       \n",
       "2019-09-03  1.32   0       \n",
       "2019-09-02  1.57   0       \n",
       "2019-09-01  1.66   0       \n",
       "2019-08-31  1.72   0       \n",
       "2019-08-30  1.73   0       \n",
       "2019-08-29  1.59   0       \n",
       "2019-08-28  1.45   0       \n",
       "2019-08-27  1.22   0       \n",
       "2019-08-26  1.16   0       \n",
       "2019-08-25  1.19   0       \n",
       "2019-08-24  1.31   0       \n",
       "2019-08-23  1.48   0       \n",
       "2019-08-22  1.39   0       \n",
       "2019-08-21  1.44   0       \n",
       "2019-08-20  1.50   0       \n",
       "2019-08-19  1.48   0       \n",
       "2019-08-18  1.51   0       \n",
       "2019-08-17  1.71   0       \n",
       "2019-08-16  1.71   0       \n",
       "2019-08-15  1.46   0       \n",
       "2019-08-14  1.32   0       \n",
       "2019-08-13  1.34   0       \n",
       "2019-08-12  1.34   0       \n",
       "2019-08-11  1.14   0       \n",
       "2019-08-10  1.10   0       \n",
       "2019-08-09  0.93   0       \n",
       "2019-08-08  0.78   0       \n",
       "2019-08-07  0.84   0       \n",
       "2019-08-06  0.94   0       \n",
       "2019-08-05  0.94   0       \n",
       "2019-08-04  1.06   0       \n",
       "2019-08-03  0.94   0       \n",
       "2019-08-02  0.96   0       \n",
       "2019-08-01  1.01   0       \n",
       "2019-07-31  1.01   0       \n",
       "2019-07-30  1.25   0       \n",
       "2019-07-29  1.19   0       \n",
       "2019-07-28  0.94   0       \n",
       "2019-07-27  0.94   0       \n",
       "2019-07-26  0.98   0       \n",
       "2019-07-25  0.91   0       \n",
       "2019-07-24  0.92   0       \n",
       "2019-07-23  0.91   0       \n",
       "2019-07-22  0.98   0       \n",
       "2019-07-21  1.05   0       \n",
       "2019-07-20  1.02   0       \n",
       "2019-07-19  1.16   0       \n",
       "2019-07-18  1.30   0       \n",
       "2019-07-17  1.39   0       \n",
       "2019-07-16  1.30   0       \n",
       "2019-07-15  1.33   0       \n",
       "2019-07-14  1.07   0       \n",
       "2019-07-13  1.16   0       \n",
       "2019-07-12  1.19   0       \n",
       "2019-07-11  1.23   0       \n",
       "2019-07-10  1.11   0       \n",
       "2019-07-09  1.26   0       \n",
       "2019-07-08  1.40   0       \n",
       "2019-07-07  1.49   0       \n",
       "2019-07-06  1.58   0       \n",
       "2019-07-05  1.65   0       \n",
       "2019-07-04  1.61   0       \n",
       "2019-07-03  1.70   0       \n",
       "2019-07-02  1.55   0       \n",
       "2019-07-01  1.46   0       \n",
       "2019-06-30  1.32   0       \n",
       "2019-06-29  1.12   0       \n",
       "2019-06-28  1.19   0       \n",
       "2019-06-27  1.17   0       \n",
       "2019-06-26  1.15   0       \n",
       "2019-06-25  1.29   0       \n",
       "2019-06-24  1.38   0       \n",
       "2019-06-23  1.65   0       \n",
       "2019-06-22  1.64   0       \n",
       "2019-06-21  1.47   0       \n",
       "2019-06-20  1.55   0       \n",
       "2019-06-19  1.46   0       \n",
       "2019-06-18  1.44   0       \n",
       "2019-06-17  1.41   0       \n",
       "2019-06-16  1.34   0       \n",
       "2019-06-15  1.26   0       \n",
       "2019-06-14  1.45   0       \n",
       "2019-06-13  1.51   0       \n",
       "2019-06-12  1.52   0       \n",
       "2019-06-11  1.39   0       \n",
       "2019-06-10  1.31   0       \n",
       "2019-06-09  1.55   0       \n",
       "2019-06-08  1.63   0       \n",
       "2019-06-07  1.65   0       \n",
       "2019-06-06  1.64   0       \n",
       "2019-06-05  1.62   0       \n",
       "2019-06-04  1.79   0       \n",
       "2019-06-03  1.63   0       \n",
       "2019-06-02  1.38   0       \n",
       "2019-06-01  1.63   0       \n",
       "2019-05-31  1.56   0       \n",
       "2019-05-30  1.46   0       \n",
       "2019-05-29  1.45   0       \n",
       "2019-05-28  1.42   0       \n",
       "2019-05-27  1.50   0       \n",
       "2019-05-26  1.31   0       \n",
       "2019-05-25  1.20   0       \n",
       "2019-05-24  1.17   0       \n",
       "2019-05-23  1.21   0       \n",
       "2019-05-22  1.18   0       \n",
       "2019-05-21  1.08   0       \n",
       "2019-05-20  1.04   0       \n",
       "2019-05-19  1.12   0       \n",
       "2019-05-18  1.03   0       \n",
       "2019-05-17  1.14   0       \n",
       "2019-05-16  1.31   0       \n",
       "2019-05-15  1.31   0       \n",
       "2019-05-14  1.63   0       \n",
       "2019-05-13  1.48   0       \n",
       "2019-05-12  1.20   0       \n",
       "2019-05-11  1.15   0       \n",
       "2019-05-10  1.13   0       \n",
       "2019-05-09  1.21   0       \n",
       "2019-05-08  1.28   0       \n",
       "2019-05-07  1.28   0       \n",
       "2019-05-06  1.46   0       \n",
       "2019-05-05  1.35   0       \n",
       "2019-05-04  1.19   0       \n",
       "2019-05-03  1.16   0       \n",
       "2019-05-02  1.08   0       \n",
       "2019-05-01  0.93   0       \n",
       "2019-04-30  0.74   0       \n",
       "2019-04-29  0.66   0       \n",
       "2019-04-28  0.59   0       \n",
       "2019-04-27  0.58   0       \n",
       "2019-04-26  0.40   0       \n",
       "2019-04-25  0.51   0       \n",
       "2019-04-24  0.60   0       \n",
       "2019-04-23  0.73   0       \n",
       "2019-04-22  0.96   0       \n",
       "2019-04-21  1.12   0       \n",
       "2019-04-20  1.04   0       \n",
       "2019-04-19  1.13   0       \n",
       "2019-04-18  1.17   0       \n",
       "2019-04-17  1.23   0       \n",
       "2019-04-16  1.21   0       \n",
       "2019-04-15  1.24   0       \n",
       "2019-04-14  1.12   0       \n",
       "2019-04-13  1.13   0       \n",
       "2019-04-12  1.02   0       \n",
       "2019-04-11  1.19   0       \n",
       "2019-04-10  1.08   0       \n",
       "2019-04-09  1.19   0       \n",
       "2019-04-08  1.06   0       \n",
       "2019-04-07  0.78   0       \n",
       "2019-04-06  0.77   0       \n",
       "2019-04-05  0.77   0       \n",
       "2019-04-04  0.71   0       \n",
       "2019-04-03  0.67   0       \n",
       "2019-04-02  0.71   0       \n",
       "2019-04-01  0.72   0       \n",
       "2019-03-31  0.74   0       \n",
       "2019-03-30  0.74   0       \n",
       "2019-03-29  0.79   0       \n",
       "2019-03-28  0.79   0       \n",
       "2019-03-27  0.77   0       \n",
       "2019-03-26  0.75   0       \n",
       "2019-03-25  0.75   0       \n",
       "2019-03-24  0.79   0       \n",
       "2019-03-23  0.55   0       \n",
       "2019-03-22  0.56   0       \n",
       "2019-03-21  0.58   0       \n",
       "2019-03-20  0.65   0       \n",
       "2019-03-19  0.66   0       \n",
       "2019-03-18  0.70   0       \n",
       "2019-03-17  0.70   0       \n",
       "2019-03-16  0.65   0       \n",
       "2019-03-15  0.64   0       \n",
       "2019-03-14  0.67   0       \n",
       "2019-03-13  0.64   0       \n",
       "2019-03-12  0.71   0       \n",
       "2019-03-11  0.68   0       \n",
       "2019-03-10  0.67   0       \n",
       "2019-03-09  0.55   0       \n",
       "2019-03-08  0.38   0       \n",
       "2019-03-07  0.43   0       \n",
       "2019-03-06  0.46   0       \n",
       "2019-03-05  0.39   0       \n",
       "2019-03-04  0.42   0       \n",
       "2019-03-03  0.45   0       \n",
       "2019-03-02  0.39   0       \n",
       "2019-03-01  0.53   0       \n",
       "2019-02-28  0.60   0       \n",
       "2019-02-27  0.45   0       \n",
       "2019-02-26  0.35   0       \n",
       "2019-02-25  0.34   0       \n",
       "2019-02-24  0.26   0       \n",
       "2019-02-23  0.23   0       \n",
       "2019-02-22  0.26   0       \n",
       "2019-02-21  0.26   0       \n",
       "2019-02-20  0.29   0       \n",
       "2019-02-19  0.30   0       \n",
       "2019-02-18  0.33   0       \n",
       "2019-02-17  0.24   0       \n",
       "2019-02-16  0.25   0       \n",
       "2019-02-15  0.19   0       \n",
       "2019-02-14  0.19   0       \n",
       "2019-02-13  0.23   0       \n",
       "2019-02-12  0.22   0       \n",
       "2019-02-11  0.29   0       \n",
       "2019-02-10  0.22   0       \n",
       "2019-02-09  0.20   0       \n",
       "2019-02-08  0.20   0       \n",
       "2019-02-07  0.22   0       \n",
       "2019-02-06  0.28   0       \n",
       "2019-02-05  0.24   0       \n",
       "2019-02-04  0.18   0       \n",
       "2019-02-03  0.18   0       \n",
       "2019-02-02  0.16   0       \n",
       "2019-02-01  0.21   0       \n",
       "2019-01-31  0.23   0       \n",
       "2019-01-30  0.22   0       \n",
       "2019-01-29  0.25   0       \n",
       "2019-01-28  0.26   0       \n",
       "2019-01-27  0.26   0       \n",
       "2019-01-26  0.22   0       \n",
       "2019-01-25  0.27   0       \n",
       "2019-01-24  0.23   0       \n",
       "2019-01-23  0.26   0       \n",
       "2019-01-22  0.38   0       \n",
       "2019-01-21  0.47   0       \n",
       "2019-01-20  0.40   0       \n",
       "2019-01-19  0.36   0       \n",
       "2019-01-18  0.42   0       \n",
       "2019-01-17  0.46   0       \n",
       "2019-01-16  0.60   0       \n",
       "2019-01-15  0.62   0       \n",
       "2019-01-14  0.65   0       \n",
       "2019-01-13  0.64   0       \n",
       "2019-01-12  0.62   0       \n",
       "2019-01-11  0.58   0       \n",
       "2019-01-10  0.60   0       \n",
       "2019-01-09  0.61   0       \n",
       "2019-01-08  0.50   0       \n",
       "2019-01-07  0.51   0       \n",
       "2019-01-06  0.48   0       \n",
       "2019-01-05  0.50   0       \n",
       "2019-01-04  0.58   0       \n",
       "2019-01-03  0.68   0       \n",
       "2019-01-02  0.69   0       \n",
       "2019-01-01  0.74   0       \n",
       "2018-12-31  0.84   0       \n",
       "2018-12-30  0.77   0       \n",
       "2018-12-29  0.70   0       \n",
       "2018-12-28  0.62   0       \n",
       "2018-12-27  0.53   0       \n",
       "2018-12-26  0.56   0       \n",
       "2018-12-25  0.56   0       \n",
       "2018-12-24  0.48   0       \n",
       "2018-12-23  0.40   0       \n",
       "2018-12-22  0.43   0       \n",
       "2018-12-21  0.41   0       \n",
       "2018-12-20  0.43   0       \n",
       "2018-12-19  0.54   0       \n",
       "2018-12-18  0.51   0       \n",
       "2018-12-17  0.51   0       \n",
       "2018-12-16  0.40   0       \n",
       "2018-12-15  0.30   0       \n",
       "2018-12-14  0.22   0       \n",
       "2018-12-13  0.23   0       \n",
       "2018-12-12  0.24   0       \n",
       "2018-12-11  0.23   0       \n",
       "2018-12-10  0.27   0       \n",
       "2018-12-09  0.22   0       \n",
       "2018-12-08  0.14   0       \n",
       "2018-12-07  0.15   0       \n",
       "2018-12-06  0.23   0       \n",
       "2018-12-05  0.15   0       \n",
       "2018-12-04  0.56   0       \n",
       "2018-12-03  0.52   0       \n",
       "2018-12-02  0.65   0       \n",
       "2018-12-01  0.76   0       \n",
       "2018-11-30  0.66   0       \n",
       "2018-11-29  0.58   0       \n",
       "2018-11-28  0.56   0       \n",
       "2018-11-27  0.54   0       \n",
       "2018-11-26  0.54   0       \n",
       "2018-11-25  0.47   0       \n",
       "2018-11-24  0.38   0       \n",
       "2018-11-23  0.51   0       \n",
       "2018-11-22  0.53   0       \n",
       "2018-11-21  0.60   0       \n",
       "2018-11-20  0.48   0       \n",
       "2018-11-19  0.38   0       \n",
       "2018-11-18  0.43   0       \n",
       "2018-11-17  0.44   0       \n",
       "2018-11-16  0.42   0       \n",
       "2018-11-15  0.51   0       \n",
       "2018-11-14  0.44   0       \n",
       "2018-11-13  0.44   0       \n",
       "2018-11-12  0.35   0       \n",
       "2018-11-11  0.55   0       \n",
       "2018-11-10  0.60   0       \n",
       "2018-11-09  0.61   0       \n",
       "2018-11-08  0.61   0       \n",
       "2018-11-07  0.67   0       \n",
       "2018-11-06  0.67   0       \n",
       "2018-11-05  0.70   0       \n",
       "2018-11-04  0.69   0       \n",
       "2018-11-03  0.68   0       \n",
       "2018-11-02  0.76   0       \n",
       "2018-11-01  0.70   0       \n",
       "2018-10-31  0.61   0       \n",
       "2018-10-30  0.80   0       \n",
       "2018-10-29  0.77   0       \n",
       "2018-10-28  0.67   0       \n",
       "2018-10-27  0.72   0       \n",
       "2018-10-26  0.76   0       \n",
       "2018-10-25  0.78   0       \n",
       "2018-10-24  0.85   0       \n",
       "2018-10-23  0.86   0       \n",
       "2018-10-22  0.83   0       \n",
       "2018-10-21  0.72   0       \n",
       "2018-10-20  0.72   0       \n",
       "2018-10-19  0.63   0       \n",
       "2018-10-18  0.58   0       \n",
       "2018-10-17  0.80   0       \n",
       "2018-10-16  0.83   0       \n",
       "2018-10-15  0.81   0       \n",
       "2018-10-14  0.83   0       \n",
       "2018-10-13  0.75   0       \n",
       "2018-10-12  0.75   0       \n",
       "2018-10-11  0.77   0       \n",
       "2018-10-10  0.77   0       \n",
       "2018-10-09  0.78   0       \n",
       "2018-10-08  0.95   0       \n",
       "2018-10-07  0.89   0       \n",
       "2018-10-06  0.97   0       \n",
       "2018-10-05  1.06   0       \n",
       "2018-10-04  1.14   0       \n",
       "2018-10-03  1.15   0       \n",
       "2018-10-02  1.08   0       \n",
       "2018-10-01  1.19   0       \n",
       "2018-09-30  1.09   0       \n",
       "2018-09-29  0.82   0       \n",
       "2018-09-28  1.04   0       \n",
       "2018-09-27  1.15   0       \n",
       "2018-09-26  1.39   0       \n",
       "2018-09-25  1.41   0       \n",
       "2018-09-24  1.41   0       \n",
       "2018-09-23  1.52   0       \n",
       "2018-09-22  1.65   0       \n",
       "2018-09-21  1.62   0       \n",
       "2018-09-20  1.70   0       \n",
       "2018-09-19  1.80   0       \n",
       "2018-09-18  1.88   0       \n",
       "2018-09-17  1.87   0       \n",
       "2018-09-16  1.83   0       \n",
       "2018-09-15  1.80   0       \n",
       "2018-09-14  1.72   0       \n",
       "2018-09-13  1.81   0       \n",
       "2018-09-12  2.19   0       \n",
       "2018-09-11  2.36   0       \n",
       "2018-09-10  2.51   0       \n",
       "2018-09-09  2.23   0       \n",
       "2018-09-08  1.97   0       \n",
       "2018-09-07  1.95   0       \n",
       "2018-09-06  2.04   0       \n",
       "2018-09-05  2.01   0       \n",
       "2018-09-04  1.91   0       \n",
       "2018-09-03  1.75   0       \n",
       "2018-09-02  1.53   0       \n",
       "2018-09-01  1.47   0       \n",
       "2018-08-31  1.36   0       \n",
       "2018-08-30  1.24   0       \n",
       "2018-08-29  1.24   0       \n",
       "2018-08-28  1.37   0       \n",
       "2018-08-27  1.50   0       \n",
       "2018-08-26  1.62   0       \n",
       "2018-08-25  1.68   0       \n",
       "2018-08-24  1.55   0       \n",
       "2018-08-23  1.56   0       \n",
       "2018-08-22  1.43   0       \n",
       "2018-08-21  1.36   0       \n",
       "2018-08-20  1.58   0       \n",
       "2018-08-19  1.51   0       \n",
       "2018-08-18  1.41   0       \n",
       "2018-08-17  1.42   0       \n",
       "2018-08-16  1.55   0       \n",
       "2018-08-15  1.51   0       \n",
       "2018-08-14  1.61   0       \n",
       "2018-08-13  1.50   0       \n",
       "2018-08-12  1.48   0       \n",
       "2018-08-11  1.57   0       \n",
       "2018-08-10  1.41   0       \n",
       "2018-08-09  1.38   0       \n",
       "2018-08-08  1.44   0       \n",
       "2018-08-07  1.41   0       \n",
       "2018-08-06  1.30   0       \n",
       "2018-08-05  1.39   0       \n",
       "2018-08-04  1.30   0       \n",
       "2018-08-03  1.14   0       \n",
       "2018-08-02  1.26   0       \n",
       "2018-08-01  1.40   0       \n",
       "2018-07-31  1.57   0       \n",
       "2018-07-30  1.66   0       \n",
       "2018-07-29  1.65   0       \n",
       "2018-07-28  1.75   0       \n",
       "2018-07-27  1.75   0       \n",
       "2018-07-26  1.84   0       \n",
       "2018-07-25  1.71   0       \n",
       "2018-07-24  1.87   0       \n",
       "2018-07-23  1.68   0       \n",
       "2018-07-22  1.53   0       \n",
       "2018-07-21  1.19   0       \n",
       "2018-07-20  1.05   0       \n",
       "2018-07-19  1.06   0       \n",
       "2018-07-18  1.16   0       \n",
       "2018-07-17  1.22   0       \n",
       "2018-07-16  1.23   0       \n",
       "2018-07-15  1.20   0       \n",
       "2018-07-14  0.97   0       \n",
       "2018-07-13  1.19   0       \n",
       "2018-07-12  1.48   0       \n",
       "2018-07-11  1.44   0       \n",
       "2018-07-10  1.52   0       \n",
       "2018-07-09  1.89   0       \n",
       "2018-07-08  1.80   0       \n",
       "2018-07-07  1.79   0       \n",
       "2018-07-06  1.76   0       \n",
       "2018-07-05  1.63   0       \n",
       "2018-07-04  1.70   0       \n",
       "2018-07-03  1.59   0       \n",
       "2018-07-02  1.43   0       \n",
       "2018-07-01  1.42   0       \n",
       "2018-06-30  1.38   0       \n",
       "2018-06-29  1.35   0       \n",
       "2018-06-28  1.33   0       \n",
       "2018-06-27  1.30   0       \n",
       "2018-06-26  1.18   0       \n",
       "2018-06-25  1.46   0       \n",
       "2018-06-24  1.37   0       \n",
       "2018-06-23  1.57   0       \n",
       "2018-06-22  1.65   0       \n",
       "2018-06-21  1.63   0       \n",
       "2018-06-20  1.73   0       \n",
       "2018-06-19  1.71   0       \n",
       "2018-06-18  1.83   0       \n",
       "2018-06-17  1.79   0       \n",
       "2018-06-16  1.76   0       \n",
       "2018-06-15  1.78   0       \n",
       "2018-06-14  1.98   0       \n",
       "2018-06-13  1.82   0       \n",
       "2018-06-12  1.67   0       \n",
       "2018-06-11  1.84   0       \n",
       "2018-06-10  1.78   0       \n",
       "2018-06-09  1.75   0       \n",
       "2018-06-08  1.60   0       \n",
       "2018-06-07  1.57   0       \n",
       "2018-06-06  1.41   0       \n",
       "2018-06-05  1.65   0       \n",
       "2018-06-04  1.88   0       \n",
       "2018-06-03  2.10   0       \n",
       "2018-06-02  2.22   0       \n",
       "2018-06-01  2.21   0       \n",
       "2018-05-31  2.13   0       \n",
       "2018-05-30  2.08   0       \n",
       "2018-05-29  1.93   0       \n",
       "2018-05-28  1.78   0       \n",
       "2018-05-27  1.57   0       \n",
       "2018-05-26  1.58   0       \n",
       "2018-05-25  1.84   0       \n",
       "2018-05-24  1.96   0       \n",
       "2018-05-23  1.90   0       \n",
       "2018-05-22  1.95   0       \n",
       "2018-05-21  1.97   0       \n",
       "2018-05-20  2.05   0       \n",
       "2018-05-19  1.98   0       \n",
       "2018-05-18  2.08   0       \n",
       "2018-05-17  2.07   0       \n",
       "2018-05-16  1.98   0       \n",
       "2018-05-15  1.74   0       \n",
       "2018-05-14  1.80   0       \n",
       "2018-05-13  1.84   0       \n",
       "2018-05-12  1.79   0       \n",
       "2018-05-11  2.02   0       \n",
       "2018-05-10  1.94   0       \n",
       "2018-05-09  1.80   0       \n",
       "2018-05-08  1.86   0       \n",
       "2018-05-07  1.53   0       \n",
       "2018-05-06  1.27   0       \n",
       "2018-05-05  1.44   0       \n",
       "2018-05-04  1.49   0       \n",
       "2018-05-03  1.29   0       \n",
       "2018-05-02  1.53   0       \n",
       "2018-05-01  1.93   0       \n",
       "2018-04-30  1.53   0       \n",
       "2018-04-29  1.46   0       \n",
       "2018-04-28  1.53   0       \n",
       "2018-04-27  1.95   0       \n",
       "2018-04-26  2.10   0       \n",
       "2018-04-25  1.94   0       \n",
       "2018-04-24  1.60   0       \n",
       "2018-04-23  1.81   0       \n",
       "2018-04-22  2.05   0       \n",
       "2018-04-21  2.11   0       \n",
       "2018-04-20  2.24   0       \n",
       "2018-04-19  2.30   0       \n",
       "2018-04-18  2.38   0       \n",
       "2018-04-16  2.38   0       \n",
       "2018-04-15  2.23   0       \n",
       "2018-04-14  2.21   0       \n",
       "2018-04-13  2.37   0       \n",
       "2018-04-12  2.54   0       \n",
       "2018-04-11  2.68   0       \n",
       "2018-04-10  2.69   0       \n",
       "2018-04-09  2.67   0       \n",
       "2018-04-08  2.22   0       \n",
       "2018-04-07  2.31   0       \n",
       "2018-04-06  2.06   0       \n",
       "2018-04-05  2.16   0       \n",
       "2018-04-04  2.38   0       \n",
       "2018-04-03  2.47   0       \n",
       "2018-04-02  2.63   0       \n",
       "2018-04-01  2.73   0       \n",
       "2018-03-31  2.75   0       \n",
       "2018-03-30  2.71   0       \n",
       "2018-03-29  2.83   0       \n",
       "2018-03-28  2.78   0       \n",
       "2018-03-27  2.66   0       \n",
       "2018-03-26  2.67   0       \n",
       "2018-03-25  2.69   0       \n",
       "2018-03-24  2.46   0       \n",
       "2018-03-23  2.53   0       \n",
       "2018-03-22  2.43   0       \n",
       "2018-03-21  2.34   0       \n",
       "2018-03-20  2.24   0       \n",
       "2018-03-19  2.16   0       \n",
       "2018-03-18  2.15   0       \n",
       "2018-03-17  1.95   0       \n",
       "2018-03-16  1.83   0       \n",
       "2018-03-15  1.79   0       \n",
       "2018-03-14  1.78   0       \n",
       "2018-03-13  1.69   0       \n",
       "2018-03-12  1.83   0       \n",
       "2018-03-11  1.75   0       \n",
       "2018-03-10  1.92   0       \n",
       "2018-03-09  2.11   0       \n",
       "2018-03-08  2.04   0       \n",
       "2018-03-07  2.05   0       \n",
       "2018-03-06  1.97   0       \n",
       "2018-03-05  1.79   0       \n",
       "2018-03-04  1.88   0       \n",
       "2018-03-03  1.96   0       \n",
       "2018-03-02  1.82   0       \n",
       "2018-03-01  1.66   0       \n",
       "2018-02-28  1.92   0       \n",
       "2018-02-27  2.00   0       \n",
       "2018-02-26  2.11   0       \n",
       "2018-02-25  2.01   0       \n",
       "2018-02-24  2.06   0       \n",
       "2018-02-23  2.09   0       \n",
       "2018-02-22  2.27   0       \n",
       "2018-02-21  2.44   0       \n",
       "2018-02-20  2.48   0       \n",
       "2018-02-19  2.87   0       \n",
       "2018-02-18  2.82   0       \n",
       "2018-02-17  2.78   0       \n",
       "2018-02-16  2.71   0       \n",
       "2018-02-15  2.68   0       \n",
       "2018-02-14  2.63   0       \n",
       "2018-02-13  2.54   0       \n",
       "2018-02-12  2.28   0       \n",
       "2018-02-11  2.24   0       \n",
       "2018-02-10  2.01   0       \n",
       "2018-02-09  2.02   0       \n",
       "2018-02-08  1.97   0       \n",
       "2018-02-07  1.97   0       \n",
       "2018-02-06  2.22   0       \n",
       "2018-02-05  2.29   0       \n",
       "2018-02-04  2.34   0       \n",
       "2018-02-03  2.17   0       \n",
       "2018-02-02  2.14   0       \n",
       "2018-02-01  2.23   0       \n",
       "2018-01-31  2.02   0       \n",
       "2018-01-30  2.07   0       \n",
       "2018-01-29  2.07   0       \n",
       "2018-01-28  2.20   0       \n",
       "2018-01-27  2.17   0       \n",
       "2018-01-26  2.17   0       \n",
       "2018-01-25  2.04   0       \n",
       "2018-01-24  1.93   0       \n",
       "2018-01-23  2.26   0       \n",
       "2018-01-22  2.30   0       \n",
       "2018-01-21  2.43   0       \n",
       "2018-01-20  2.37   0       \n",
       "2018-01-19  2.16   0       \n",
       "2018-01-18  2.35   0       \n",
       "2018-01-17  2.50   0       \n",
       "2018-01-16  2.69   0       \n",
       "2018-01-15  2.80   0       \n",
       "2018-01-14  2.78   0       \n",
       "2018-01-13  2.34   0       \n",
       "2018-01-12  2.27   0       \n",
       "2018-01-11  2.20   0       \n",
       "2018-01-10  2.11   0       \n",
       "2018-01-09  2.10   0       \n",
       "2018-01-08  2.13   0       \n",
       "2018-01-07  1.83   0       \n",
       "2018-01-06  1.83   0       \n",
       "2018-01-05  1.61   0       \n",
       "2018-01-04  1.58   0       \n",
       "2018-01-03  1.54   0       \n",
       "2018-01-02  1.58   0       \n",
       "2018-01-01  1.36   0       \n",
       "2017-12-31  1.26   0       \n",
       "2017-12-30  1.48   0       \n",
       "2017-12-29  1.63   0       \n",
       "2017-12-28  1.65   0       \n",
       "2017-12-27  1.67   0       \n",
       "2017-12-26  1.79   0       \n",
       "2017-12-25  1.81   0       \n",
       "2017-12-24  1.67   0       \n",
       "2017-12-23  1.88   0       \n",
       "2017-12-22  1.90   0       \n",
       "2017-12-21  1.85   0       \n",
       "2017-12-20  2.00   0       \n",
       "2017-12-19  2.18   0       \n",
       "2017-12-18  1.97   0       \n",
       "2017-12-17  2.07   0       \n",
       "2017-12-16  2.00   0       \n",
       "2017-12-15  1.92   0       \n",
       "2017-12-14  1.86   0       \n",
       "2017-12-13  2.09   0       \n",
       "2017-12-12  2.08   0       \n",
       "2017-12-11  2.08   0       \n",
       "2017-12-10  2.06   0       \n",
       "2017-12-09  2.03   0       \n",
       "2017-12-08  1.90   0       \n",
       "2017-12-07  2.06   0       \n",
       "2017-12-06  1.67   0       \n",
       "2017-12-05  1.49   0       \n",
       "2017-12-04  1.52   0       \n",
       "2017-12-03  1.64   0       \n",
       "2017-12-02  1.91   0       \n",
       "2017-12-01  2.13   0       \n",
       "2017-11-30  2.04   0       \n",
       "2017-11-29  1.95   0       \n",
       "2017-11-28  1.86   0       \n",
       "2017-11-27  1.66   0       \n",
       "2017-11-26  1.54   0       \n",
       "2017-11-25  1.67   0       \n",
       "2017-11-24  1.57   0       \n",
       "2017-11-23  1.35   0       \n",
       "2017-11-22  1.35   0       \n",
       "2017-11-21  1.31   0       \n",
       "2017-11-20  1.34   0       \n",
       "2017-11-19  1.32   0       \n",
       "2017-11-18  1.33   0       \n",
       "2017-11-17  1.45   0       \n",
       "2017-11-16  1.78   0       \n",
       "2017-11-15  2.09   0       \n",
       "2017-11-14  2.27   0       \n",
       "2017-11-13  2.48   0       \n",
       "2017-11-12  2.57   0       \n",
       "2017-11-11  3.12   0       \n",
       "2017-11-10  3.15   0       \n",
       "2017-11-09  3.18   0       \n",
       "2017-11-08  3.31   0       \n",
       "2017-11-07  3.00   0       \n",
       "2017-11-06  2.80   0       \n",
       "2017-11-05  2.72   0       \n",
       "2017-11-04  2.57   0       \n",
       "2017-11-03  2.68   0       \n",
       "2017-11-02  2.46   0       \n",
       "2017-11-01  2.47   0       \n",
       "2017-10-31  2.33   0       \n",
       "2017-10-30  2.36   0       \n",
       "2017-10-29  2.17   0       \n",
       "2017-10-28  1.92   0       \n",
       "2017-10-27  1.81   0       \n",
       "2017-10-26  2.01   0       \n",
       "2017-10-25  2.01   0       \n",
       "2017-10-24  1.71   0       \n",
       "2017-10-23  1.64   0       \n",
       "2017-10-22  1.45   0       \n",
       "2017-10-21  1.60   0       \n",
       "2017-10-20  1.55   0       \n",
       "2017-10-19  1.44   0       \n",
       "2017-10-18  1.57   0       \n",
       "2017-10-17  1.69   0       \n",
       "2017-10-16  1.67   0       \n",
       "2017-10-15  1.59   0       \n",
       "2017-10-14  1.54   0       \n",
       "2017-10-13  1.54   0       \n",
       "2017-10-12  1.39   0       \n",
       "2017-10-11  1.58   0       \n",
       "2017-10-10  1.71   0       \n",
       "2017-10-09  1.77   0       \n",
       "2017-10-08  1.88   0       \n",
       "2017-10-07  1.78   0       \n",
       "2017-10-06  1.74   0       \n",
       "2017-10-05  1.92   0       \n",
       "2017-10-04  1.89   0       \n",
       "2017-10-03  2.01   0       \n",
       "2017-10-02  2.13   0       \n",
       "2017-10-01  1.68   0       \n",
       "2017-09-30  1.34   0       \n",
       "2017-09-29  1.45   0       \n",
       "2017-09-28  1.51   0       \n",
       "2017-09-27  1.80   0       \n",
       "2017-09-26  1.84   0       \n",
       "2017-09-25  2.12   0       \n",
       "2017-09-24  2.32   0       \n",
       "2017-09-23  2.57   0       \n",
       "2017-09-22  2.39   0       \n",
       "2017-09-21  2.18   0       \n",
       "2017-09-20  1.96   0       \n",
       "2017-09-19  1.89   0       \n",
       "2017-09-18  2.05   0       \n",
       "2017-09-17  2.07   0       \n",
       "2017-09-16  2.14   0       \n",
       "2017-09-15  2.23   0       \n",
       "2017-09-14  2.17   0       \n",
       "2017-09-13  2.19   0       \n",
       "2017-09-12  2.34   0       \n",
       "2017-09-11  2.54   0       \n",
       "2017-09-10  2.44   0       \n",
       "2017-09-09  2.29   0       \n",
       "2017-09-08  3.05   0       \n",
       "2017-09-07  3.57   0       \n",
       "2017-09-06  3.59   0       \n",
       "2017-09-05  4.05   0       \n",
       "2017-09-04  4.15   0       \n",
       "2017-09-03  3.72   0       \n",
       "2017-09-02  3.35   0       \n",
       "2017-09-01  3.20   0       \n",
       "2017-08-31  2.81   0       \n",
       "2017-08-30  2.68   0       \n",
       "2017-08-29  2.56   0       \n",
       "2017-08-28  2.79   0       \n",
       "2017-08-27  2.65   0       \n",
       "2017-08-26  2.49   0       \n",
       "2017-08-25  2.63   0       \n",
       "2017-08-24  2.27   0       \n",
       "2017-08-23  2.27   0       \n",
       "2017-08-22  2.40   0       \n",
       "2017-08-21  2.93   0       \n",
       "2017-08-20  3.12   0       \n",
       "2017-08-19  3.18   0       \n",
       "2017-08-18  3.23   0       \n",
       "2017-08-17  3.47   0       \n",
       "2017-08-16  3.64   0       \n",
       "2017-08-15  3.84   0       \n",
       "2017-08-14  3.83   0       \n",
       "2017-08-13  4.18   0       \n",
       "2017-08-12  3.97   0       \n",
       "2017-08-11  3.94   0       \n",
       "2017-08-10  3.66   0       \n",
       "2017-08-09  3.59   0       \n",
       "2017-08-08  3.65   0       \n",
       "2017-08-07  3.49   0       \n",
       "2017-08-06  3.40   0       \n",
       "2017-08-05  3.65   0       \n",
       "2017-08-04  3.76   0       \n",
       "2017-08-03  3.44   0       \n",
       "2017-08-02  3.09   0       \n",
       "2017-08-01  3.35   0       \n",
       "2017-07-31  3.41   0       \n",
       "2017-07-30  3.34   0       \n",
       "2017-07-29  3.66   0       \n",
       "2017-07-28  3.79   0       \n",
       "2017-07-27  3.92   0       \n",
       "2017-07-26  3.89   0       \n",
       "2017-07-25  3.97   0       \n",
       "2017-07-24  4.16   0       \n",
       "2017-07-23  3.88   0       \n",
       "2017-07-22  3.74   0       \n",
       "2017-07-21  3.60   0       \n",
       "2017-07-20  3.45   0       \n",
       "2017-07-19  3.38   0       \n",
       "2017-07-18  3.69   0       \n",
       "2017-07-17  3.59   0       \n",
       "2017-07-16  3.63   0       \n",
       "2017-07-15  3.47   0       \n",
       "2017-07-14  3.10   0       \n",
       "2017-07-13  3.11   0       \n",
       "2017-07-12  3.32   0       \n",
       "2017-07-11  3.42   0       \n",
       "2017-07-10  3.91   0       \n",
       "2017-07-09  4.16   0       \n",
       "2017-07-08  3.92   0       \n",
       "2017-07-07  3.84   0       \n",
       "2017-07-06  3.79   0       \n",
       "2017-07-05  3.54   0       \n",
       "2017-07-04  3.57   0       \n",
       "2017-07-03  3.06   0       \n",
       "2017-07-02  2.86   0       \n",
       "2017-07-01  2.85   0       \n",
       "2017-06-30  2.50   0       \n",
       "2017-06-29  2.14   0       \n",
       "2020-06-28  7.07   1       \n",
       "2020-06-27  7.07   1       \n",
       "2020-06-26  7.05   1       \n",
       "2020-06-25  7.05   1       \n",
       "2020-06-24  7.07   1       \n",
       "2020-06-22  7.07   1       \n",
       "2020-06-21  7.08   1       \n",
       "2020-06-20  7.15   1       \n",
       "2020-06-19  7.17   1       \n",
       "2020-06-18  7.18   1       \n",
       "2020-06-17  7.17   1       \n",
       "2020-06-16  7.29   1       \n",
       "2020-06-15  7.43   1       \n",
       "2020-06-14  7.43   1       \n",
       "2020-06-13  7.40   1       \n",
       "2020-06-10  7.40   1       \n",
       "2020-06-09  7.45   1       \n",
       "2020-06-08  7.45   1       \n",
       "2020-06-07  7.47   1       \n",
       "2020-06-06  7.50   1       \n",
       "2020-06-04  7.50   1       \n",
       "2020-06-03  7.44   1       \n",
       "2020-06-02  7.44   1       \n",
       "2020-06-01  7.66   1       \n",
       "2020-05-31  7.67   1       \n",
       "2020-05-28  7.67   1       \n",
       "2020-05-27  7.68   1       \n",
       "2020-05-26  7.70   1       \n",
       "2020-05-25  7.67   1       \n",
       "2020-05-24  7.68   1       \n",
       "2020-05-23  7.67   1       \n",
       "2020-05-22  6.77   1       \n",
       "2020-05-21  6.77   1       \n",
       "2020-05-20  7.08   1       \n",
       "2020-05-19  7.31   1       \n",
       "2020-05-18  7.31   1       \n",
       "2020-05-17  7.20   1       \n",
       "2020-05-16  7.15   1       \n",
       "2020-05-15  7.14   1       \n",
       "2020-05-14  7.23   1       \n",
       "2020-05-13  7.25   1       \n",
       "2020-05-12  7.13   1       \n",
       "2020-05-11  6.96   1       \n",
       "2020-05-10  6.92   1       \n",
       "2020-05-09  6.69   1       \n",
       "2020-05-08  6.36   1       \n",
       "2020-05-07  6.08   1       \n",
       "2020-05-06  5.97   1       \n",
       "2020-05-05  6.21   1       \n",
       "2020-05-04  6.27   1       \n",
       "2020-05-03  6.28   1       \n",
       "2020-05-02  6.13   1       \n",
       "2020-05-01  5.84   1       \n",
       "2020-04-30  5.89   1       \n",
       "2020-04-29  5.83   1       \n",
       "2020-04-28  5.73   1       \n",
       "2020-04-27  5.80   1       \n",
       "2020-04-26  6.33   1       \n",
       "2020-04-25  5.99   1       \n",
       "2020-04-24  5.86   1       \n",
       "2020-04-23  5.71   1       \n",
       "2020-04-22  5.58   1       \n",
       "2020-04-21  5.56   1       \n",
       "2020-04-20  5.53   1       \n",
       "2020-04-19  5.64   1       \n",
       "2020-04-18  5.85   1       \n",
       "2020-04-17  5.82   1       \n",
       "2020-04-16  5.81   1       \n",
       "2020-04-15  5.81   1       \n",
       "2020-04-14  5.77   1       \n",
       "2020-04-13  5.75   1       \n",
       "2020-04-12  5.94   1       \n",
       "2020-04-11  6.16   1       \n",
       "2020-04-10  6.33   1       \n",
       "2020-04-09  6.32   1       \n",
       "2020-04-08  6.34   1       \n",
       "2020-04-07  6.61   1       \n",
       "2020-04-06  6.60   1       \n",
       "2020-04-05  6.87   1       \n",
       "2020-04-04  7.05   1       \n",
       "2020-03-31  7.05   1       \n",
       "2020-03-30  7.06   1       \n",
       "2020-03-29  7.19   1       \n",
       "2020-03-28  7.60   1       \n",
       "2020-03-27  7.63   1       \n",
       "2020-03-26  7.63   1       \n",
       "2020-03-25  7.67   1       \n",
       "2020-03-24  7.97   1       \n",
       "2020-03-23  7.87   1       \n",
       "2020-03-22  7.79   1       \n",
       "2020-03-21  7.89   1       \n",
       "2020-03-17  7.89   1       \n",
       "2020-03-16  7.91   1       \n",
       "2020-03-15  8.29   1       \n",
       "2020-03-14  8.64   1       \n",
       "2020-03-13  9.09   1       \n",
       "2020-03-12  9.54   1       \n",
       "2020-03-11  9.66   1       \n",
       "2020-03-10  9.66   1       \n",
       "2020-03-09  9.68   1       \n",
       "2020-03-08  9.79   1       \n",
       "2020-03-05  9.79   1       \n",
       "2020-03-04  9.84   1       \n",
       "2020-03-03  9.96   1       \n",
       "2020-03-02  10.68  1       \n",
       "2020-03-01  10.71  1       \n",
       "2020-02-29  10.92  1       \n",
       "2020-02-28  11.02  1       \n",
       "2020-02-27  11.05  1       \n",
       "2020-02-25  11.05  1       \n",
       "2020-02-24  11.04  1       \n",
       "2020-02-23  11.35  1       \n",
       "2020-02-22  11.42  1       \n",
       "2020-02-20  11.42  1       \n",
       "2020-02-19  11.13  1       \n",
       "2020-02-18  11.05  1       \n",
       "2020-02-17  11.01  1       \n",
       "2020-02-16  10.74  1       \n",
       "2020-02-15  10.72  1       \n",
       "2020-02-14  10.73  1       \n",
       "2020-02-13  10.71  1       \n",
       "2020-02-12  10.73  1       \n",
       "2020-02-11  10.72  1       \n",
       "2020-02-10  10.72  1       \n",
       "2020-02-09  10.06  1       \n",
       "2020-02-08  9.93   1       \n",
       "2020-02-07  9.94   1       \n",
       "2020-02-06  9.93   1       \n",
       "2020-02-05  9.90   1       \n",
       "2020-02-04  10.04  1       \n",
       "2020-02-03  11.71  1       \n",
       "2020-02-02  11.74  1       \n",
       "2020-02-01  11.84  1       \n",
       "2020-01-31  11.88  1       \n",
       "2020-01-30  11.78  1       \n",
       "2020-01-29  11.82  1       \n",
       "2020-01-28  11.95  1       \n",
       "2020-01-27  12.54  1       \n",
       "2020-01-26  12.53  1       \n",
       "2020-01-25  12.56  1       \n",
       "2020-01-24  12.06  1       \n",
       "2020-01-23  11.86  1       \n",
       "2020-01-22  12.22  1       \n",
       "2020-01-21  11.98  1       \n",
       "2020-01-20  12.29  1       \n",
       "2020-01-19  12.66  1       \n",
       "2020-01-18  12.66  1       \n",
       "2020-01-17  12.67  1       \n",
       "2020-01-16  11.74  1       \n",
       "2020-01-15  11.98  1       \n",
       "2020-01-14  12.09  1       \n",
       "2020-01-13  11.97  1       \n",
       "2020-01-12  11.87  1       \n",
       "2020-01-11  11.92  1       \n",
       "2020-01-10  11.94  1       \n",
       "2020-01-09  12.07  1       \n",
       "2020-01-08  11.96  1       \n",
       "2020-01-07  12.29  1       \n",
       "2020-01-06  12.72  1       \n",
       "2020-01-05  13.23  1       \n",
       "2020-01-04  13.11  1       \n",
       "2020-01-03  13.10  1       \n",
       "2020-01-02  13.10  1       \n",
       "2020-01-01  13.12  1       \n",
       "2019-12-31  13.13  1       \n",
       "2019-12-30  13.14  1       \n",
       "2019-12-29  13.22  1       \n",
       "2019-12-28  13.29  1       \n",
       "2019-12-27  13.60  1       \n",
       "2019-12-26  13.25  1       \n",
       "2019-12-25  13.09  1       \n",
       "2019-12-24  12.59  1       \n",
       "2019-12-23  12.44  1       \n",
       "2019-12-22  11.67  1       \n",
       "2019-12-21  11.48  1       \n",
       "2019-12-20  11.47  1       \n",
       "2019-12-19  11.52  1       \n",
       "2019-12-18  11.54  1       \n",
       "2019-12-17  11.49  1       \n",
       "2019-12-16  11.27  1       \n",
       "2019-12-15  10.80  1       \n",
       "2019-12-14  10.88  1       \n",
       "2019-12-13  10.96  1       \n",
       "2019-12-12  10.86  1       \n",
       "2019-12-11  10.85  1       \n",
       "2019-12-10  11.25  1       \n",
       "2019-12-09  11.68  1       \n",
       "2019-12-08  11.89  1       \n",
       "2019-12-07  11.93  1       \n",
       "2019-12-06  11.93  1       \n",
       "2019-12-05  12.35  1       \n",
       "2019-12-04  12.12  1       \n",
       "2019-12-03  12.19  1       \n",
       "2019-12-02  12.20  1       \n",
       "2019-12-01  12.17  1       \n",
       "2019-11-30  12.17  1       \n",
       "2019-11-29  12.50  1       \n",
       "2019-11-28  12.76  1       \n",
       "2019-11-27  12.87  1       \n",
       "2019-11-26  12.94  1       \n",
       "2019-11-25  13.12  1       \n",
       "2019-11-24  12.52  1       \n",
       "2019-11-23  12.66  1       \n",
       "2019-11-22  12.80  1       \n",
       "2019-11-21  12.16  1       \n",
       "2019-11-20  12.01  1       \n",
       "2019-11-19  12.06  1       \n",
       "2019-11-18  11.94  1       \n",
       "2019-11-17  11.93  1       \n",
       "2019-11-16  12.04  1       \n",
       "2019-11-15  13.36  1       \n",
       "2019-11-14  13.50  1       \n",
       "2019-11-13  12.86  1       \n",
       "2019-11-12  12.92  1       \n",
       "2019-11-11  13.09  1       \n",
       "2019-11-10  11.57  1       \n",
       "2019-11-09  10.34  1       \n",
       "2019-11-08  10.07  1       \n",
       "2019-11-07  10.02  1       \n",
       "2019-11-06  9.60   1       \n",
       "2019-11-05  9.01   1       \n",
       "2019-11-04  8.91   1       \n",
       "2019-11-03  8.84   1       \n",
       "2019-11-02  8.92   1       \n",
       "2019-11-01  9.90   1       \n",
       "2019-10-31  9.51   1       \n",
       "2019-10-30  7.97   1       \n",
       "2019-10-29  7.61   1       \n",
       "2019-10-28  8.22   1       \n",
       "2019-10-27  8.17   1       \n",
       "2019-10-26  7.97   1       \n",
       "2019-10-25  6.93   1       \n",
       "2019-10-24  4.95   1       \n",
       "2019-10-23  3.95   1       \n",
       "2019-10-22  3.23   1       \n",
       "2019-10-21  3.13   1       \n",
       "2019-10-20  3.12   1       \n",
       "2019-10-19  2.87   1       \n",
       "2019-10-18  2.83   1       \n",
       "2019-10-17  2.80   1       \n",
       "2019-10-16  2.69   1       \n",
       "2019-10-15  2.67   1       \n",
       "2019-10-14  2.66   1       \n",
       "2019-10-13  2.62   1       \n",
       "2019-10-12  2.65   1       \n",
       "2019-10-11  2.66   1       \n",
       "2019-10-10  2.65   1       \n",
       "2019-10-09  2.66   1       \n",
       "2019-10-08  2.67   1       \n",
       "2019-10-07  2.65   1       \n",
       "2019-10-06  2.66   1       \n",
       "2020-01-02  5.02   0       \n",
       "2020-01-01  5.77   0       \n",
       "2019-12-31  5.74   0       \n",
       "2019-12-30  5.94   0       \n",
       "2019-12-29  6.36   0       \n",
       "2019-12-28  7.53   0       \n",
       "2019-12-27  7.83   0       \n",
       "2019-12-26  7.78   0       \n",
       "2019-12-25  7.79   0       \n",
       "2019-12-24  8.72   0       \n",
       "2019-12-23  10.08  0       \n",
       "2019-12-22  11.18  0       \n",
       "2019-12-21  10.14  0       \n",
       "2019-12-20  9.00   0       \n",
       "2019-12-19  8.40   0       \n",
       "2019-12-18  9.17   0       \n",
       "2019-12-17  9.43   0       \n",
       "2019-12-16  7.96   0       \n",
       "2019-12-15  7.93   0       \n",
       "2019-12-14  8.06   0       \n",
       "2019-12-13  8.48   0       \n",
       "2019-12-12  8.24   0       \n",
       "2019-12-11  8.26   0       \n",
       "2019-12-10  7.47   0       \n",
       "2019-12-09  8.06   0       \n",
       "2019-12-08  8.62   0       \n",
       "2019-12-07  8.25   0       \n",
       "2019-12-06  8.55   0       \n",
       "2019-12-05  8.24   0       \n",
       "2019-12-04  8.45   0       \n",
       "2019-12-03  7.46   0       \n",
       "2019-12-02  6.48   0       \n",
       "2019-12-01  6.63   0       \n",
       "2020-06-28  3.67   0       \n",
       "2020-06-27  3.72   0       \n",
       "2020-06-26  3.62   0       \n",
       "2020-06-25  3.59   0       \n",
       "2020-06-24  3.40   0       \n",
       "2020-06-23  3.37   0       \n",
       "2020-06-22  3.19   0       \n",
       "2020-06-21  3.15   0       \n",
       "2020-06-20  3.82   0       \n",
       "2020-06-19  3.78   0       \n",
       "2020-06-18  4.08   0       \n",
       "2020-06-17  4.69   0       \n",
       "2020-06-16  4.84   0       \n",
       "2020-06-15  4.72   0       \n",
       "2020-06-14  4.66   0       \n",
       "2020-06-13  4.49   0       \n",
       "2020-06-12  5.08   0       \n",
       "2020-06-11  4.59   0       \n",
       "2020-06-10  4.13   0       \n",
       "2020-06-09  4.09   0       \n",
       "2020-06-08  4.27   0       \n",
       "2020-06-07  4.43   0       \n",
       "2020-06-06  4.39   0       \n",
       "2020-06-05  4.55   0       \n",
       "2020-06-04  4.24   0       \n",
       "2020-06-03  3.82   0       \n",
       "2020-06-02  3.84   0       \n",
       "2020-06-01  3.99   0       \n",
       "2020-05-31  3.87   0       \n",
       "2020-05-30  4.25   0       \n",
       "2020-05-29  4.47   0       \n",
       "2020-05-28  4.47   0       \n",
       "2020-05-27  4.86   0       \n",
       "2020-05-26  5.32   0       \n",
       "2020-05-25  5.17   0       \n",
       "2020-05-24  4.99   0       \n",
       "2020-05-23  5.56   0       \n",
       "2020-05-22  4.73   0       \n",
       "2020-05-21  5.39   0       \n",
       "2020-05-20  5.60   0       \n",
       "2020-05-19  5.52   0       \n",
       "2020-05-18  5.40   0       \n",
       "2020-05-17  5.55   0       \n",
       "2020-05-16  5.95   0       \n",
       "2020-05-15  5.69   0       \n",
       "2020-05-14  5.35   0       \n",
       "2020-05-13  4.87   0       \n",
       "2020-05-12  4.41   0       \n",
       "2020-05-11  4.30   0       \n",
       "2020-05-10  5.92   0       \n",
       "2020-05-09  6.28   0       \n",
       "2020-05-08  5.94   0       \n",
       "2020-05-07  5.08   0       \n",
       "2020-05-06  5.37   0       \n",
       "2020-05-05  5.71   0       \n",
       "2020-05-04  5.40   0       \n",
       "2020-05-03  5.18   0       \n",
       "2019-09-03  2.98   1       \n",
       "2019-09-02  2.98   1       \n",
       "2019-09-01  2.95   1       \n",
       "2019-08-31  2.93   1       \n",
       "2019-08-30  2.84   1       \n",
       "2019-08-29  2.62   1       \n",
       "2019-08-28  2.61   1       \n",
       "2019-08-27  2.61   1       \n",
       "2019-08-26  2.63   1       \n",
       "2019-08-25  2.64   1       \n",
       "2019-08-23  2.64   1       \n",
       "2019-08-22  2.77   1       \n",
       "2019-08-21  2.79   1       \n",
       "2019-08-19  2.79   1       \n",
       "2019-08-18  2.78   1       \n",
       "2019-08-17  2.70   1       \n",
       "2019-08-16  2.57   1       \n",
       "2019-08-15  2.65   1       \n",
       "2019-08-14  2.69   1       \n",
       "2019-08-13  2.69   1       \n",
       "2019-08-12  2.71   1       \n",
       "2019-08-11  2.71   1       \n",
       "2019-08-10  2.72   1       \n",
       "2019-08-09  2.70   1       \n",
       "2019-08-08  2.67   1       \n",
       "2019-08-07  2.71   1       \n",
       "2019-08-06  2.73   1       \n",
       "2019-08-05  2.79   1       \n",
       "2019-08-04  2.80   1       \n",
       "2019-08-01  2.80   1       \n",
       "2019-07-31  3.20   1       \n",
       "2019-07-30  3.48   1       \n",
       "2019-07-29  3.52   1       \n",
       "2019-07-28  3.52   1       \n",
       "2019-07-27  3.54   1       \n",
       "2019-07-26  3.54   1       \n",
       "2019-07-25  3.55   1       \n",
       "2019-07-24  3.54   1       \n",
       "2019-07-23  3.56   1       \n",
       "2019-07-22  3.51   1       \n",
       "2019-07-21  3.25   1       \n",
       "2019-07-20  3.24   1       \n",
       "2019-07-19  3.24   1       \n",
       "2019-07-18  3.26   1       \n",
       "2019-07-17  3.24   1       \n",
       "2019-07-16  3.31   1       \n",
       "2019-07-15  3.62   1       \n",
       "2019-07-14  3.61   1       \n",
       "2019-07-13  3.63   1       \n",
       "2019-07-11  3.63   1       \n",
       "2019-07-10  3.62   1       \n",
       "2019-07-09  3.63   1       \n",
       "2019-07-08  3.64   1       \n",
       "2019-07-07  3.67   1       \n",
       "2019-07-06  3.68   1       \n",
       "2019-07-05  3.71   1       \n",
       "2019-07-04  3.70   1       \n",
       "2019-07-03  3.70   1       \n",
       "2019-07-02  3.68   1       \n",
       "2019-07-01  4.05   1       \n",
       "2019-06-30  3.95   1       \n",
       "2019-06-29  3.69   1       \n",
       "2019-06-28  3.70   1       \n",
       "2019-06-27  3.56   1       \n",
       "2019-06-26  3.50   1       \n",
       "2019-06-25  3.48   1       \n",
       "2019-06-24  3.50   1       \n",
       "2019-06-23  3.76   1       \n",
       "2019-06-22  3.69   1       \n",
       "2019-06-21  3.36   1       \n",
       "2019-06-20  3.51   1       \n",
       "2019-06-19  3.57   1       \n",
       "2019-06-18  3.47   1       \n",
       "2019-06-17  3.34   1       \n",
       "2019-06-16  3.31   1       \n",
       "2019-06-15  3.12   1       \n",
       "2019-06-14  3.06   1       \n",
       "2019-06-13  3.01   1       \n",
       "2019-06-12  2.74   1       \n",
       "2019-06-11  2.67   1       \n",
       "2019-06-10  2.63   1       \n",
       "2019-06-09  2.60   1       \n",
       "2019-06-08  2.77   1       \n",
       "2019-06-07  2.76   1       \n",
       "2019-06-06  2.77   1       \n",
       "2019-06-05  2.77   1       \n",
       "2019-06-04  2.74   1       \n",
       "2019-06-03  2.69   1       \n",
       "2019-06-02  2.71   1       \n",
       "2019-06-01  2.66   1       \n",
       "2019-05-31  2.78   1       \n",
       "2019-05-30  2.76   1       \n",
       "2019-05-29  2.58   1       \n",
       "2019-05-28  2.67   1       \n",
       "2019-05-27  2.77   1       \n",
       "2019-05-26  2.68   1       \n",
       "2019-05-25  2.70   1       \n",
       "2019-05-24  2.70   1       \n",
       "2019-05-23  2.72   1       \n",
       "2019-05-22  2.76   1       \n",
       "2019-05-21  2.80   1       \n",
       "2019-05-20  2.81   1       \n",
       "2019-05-19  2.83   1       \n",
       "2019-05-18  2.86   1       \n",
       "2019-05-17  3.00   1       \n",
       "2019-05-16  3.00   1       \n",
       "2019-05-15  2.86   1       \n",
       "2019-05-14  2.79   1       \n",
       "2019-05-13  2.81   1       \n",
       "2019-05-12  2.65   1       \n",
       "2019-05-11  2.58   1       \n",
       "2019-05-10  2.49   1       \n",
       "2019-05-09  2.43   1       \n",
       "2019-05-08  2.40   1       \n",
       "2019-05-07  2.32   1       \n",
       "2019-05-06  2.23   1       \n",
       "2019-05-05  2.05   1       \n",
       "2019-05-04  2.02   1       \n",
       "2019-05-03  2.01   1       \n",
       "2019-05-02  2.01   1       \n",
       "2019-05-01  1.98   1       \n",
       "2019-04-30  1.99   1       \n",
       "2019-04-29  1.99   1       \n",
       "2019-04-28  2.04   1       \n",
       "2019-04-27  2.07   1       \n",
       "2019-04-26  2.04   1       \n",
       "2019-04-25  2.02   1       \n",
       "2019-04-24  2.02   1       \n",
       "2019-04-23  2.01   1       \n",
       "2019-04-22  2.02   1       \n",
       "2019-04-21  2.07   1       \n",
       "2019-04-20  1.96   1       \n",
       "2019-04-19  1.91   1       \n",
       "2019-04-18  1.90   1       \n",
       "2019-04-17  1.88   1       \n",
       "2019-04-16  1.71   1       \n",
       "2019-04-15  1.59   1       \n",
       "2019-04-14  1.53   1       \n",
       "2019-04-13  1.53   1       \n",
       "2019-04-12  1.45   1       \n",
       "2019-04-11  1.44   1       \n",
       "2019-04-10  1.62   1       \n",
       "2019-04-09  1.58   1       \n",
       "2019-04-08  1.40   1       \n",
       "2019-04-07  1.37   1       \n",
       "2019-04-06  1.37   1       \n",
       "2019-04-05  1.35   1       \n",
       "2019-04-04  1.35   1       \n",
       "2019-04-03  1.32   1       \n",
       "2019-04-02  1.30   1       \n",
       "2019-04-01  1.29   1       \n",
       "2019-03-31  1.28   1       \n",
       "2019-03-30  1.34   1       \n",
       "2019-03-29  1.52   1       \n",
       "2019-03-28  1.59   1       \n",
       "2019-03-27  1.52   1       \n",
       "2019-03-26  1.35   1       \n",
       "2019-03-25  1.27   1       \n",
       "2019-03-24  1.41   1       \n",
       "2019-03-23  1.39   1       \n",
       "2019-03-22  1.32   1       \n",
       "2019-03-21  1.39   1       \n",
       "2019-03-20  1.33   1       \n",
       "2019-03-19  1.11   1       \n",
       "2019-03-18  1.11   1       \n",
       "2019-03-17  1.10   1       \n",
       "2019-03-16  1.09   1       \n",
       "2019-03-15  1.08   1       \n",
       "2019-03-14  1.08   1       \n",
       "2019-03-13  1.10   1       \n",
       "2019-03-12  1.11   1       \n",
       "2019-03-11  1.01   1       \n",
       "2019-03-10  1.12   1       \n",
       "2019-03-09  1.31   1       \n",
       "2019-03-08  1.28   1       \n",
       "2019-03-07  1.17   1       \n",
       "2019-03-06  1.07   1       \n",
       "2019-03-05  0.98   1       \n",
       "2019-03-04  0.92   1       \n",
       "2019-03-03  0.93   1       \n",
       "2019-03-02  0.96   1       \n",
       "2019-03-01  0.91   1       \n",
       "2019-02-28  0.89   1       \n",
       "2019-02-27  0.87   1       \n",
       "2019-02-26  0.84   1       \n",
       "2019-02-25  0.83   1       \n",
       "2019-02-24  0.79   1       \n",
       "2019-02-23  0.80   1       \n",
       "2019-02-22  0.78   1       \n",
       "2019-02-21  0.70   1       \n",
       "2019-02-20  0.72   1       \n",
       "2019-02-19  0.74   1       \n",
       "2019-02-18  0.74   1       \n",
       "2019-02-17  0.72   1       \n",
       "2019-02-16  0.69   1       \n",
       "2019-02-15  0.74   1       \n",
       "2019-02-14  0.78   1       \n",
       "2019-02-13  0.77   1       \n",
       "2019-02-12  0.69   1       \n",
       "2019-02-11  0.68   1       \n",
       "2019-02-09  0.68   1       \n",
       "2019-02-08  0.66   1       \n",
       "2019-02-07  0.67   1       \n",
       "2019-02-05  0.67   1       \n",
       "2019-02-04  0.68   1       \n",
       "2019-02-03  0.70   1       \n",
       "2019-02-02  0.69   1       \n",
       "2019-02-01  0.81   1       \n",
       "2019-01-31  0.87   1       \n",
       "2019-01-30  0.92   1       \n",
       "2019-01-29  0.96   1       \n",
       "2019-01-28  1.03   1       \n",
       "2019-01-27  1.03   1       \n",
       "2019-01-26  1.01   1       \n",
       "2019-01-25  0.85   1       \n",
       "2019-01-24  0.83   1       \n",
       "2019-01-23  0.89   1       \n",
       "2019-01-22  0.93   1       \n",
       "2019-01-21  0.93   1       \n",
       "2019-01-20  0.92   1       \n",
       "2019-01-19  0.91   1       \n",
       "2019-01-18  0.91   1       \n",
       "2019-01-17  0.99   1       \n",
       "2019-01-16  1.17   1       \n",
       "2019-01-15  1.18   1       \n",
       "2019-01-14  1.18   1       \n",
       "2019-01-13  1.15   1       \n",
       "2019-01-12  1.20   1       \n",
       "2019-01-11  1.24   1       \n",
       "2019-01-10  1.24   1       \n",
       "2019-01-09  1.25   1       \n",
       "2019-01-06  1.25   1       \n",
       "2019-01-05  1.26   1       \n",
       "2019-01-04  1.26   1       \n",
       "2019-01-03  1.25   1       \n",
       "2019-01-02  1.25   1       \n",
       "2019-01-01  1.06   1       \n",
       "2018-12-31  1.02   1       \n",
       "2018-12-30  1.00   1       \n",
       "2018-12-28  1.00   1       \n",
       "2018-12-27  1.01   1       \n",
       "2018-12-26  1.02   1       \n",
       "2018-12-25  1.01   1       \n",
       "2018-12-23  1.01   1       \n",
       "2018-12-22  1.02   1       \n",
       "2018-12-21  1.03   1       \n",
       "2018-12-20  1.03   1       \n",
       "2018-12-19  1.01   1       \n",
       "2018-12-18  1.00   1       \n",
       "2018-12-16  1.00   1       \n",
       "2018-12-15  0.99   1       \n",
       "2018-12-14  0.99   1       \n",
       "2018-12-13  1.01   1       \n",
       "2018-12-12  1.13   1       \n",
       "2018-12-10  1.13   1       \n",
       "2018-12-09  1.15   1       \n",
       "2018-12-08  1.15   1       \n",
       "2018-12-07  1.20   1       \n",
       "2018-12-06  1.22   1       \n",
       "2018-12-05  1.19   1       \n",
       "2018-12-04  1.19   1       \n",
       "2018-12-03  1.20   1       \n",
       "2018-12-02  1.20   1       \n",
       "2018-12-01  1.18   1       \n",
       "2018-11-30  1.17   1       \n",
       "2018-11-28  1.17   1       \n",
       "2018-11-27  1.15   1       \n",
       "2018-11-25  1.15   1       \n",
       "2018-11-24  1.14   1       \n",
       "2018-11-23  1.12   1       \n",
       "2018-11-22  1.10   1       \n",
       "2018-11-19  1.10   1       \n",
       "2018-11-18  1.03   1       \n",
       "2018-11-17  0.99   1       \n",
       "2018-11-16  0.97   1       \n",
       "2018-11-15  0.97   1       \n",
       "2018-11-14  0.95   1       \n",
       "2018-11-13  0.93   1       \n",
       "2018-11-12  0.88   1       \n",
       "2018-11-11  0.84   1       \n",
       "2018-11-10  0.86   1       \n",
       "2018-11-09  0.94   1       \n",
       "2018-11-08  1.00   1       \n",
       "2018-11-07  0.93   1       \n",
       "2018-11-06  0.93   1       \n",
       "2018-11-05  0.98   1       \n",
       "2018-11-04  0.95   1       \n",
       "2018-11-03  0.93   1       \n",
       "2018-11-02  0.96   1       \n",
       "2018-11-01  0.85   1       \n",
       "2018-10-31  1.03   1       \n",
       "2018-10-30  1.22   1       \n",
       "2018-10-29  1.27   1       \n",
       "2018-10-28  1.40   1       \n",
       "2018-10-27  1.47   1       \n",
       "2018-10-26  1.51   1       \n",
       "2018-10-25  1.54   1       \n",
       "2018-10-24  1.54   1       \n",
       "2018-10-23  1.85   1       \n",
       "2018-10-22  1.94   1       \n",
       "2018-10-21  1.96   1       \n",
       "2018-10-20  2.26   1       \n",
       "2018-10-19  2.39   1       \n",
       "2018-10-18  2.33   1       \n",
       "2018-10-17  2.53   1       \n",
       "2018-10-16  2.75   1       \n",
       "2018-10-15  2.77   1       \n",
       "2018-10-14  2.71   1       \n",
       "2018-10-13  2.71   1       \n",
       "2018-10-12  2.66   1       \n",
       "2018-10-11  2.69   1       \n",
       "2018-10-10  2.76   1       \n",
       "2018-10-09  2.78   1       \n",
       "2018-10-08  2.85   1       \n",
       "2018-10-07  2.85   1       \n",
       "2018-10-06  2.89   1       \n",
       "2018-10-05  3.01   1       \n",
       "2018-10-04  3.01   1       \n",
       "2018-10-03  3.14   1       \n",
       "2018-09-30  3.14   1       \n",
       "2018-09-29  3.30   1       \n",
       "2018-09-28  3.31   1       \n",
       "2018-09-27  3.31   1       \n",
       "2018-09-26  3.32   1       \n",
       "2018-09-25  3.58   1       \n",
       "2018-09-24  3.62   1       \n",
       "2018-09-23  3.51   1       \n",
       "2018-09-22  3.43   1       \n",
       "2018-09-21  3.32   1       \n",
       "2018-09-19  3.32   1       \n",
       "2018-09-18  3.35   1       \n",
       "2018-09-17  3.35   1       \n",
       "2018-09-16  3.33   1       \n",
       "2018-09-15  3.33   1       \n",
       "2018-09-14  3.32   1       \n",
       "2018-09-13  3.29   1       \n",
       "2018-09-12  3.29   1       \n",
       "2018-09-11  3.11   1       \n",
       "2018-09-10  3.09   1       \n",
       "2018-09-09  3.09   1       \n",
       "2018-09-08  3.12   1       \n",
       "2018-09-07  3.12   1       \n",
       "2018-09-06  3.13   1       \n",
       "2018-09-05  3.11   1       \n",
       "2018-09-04  3.09   1       \n",
       "2018-09-03  2.93   1       \n",
       "2018-09-02  2.90   1       \n",
       "2018-09-01  2.88   1       \n",
       "2018-08-31  2.85   1       \n",
       "2018-08-30  2.85   1       \n",
       "2018-08-29  2.84   1       \n",
       "2018-08-28  2.80   1       \n",
       "2018-08-27  2.80   1       \n",
       "2018-08-26  2.79   1       \n",
       "2018-08-25  2.78   1       \n",
       "2018-08-24  2.75   1       \n",
       "2018-08-22  2.75   1       \n",
       "2018-08-21  2.73   1       \n",
       "2018-08-20  2.73   1       \n",
       "2018-08-19  2.75   1       \n",
       "2018-08-18  2.75   1       \n",
       "2018-08-17  2.77   1       \n",
       "2018-08-16  2.90   1       \n",
       "2018-08-15  3.06   1       \n",
       "2018-08-14  3.05   1       \n",
       "2018-08-13  2.91   1       \n",
       "2018-08-12  2.90   1       \n",
       "2018-08-11  2.80   1       \n",
       "2018-08-10  2.75   1       \n",
       "2018-08-09  2.83   1       \n",
       "2018-08-08  2.56   1       \n",
       "2018-08-07  2.55   1       \n",
       "2018-08-06  2.59   1       \n",
       "2018-08-05  2.68   1       \n",
       "2018-08-04  2.67   1       \n",
       "2018-08-03  2.66   1       \n",
       "2018-08-02  2.72   1       \n",
       "2018-08-01  2.69   1       \n",
       "2018-07-31  2.78   1       \n",
       "2018-07-30  2.81   1       \n",
       "2018-07-29  2.80   1       \n",
       "2018-07-28  2.89   1       \n",
       "2018-07-27  2.89   1       \n",
       "2018-07-26  2.90   1       \n",
       "2018-07-25  2.86   1       \n",
       "2018-07-24  2.91   1       \n",
       "2018-07-23  2.90   1       \n",
       "2018-07-22  2.87   1       \n",
       "2018-07-21  2.84   1       \n",
       "2018-07-20  2.84   1       \n",
       "2018-07-19  2.82   1       \n",
       "2018-07-18  2.83   1       \n",
       "2018-07-17  2.71   1       \n",
       "2018-07-16  2.67   1       \n",
       "2018-07-12  2.67   1       \n",
       "2018-07-11  2.70   1       \n",
       "2018-07-10  2.77   1       \n",
       "2018-07-09  2.81   1       \n",
       "2018-07-08  2.79   1       \n",
       "2018-07-07  2.96   1       \n",
       "2018-07-06  2.99   1       \n",
       "2018-07-05  3.00   1       \n",
       "2018-07-04  2.96   1       \n",
       "2018-07-03  2.95   1       \n",
       "2018-07-02  2.94   1       \n",
       "2018-07-01  2.91   1       \n",
       "2018-06-30  2.78   1       \n",
       "2018-06-29  2.68   1       \n",
       "2018-06-27  2.68   1       \n",
       "2018-06-26  2.69   1       \n",
       "2018-06-25  2.69   1       \n",
       "2018-06-24  2.71   1       \n",
       "2018-06-22  2.71   1       \n",
       "2018-06-21  2.69   1       \n",
       "2018-06-20  2.68   1       \n",
       "2018-06-19  2.68   1       \n",
       "2018-06-18  2.69   1       \n",
       "2018-06-17  2.68   1       \n",
       "2018-06-16  2.55   1       \n",
       "2018-06-15  2.52   1       \n",
       "2018-06-14  2.50   1       \n",
       "2018-06-13  2.59   1       \n",
       "2018-06-12  2.69   1       \n",
       "2018-06-11  2.69   1       \n",
       "2018-06-10  2.70   1       \n",
       "2018-06-09  2.66   1       \n",
       "2018-06-08  2.63   1       \n",
       "2018-06-07  2.62   1       \n",
       "2018-06-06  2.64   1       \n",
       "2018-06-05  2.74   1       \n",
       "2018-06-04  2.71   1       \n",
       "2018-06-03  2.66   1       \n",
       "2018-06-02  2.64   1       \n",
       "2018-06-01  2.64   1       \n",
       "2018-05-31  2.57   1       \n",
       "2018-05-30  2.53   1       \n",
       "2018-05-29  2.53   1       \n",
       "2018-05-28  2.51   1       \n",
       "2018-05-27  2.61   1       \n",
       "2018-05-26  2.75   1       \n",
       "2018-05-25  2.81   1       \n",
       "2018-05-24  2.78   1       \n",
       "2018-05-23  2.56   1       \n",
       "2018-05-22  2.36   1       \n",
       "2018-05-21  2.31   1       \n",
       "2018-05-20  2.29   1       \n",
       "2018-05-19  2.28   1       \n",
       "2018-05-18  2.26   1       \n",
       "2018-05-17  2.47   1       \n",
       "2018-05-16  2.50   1       \n",
       "2018-05-15  2.34   1       \n",
       "2018-05-14  2.31   1       \n",
       "2018-05-13  2.29   1       \n",
       "2018-05-12  2.32   1       \n",
       "2018-05-11  2.37   1       \n",
       "2018-05-10  2.57   1       \n",
       "2018-05-09  2.74   1       \n",
       "2018-05-08  2.74   1       \n",
       "2018-05-07  3.01   1       \n",
       "2018-05-06  3.20   1       \n",
       "2018-05-05  3.40   1       \n",
       "2018-05-04  3.47   1       \n",
       "2018-05-03  3.44   1       \n",
       "2018-05-02  3.26   1       \n",
       "2018-05-01  3.24   1       \n",
       "2018-04-30  3.28   1       \n",
       "2018-04-29  3.50   1       \n",
       "2018-04-28  3.53   1       \n",
       "2018-04-26  3.53   1       \n",
       "2018-04-25  3.49   1       \n",
       "2018-04-24  3.42   1       \n",
       "2018-04-23  3.41   1       \n",
       "2018-04-22  3.37   1       \n",
       "2018-04-21  3.34   1       \n",
       "2018-04-20  3.33   1       \n",
       "2018-04-19  3.26   1       \n",
       "2018-04-18  3.32   1       \n",
       "2018-04-17  3.44   1       \n",
       "2018-04-16  3.46   1       \n",
       "2018-04-15  3.48   1       \n",
       "2018-04-14  3.72   1       \n",
       "2018-04-13  3.85   1       \n",
       "2018-04-12  3.88   1       \n",
       "2018-04-11  3.95   1       \n",
       "2018-04-10  3.78   1       \n",
       "2018-04-09  3.68   1       \n",
       "2018-04-08  3.54   1       \n",
       "2018-04-07  3.47   1       \n",
       "2018-04-06  3.60   1       \n",
       "2018-04-05  3.71   1       \n",
       "2018-04-04  3.48   1       \n",
       "2018-04-03  3.49   1       \n",
       "2018-04-02  3.23   1       \n",
       "2018-04-01  3.08   1       \n",
       "2018-03-31  2.89   1       \n",
       "2018-03-30  2.83   1       \n",
       "2018-03-29  2.90   1       \n",
       "2018-03-28  2.89   1       \n",
       "2018-03-27  2.92   1       \n",
       "2018-03-26  2.72   1       \n",
       "2018-03-25  2.71   1       \n",
       "2018-03-24  2.66   1       \n",
       "2018-03-23  2.67   1       \n",
       "2018-03-22  2.66   1       \n",
       "2018-03-21  2.56   1       \n",
       "2018-03-20  2.40   1       \n",
       "2018-03-19  2.42   1       \n",
       "2018-03-18  2.33   1       \n",
       "2018-03-17  2.22   1       \n",
       "2018-03-16  2.31   1       \n",
       "2018-03-15  2.27   1       \n",
       "2018-03-14  2.26   1       \n",
       "2018-03-13  2.31   1       \n",
       "2018-03-12  2.26   1       \n",
       "2018-03-11  2.24   1       \n",
       "2018-03-10  2.23   1       \n",
       "2018-03-09  2.24   1       \n",
       "2018-03-08  2.32   1       \n",
       "2018-03-07  2.33   1       \n",
       "2018-03-06  2.34   1       \n",
       "2018-03-05  2.36   1       \n",
       "2018-03-04  2.26   1       \n",
       "2018-03-03  2.22   1       \n",
       "2018-03-02  2.21   1       \n",
       "2018-03-01  2.23   1       \n",
       "2018-02-28  2.25   1       \n",
       "2018-02-27  2.23   1       \n",
       "2018-02-26  2.33   1       \n",
       "2018-02-25  2.38   1       \n",
       "2018-02-24  2.41   1       \n",
       "2018-02-23  2.48   1       \n",
       "2018-02-22  2.49   1       \n",
       "2018-02-21  2.51   1       \n",
       "2018-02-20  2.55   1       \n",
       "2018-02-19  2.73   1       \n",
       "2018-02-18  2.86   1       \n",
       "2018-02-17  2.82   1       \n",
       "2018-02-16  2.95   1       \n",
       "2018-02-15  2.91   1       \n",
       "2018-02-14  2.69   1       \n",
       "2018-02-13  2.85   1       \n",
       "2018-02-12  2.73   1       \n",
       "2018-02-11  2.34   1       \n",
       "2018-02-10  2.26   1       \n",
       "2018-02-09  2.26   1       \n",
       "2018-02-08  2.27   1       \n",
       "2018-02-07  2.30   1       \n",
       "2018-02-06  2.51   1       \n",
       "2018-02-05  2.69   1       \n",
       "2018-02-04  2.24   1       \n",
       "2018-02-03  2.30   1       \n",
       "2018-02-02  2.38   1       \n",
       "2018-02-01  2.47   1       \n",
       "2018-01-31  2.45   1       \n",
       "2018-01-30  2.48   1       \n",
       "2018-01-29  2.48   1       \n",
       "2018-01-28  2.51   1       \n",
       "2018-01-27  2.62   1       \n",
       "2018-01-26  2.65   1       \n",
       "2018-01-25  2.65   1       \n",
       "2018-01-24  2.63   1       \n",
       "2018-01-23  2.53   1       \n",
       "2018-01-22  2.48   1       \n",
       "2018-01-21  2.76   1       \n",
       "2018-01-20  2.89   1       \n",
       "2018-01-19  2.86   1       \n",
       "2018-01-18  2.65   1       \n",
       "2018-01-17  2.45   1       \n",
       "2018-01-16  2.60   1       \n",
       "2018-01-15  2.78   1       \n",
       "2018-01-14  2.61   1       \n",
       "2018-01-13  2.54   1       \n",
       "2018-01-12  2.52   1       \n",
       "2018-01-11  2.53   1       \n",
       "2018-01-10  2.32   1       \n",
       "2018-01-09  2.22   1       \n",
       "2018-01-08  2.01   1       \n",
       "2018-01-07  1.91   1       \n",
       "2018-01-06  2.00   1       \n",
       "2018-01-05  1.77   1       \n",
       "2018-01-04  1.76   1       \n",
       "2018-01-03  1.82   1       \n",
       "2018-01-02  1.73   1       \n",
       "2018-01-01  1.63   1       \n",
       "2017-12-31  1.63   1       \n",
       "2017-12-30  1.84   1       \n",
       "2017-12-29  1.90   1       \n",
       "2017-12-26  1.90   1       \n",
       "2017-12-25  1.91   1       \n",
       "2017-12-24  1.99   1       \n",
       "2017-12-22  1.99   1       \n",
       "2017-12-21  2.07   1       \n",
       "2017-12-20  2.12   1       \n",
       "2017-12-19  2.08   1       \n",
       "2017-12-15  2.08   1       \n",
       "2017-12-14  1.93   1       \n",
       "2017-12-13  1.97   1       \n",
       "2017-12-12  1.92   1       \n",
       "2017-12-11  1.87   1       \n",
       "2017-12-10  1.83   1       \n",
       "2017-12-09  1.90   1       \n",
       "2017-12-08  1.97   1       \n",
       "2017-12-07  1.92   1       \n",
       "2017-12-06  1.74   1       \n",
       "2017-12-05  1.61   1       \n",
       "2017-12-04  1.49   1       \n",
       "2017-12-03  1.41   1       \n",
       "2017-12-02  1.57   1       \n",
       "2017-12-01  1.60   1       \n",
       "2017-11-30  1.54   1       \n",
       "2017-11-29  1.53   1       \n",
       "2017-11-28  1.56   1       \n",
       "2017-11-27  1.58   1       \n",
       "2017-11-26  1.48   1       \n",
       "2017-11-25  1.33   1       \n",
       "2017-11-24  1.35   1       \n",
       "2017-11-23  1.30   1       \n",
       "2017-11-22  1.22   1       \n",
       "2017-11-21  1.21   1       \n",
       "2017-11-20  1.27   1       \n",
       "2017-11-19  1.29   1       \n",
       "2017-11-18  1.63   1       \n",
       "2017-11-17  1.49   1       \n",
       "2020-06-28  6.28   2       \n",
       "2020-06-13  6.28   2       \n",
       "2020-06-12  6.49   2       \n",
       "2020-06-11  6.57   2       \n",
       "2020-06-10  6.29   2       \n",
       "2020-06-09  6.28   2       \n",
       "2020-05-11  6.28   2       \n",
       "2020-05-10  6.32   2       \n",
       "2020-05-09  6.11   2       \n",
       "2020-05-08  5.99   2       \n",
       "2020-05-07  6.03   2       \n",
       "2020-05-06  6.35   2       \n",
       "2020-05-05  6.57   2       \n",
       "2020-04-26  6.57   2       \n",
       "2020-04-25  6.51   2       \n",
       "2020-04-24  6.28   2       \n",
       "2020-04-23  6.28   2       \n",
       "2020-04-22  6.22   2       \n",
       "2020-04-21  5.99   2       \n",
       "2020-03-30  5.99   2       \n",
       "2020-03-29  5.98   2       \n",
       "2020-03-28  4.82   2       \n",
       "2020-03-27  4.29   2       \n",
       "2020-03-26  4.30   2       \n",
       "2020-03-25  4.64   2       \n",
       "2020-03-24  4.77   2       \n",
       "2020-03-23  4.99   2       \n",
       "2020-03-22  6.31   2       \n",
       "2020-03-21  8.01   2       \n",
       "2020-03-20  8.09   2       \n",
       "2020-03-19  8.47   2       \n",
       "2020-03-18  8.56   2       \n",
       "2020-03-17  8.56   2       \n",
       "2020-03-16  8.29   2       \n",
       "2020-03-15  8.35   2       \n",
       "2020-03-14  8.45   2       \n",
       "2020-03-13  8.96   2       \n",
       "2020-03-12  9.47   2       \n",
       "2020-03-11  9.49   2       \n",
       "2020-03-10  9.49   2       \n",
       "2020-03-09  9.60   2       \n",
       "2020-03-08  9.64   2       \n",
       "2020-03-07  9.64   2       \n",
       "2020-03-06  9.70   2       \n",
       "2020-03-05  9.72   2       \n",
       "2020-03-04  9.72   2       \n",
       "2020-03-03  9.94   2       \n",
       "2020-03-02  10.03  2       \n",
       "2020-02-29  10.03  2       \n",
       "2020-02-28  10.04  2       \n",
       "2020-02-27  10.11  2       \n",
       "2020-02-23  10.11  2       \n",
       "2020-02-22  10.15  2       \n",
       "2020-02-21  10.19  2       \n",
       "2020-02-10  10.19  2       \n",
       "2020-02-09  10.21  2       \n",
       "2020-02-08  10.21  2       \n",
       "2020-02-07  10.19  2       \n",
       "2020-02-03  10.19  2       \n",
       "2020-02-02  10.78  2       \n",
       "2020-02-01  10.91  2       \n",
       "2020-01-30  10.91  2       \n",
       "2020-01-29  10.97  2       \n",
       "2020-01-28  10.96  2       \n",
       "2020-01-27  10.88  2       \n",
       "2020-01-26  10.93  2       \n",
       "2020-01-25  11.20  2       \n",
       "2020-01-24  11.23  2       \n",
       "2020-01-23  11.23  2       \n",
       "2020-01-22  11.27  2       \n",
       "2020-01-21  11.45  2       \n",
       "2020-01-20  11.47  2       \n",
       "2020-01-19  11.52  2       \n",
       "2020-01-18  11.22  2       \n",
       "2020-01-17  11.22  2       \n",
       "2020-01-16  11.14  2       \n",
       "2020-01-15  11.22  2       \n",
       "2020-01-14  11.25  2       \n",
       "2020-01-13  11.26  2       \n",
       "2020-01-12  11.15  2       \n",
       "2020-01-11  10.38  2       \n",
       "2020-01-10  10.34  2       \n",
       "2020-01-09  10.40  2       \n",
       "2020-01-08  10.89  2       \n",
       "2020-01-07  10.90  2       \n",
       "2020-01-06  10.91  2       \n",
       "2020-01-05  10.91  2       \n",
       "2020-01-04  10.93  2       \n",
       "2020-01-03  10.93  2       \n",
       "2020-01-02  10.97  2       \n",
       "2020-01-01  10.94  2       \n",
       "2019-12-31  10.94  2       \n",
       "2019-12-30  10.96  2       \n",
       "2019-12-27  10.96  2       \n",
       "2019-12-26  10.95  2       \n",
       "2019-12-25  10.90  2       \n",
       "2019-12-24  11.08  2       \n",
       "2019-12-23  10.80  2       \n",
       "2019-12-22  10.38  2       \n",
       "2019-12-21  10.63  2       \n",
       "2019-12-20  10.69  2       \n",
       "2019-12-19  10.63  2       \n",
       "2019-12-18  10.63  2       \n",
       "2019-12-17  10.60  2       \n",
       "2019-12-16  10.58  2       \n",
       "2019-12-15  10.58  2       \n",
       "2019-12-14  10.68  2       \n",
       "2019-12-13  10.86  2       \n",
       "2019-12-12  11.38  2       \n",
       "2019-12-11  11.38  2       \n",
       "2019-12-10  11.39  2       \n",
       "2019-12-09  11.40  2       \n",
       "2019-12-08  11.40  2       \n",
       "2019-12-07  11.41  2       \n",
       "2019-12-06  11.44  2       \n",
       "2019-12-05  11.44  2       \n",
       "2019-12-04  11.45  2       \n",
       "2019-12-03  11.40  2       \n",
       "2019-12-02  11.38  2       \n",
       "2019-11-27  11.38  2       \n",
       "2019-11-26  11.45  2       \n",
       "2019-11-25  11.38  2       \n",
       "2019-11-24  11.29  2       \n",
       "2019-11-23  11.49  2       \n",
       "2019-11-22  11.72  2       \n",
       "2019-11-21  11.48  2       \n",
       "2019-11-20  11.41  2       \n",
       "2019-11-19  11.48  2       \n",
       "2019-11-18  11.50  2       \n",
       "2019-11-17  11.49  2       \n",
       "2019-11-16  11.88  2       \n",
       "2019-11-15  12.29  2       \n",
       "2019-11-14  12.35  2       \n",
       "2019-11-13  11.81  2       \n",
       "2019-11-12  10.60  2       \n",
       "2019-11-11  10.29  2       \n",
       "2019-11-10  9.77   2       \n",
       "2019-11-09  9.65   2       \n",
       "2019-11-08  9.14   2       \n",
       "2019-11-07  9.18   2       \n",
       "2019-11-06  8.73   2       \n",
       "2019-11-05  8.03   2       \n",
       "2019-11-04  7.96   2       \n",
       "2019-11-03  7.83   2       \n",
       "2019-11-02  7.93   2       \n",
       "2019-11-01  7.82   2       \n",
       "2019-10-31  7.50   2       \n",
       "2019-10-30  6.35   2       \n",
       "2019-10-29  6.17   2       \n",
       "2019-10-28  6.12   2       \n",
       "2019-10-27  5.79   2       \n",
       "2019-10-26  5.75   2       \n",
       "2019-10-25  5.49   2       \n",
       "2019-10-24  4.78   2       \n",
       "2019-10-23  3.37   2       \n",
       "2019-10-22  2.77   2       \n",
       "2019-10-21  2.42   2       \n",
       "2019-10-20  2.37   2       \n",
       "2019-10-16  2.37   2       \n",
       "2019-10-15  2.40   2       \n",
       "2019-10-14  2.44   2       \n",
       "2019-10-13  2.45   2       \n",
       "2019-10-12  2.45   2       \n",
       "2019-10-11  2.42   2       \n",
       "2019-10-10  2.41   2       \n",
       "2019-10-08  2.41   2       \n",
       "2019-10-07  2.43   2       \n",
       "2019-10-06  2.44   2       \n",
       "2019-10-03  2.44   2       \n",
       "2019-10-02  2.60   2       \n",
       "2019-10-01  2.79   2       \n",
       "2019-09-26  2.79   2       \n",
       "2019-09-25  2.99   2       \n",
       "2019-09-24  3.09   2       \n",
       "2019-09-23  3.09   2       \n",
       "2019-09-22  3.02   2       \n",
       "2019-09-21  2.96   2       \n",
       "2019-09-14  2.96   2       \n",
       "2019-09-13  2.97   2       \n",
       "2019-09-12  2.96   2       \n",
       "2019-09-11  2.96   2       \n",
       "2019-09-10  2.97   2       \n",
       "2019-09-09  3.00   2       \n",
       "2019-09-08  2.99   2       \n",
       "2019-09-06  2.99   2       \n",
       "2019-09-05  2.94   2       \n",
       "2019-09-04  2.95   2       \n",
       "2019-09-03  2.95   2       \n",
       "2019-09-02  3.04   2       \n",
       "2019-09-01  3.12   2       \n",
       "2019-08-31  3.12   2       \n",
       "2019-08-30  3.01   2       \n",
       "2019-08-29  2.97   2       \n",
       "2019-08-28  2.96   2       \n",
       "2019-08-27  2.92   2       \n",
       "2019-08-26  2.92   2       \n",
       "2019-08-25  2.91   2       \n",
       "2019-08-23  2.91   2       \n",
       "2019-08-22  2.89   2       \n",
       "2019-08-21  2.88   2       \n",
       "2019-08-19  2.88   2       \n",
       "2019-08-18  2.90   2       \n",
       "2019-08-16  2.90   2       \n",
       "2019-08-15  2.93   2       \n",
       "2019-08-11  2.93   2       \n",
       "2019-08-10  2.89   2       \n",
       "2019-08-09  2.87   2       \n",
       "2019-08-08  2.87   2       \n",
       "2019-08-07  2.88   2       \n",
       "2019-08-06  2.98   2       \n",
       "2019-08-05  3.08   2       \n",
       "2019-08-04  3.09   2       \n",
       "2019-08-03  3.19   2       \n",
       "2019-08-01  3.19   2       \n",
       "2019-07-31  3.20   2       \n",
       "2019-07-30  3.20   2       \n",
       "2019-07-29  3.21   2       \n",
       "2019-07-28  3.21   2       \n",
       "2019-07-27  3.46   2       \n",
       "2019-07-26  3.54   2       \n",
       "2019-07-25  3.49   2       \n",
       "2019-07-21  3.49   2       \n",
       "2019-07-20  3.50   2       \n",
       "2019-07-17  3.50   2       \n",
       "2019-07-16  3.97   2       \n",
       "2019-07-15  4.22   2       \n",
       "2019-07-14  4.19   2       \n",
       "2019-07-12  4.19   2       \n",
       "2019-07-11  4.21   2       \n",
       "2019-07-10  4.19   2       \n",
       "2019-07-09  4.18   2       \n",
       "2019-07-08  4.17   2       \n",
       "2019-07-07  4.14   2       \n",
       "2019-07-06  4.07   2       \n",
       "2019-07-05  4.08   2       \n",
       "2019-07-04  4.03   2       \n",
       "2019-07-03  4.03   2       \n",
       "2019-07-02  3.99   2       \n",
       "2019-07-01  3.99   2       \n",
       "2019-06-30  3.56   2       \n",
       "2019-06-29  3.30   2       \n",
       "2019-06-28  3.30   2       \n",
       "2019-06-27  3.37   2       \n",
       "2019-06-26  3.52   2       \n",
       "2019-06-25  3.55   2       \n",
       "2019-06-24  3.54   2       \n",
       "2019-06-23  3.49   2       \n",
       "2019-06-22  3.33   2       \n",
       "2019-06-21  3.29   2       \n",
       "2019-06-20  3.23   2       \n",
       "2019-06-19  3.07   2       \n",
       "2019-06-18  3.07   2       \n",
       "2019-06-17  3.03   2       \n",
       "2019-06-16  2.82   2       \n",
       "2019-06-15  2.82   2       \n",
       "2019-06-14  2.73   2       \n",
       "2019-06-13  2.67   2       \n",
       "2019-06-12  2.62   2       \n",
       "2019-06-11  2.75   2       \n",
       "2019-06-10  2.79   2       \n",
       "2019-06-09  2.78   2       \n",
       "2019-06-08  2.73   2       \n",
       "2019-06-07  2.72   2       \n",
       "2019-06-06  2.71   2       \n",
       "2019-06-05  2.68   2       \n",
       "2019-06-04  2.69   2       \n",
       "2019-06-03  2.70   2       \n",
       "2019-06-02  2.69   2       \n",
       "2019-06-01  2.71   2       \n",
       "2019-05-30  2.71   2       \n",
       "2019-05-29  2.72   2       \n",
       "2019-05-28  2.72   2       \n",
       "2019-05-27  2.67   2       \n",
       "2019-05-26  2.62   2       \n",
       "2019-05-25  2.61   2       \n",
       "2019-05-24  2.59   2       \n",
       "2019-05-23  2.55   2       \n",
       "2019-05-19  2.55   2       \n",
       "2019-05-18  2.56   2       \n",
       "2019-05-16  2.56   2       \n",
       "2019-05-15  2.55   2       \n",
       "2019-05-14  2.54   2       \n",
       "2019-05-13  2.51   2       \n",
       "2019-05-12  2.41   2       \n",
       "2019-05-11  2.28   2       \n",
       "2019-05-10  2.20   2       \n",
       "2019-05-07  2.20   2       \n",
       "2019-05-06  2.19   2       \n",
       "2019-05-05  2.19   2       \n",
       "2019-05-04  2.17   2       \n",
       "2019-05-03  2.19   2       \n",
       "2019-05-02  2.43   2       \n",
       "2019-05-01  2.42   2       \n",
       "2019-04-30  2.44   2       \n",
       "2019-04-29  2.40   2       \n",
       "2019-04-28  2.05   2       \n",
       "2019-04-27  1.83   2       \n",
       "2019-04-26  1.80   2       \n",
       "2019-04-25  1.76   2       \n",
       "2019-04-24  1.76   2       \n",
       "2019-04-23  1.77   2       \n",
       "2019-04-22  1.77   2       \n",
       "2019-04-21  1.74   2       \n",
       "2019-04-20  1.71   2       \n",
       "2019-04-19  1.74   2       \n",
       "2019-04-15  1.74   2       \n",
       "2019-04-14  1.72   2       \n",
       "2019-04-13  1.64   2       \n",
       "2019-04-12  1.55   2       \n",
       "2019-04-11  1.55   2       \n",
       "2019-04-10  1.59   2       \n",
       "2019-04-09  1.76   2       \n",
       "2019-04-08  1.71   2       \n",
       "2019-04-07  1.56   2       \n",
       "2019-04-06  1.51   2       \n",
       "2019-04-05  1.50   2       \n",
       "2019-04-04  1.49   2       \n",
       "2019-04-03  1.48   2       \n",
       "2019-04-02  1.47   2       \n",
       "2019-04-01  1.46   2       \n",
       "2019-03-31  1.43   2       \n",
       "2019-03-30  1.42   2       \n",
       "2019-03-29  1.43   2       \n",
       "2019-03-28  1.39   2       \n",
       "2019-03-27  1.40   2       \n",
       "2019-03-26  1.34   2       \n",
       "2019-03-25  1.32   2       \n",
       "2019-03-24  1.29   2       \n",
       "2019-03-23  1.28   2       \n",
       "2019-03-22  1.27   2       \n",
       "2019-03-21  1.25   2       \n",
       "2019-03-20  1.25   2       \n",
       "2019-03-19  1.21   2       \n",
       "2019-03-18  1.12   2       \n",
       "2019-03-17  1.11   2       \n",
       "2019-03-16  1.05   2       \n",
       "2019-03-15  1.04   2       \n",
       "2019-03-14  1.00   2       \n",
       "2019-03-13  0.93   2       \n",
       "2019-03-12  0.92   2       \n",
       "2019-03-11  0.93   2       \n",
       "2019-03-10  0.91   2       \n",
       "2019-03-09  0.90   2       \n",
       "2019-03-08  0.85   2       \n",
       "2019-03-07  0.96   2       \n",
       "2019-03-06  0.92   2       \n",
       "2019-03-05  0.83   2       \n",
       "2019-03-04  0.83   2       \n",
       "2019-03-03  0.81   2       \n",
       "2019-03-02  0.72   2       \n",
       "2019-03-01  0.74   2       \n",
       "2019-02-28  0.81   2       \n",
       "2019-02-27  0.73   2       \n",
       "2019-02-26  0.72   2       \n",
       "2019-02-25  0.71   2       \n",
       "2019-02-24  0.68   2       \n",
       "2019-02-23  0.62   2       \n",
       "2019-02-22  0.55   2       \n",
       "2019-02-21  0.41   2       \n",
       "2019-02-20  0.43   2       \n",
       "2019-02-16  0.43   2       \n",
       "2019-02-15  0.67   2       \n",
       "2019-02-14  0.85   2       \n",
       "2019-02-13  0.93   2       \n",
       "2019-02-12  0.88   2       \n",
       "2019-02-11  0.86   2       \n",
       "2019-02-10  0.85   2       \n",
       "2019-02-09  0.86   2       \n",
       "2019-02-08  0.82   2       \n",
       "2019-02-07  0.80   2       \n",
       "2019-02-03  0.80   2       \n",
       "2019-02-02  0.83   2       \n",
       "2019-02-01  0.90   2       \n",
       "2019-01-31  0.91   2       \n",
       "2019-01-27  0.91   2       \n",
       "2019-01-26  1.11   2       \n",
       "2019-01-25  1.18   2       \n",
       "2019-01-24  0.99   2       \n",
       "2019-01-23  1.30   2       \n",
       "2019-01-22  1.44   2       \n",
       "2019-01-21  1.46   2       \n",
       "2019-01-20  1.51   2       \n",
       "2019-01-19  1.51   2       \n",
       "2019-01-18  1.52   2       \n",
       "2019-01-17  1.55   2       \n",
       "2019-01-16  1.55   2       \n",
       "2019-01-15  1.56   2       \n",
       "2019-01-14  1.54   2       \n",
       "2019-01-13  1.41   2       \n",
       "2019-01-12  1.39   2       \n",
       "2019-01-11  1.29   2       \n",
       "2019-01-10  1.29   2       \n",
       "2019-01-09  1.35   2       \n",
       "2019-01-08  1.29   2       \n",
       "2019-01-07  1.20   2       \n",
       "2019-01-06  1.26   2       \n",
       "2019-01-03  1.26   2       \n",
       "2019-01-02  1.31   2       \n",
       "2019-01-01  1.38   2       \n",
       "2018-12-31  1.38   2       \n",
       "2018-12-30  1.40   2       \n",
       "2018-12-29  1.48   2       \n",
       "2018-12-27  1.48   2       \n",
       "2018-12-26  1.49   2       \n",
       "2018-12-24  1.49   2       \n",
       "2018-12-23  1.50   2       \n",
       "2018-12-22  1.50   2       \n",
       "2018-12-21  2.03   2       \n",
       "2018-12-20  2.29   2       \n",
       "2018-12-19  2.29   2       \n",
       "2018-12-18  2.28   2       \n",
       "2018-12-17  2.38   2       \n",
       "2018-12-16  2.38   2       \n",
       "2018-12-15  2.41   2       \n",
       "2018-12-14  2.52   2       \n",
       "2018-12-13  2.52   2       \n",
       "2018-12-12  2.53   2       \n",
       "2018-12-11  2.62   2       \n",
       "2018-12-08  2.62   2       \n",
       "2018-12-07  2.64   2       \n",
       "2018-12-06  2.65   2       \n",
       "2018-12-04  2.65   2       \n",
       "2018-12-03  2.61   2       \n",
       "2018-12-02  2.51   2       \n",
       "2018-12-01  2.51   2       \n",
       "2018-11-30  2.52   2       \n",
       "2018-11-25  2.52   2       \n",
       "2018-11-24  2.64   2       \n",
       "2018-11-23  2.66   2       \n",
       "2018-11-22  2.64   2       \n",
       "2018-11-21  2.62   2       \n",
       "2018-11-16  2.62   2       \n",
       "2018-11-15  2.69   2       \n",
       "2018-11-14  2.76   2       \n",
       "2018-11-13  2.78   2       \n",
       "2018-11-12  2.80   2       \n",
       "2018-11-10  2.80   2       \n",
       "2018-11-09  2.77   2       \n",
       "2018-11-08  2.86   2       \n",
       "2018-11-07  2.89   2       \n",
       "2018-11-06  2.72   2       \n",
       "2018-11-02  2.72   2       \n",
       "2018-11-01  2.77   2       \n",
       "2018-10-31  2.80   2       \n",
       "2018-10-30  2.80   2       \n",
       "2018-10-29  2.89   2       \n",
       "2018-10-28  2.93   2       \n",
       "2018-10-27  3.10   2       \n",
       "2018-10-26  3.11   2       \n",
       "2018-10-25  3.11   2       \n",
       "2018-10-24  3.30   2       \n",
       "2018-10-23  3.31   2       \n",
       "2018-10-22  3.31   2       \n",
       "2018-10-21  3.35   2       \n",
       "2018-10-19  3.35   2       \n",
       "2018-10-18  3.32   2       \n",
       "2018-10-17  3.31   2       \n",
       "2018-10-13  3.31   2       \n",
       "2018-10-12  3.32   2       \n",
       "2018-10-11  3.34   2       \n",
       "2018-10-08  3.34   2       \n",
       "2018-10-07  3.35   2       \n",
       "2018-10-06  3.35   2       \n",
       "2018-10-05  3.38   2       \n",
       "2018-10-04  3.50   2       \n",
       "2018-10-02  3.50   2       \n",
       "2018-10-01  3.48   2       \n",
       "2018-09-28  3.48   2       \n",
       "2018-09-27  3.49   2       \n",
       "2018-09-26  3.52   2       \n",
       "2018-09-25  3.53   2       \n",
       "2018-09-24  3.55   2       \n",
       "2018-09-23  3.55   2       \n",
       "2018-09-22  3.36   2       \n",
       "2018-09-21  3.34   2       \n",
       "2018-09-14  3.34   2       \n",
       "2018-09-13  3.33   2       \n",
       "2018-09-12  3.33   2       \n",
       "2018-09-11  3.30   2       \n",
       "2018-09-10  3.29   2       \n",
       "2018-09-09  3.37   2       \n",
       "2018-09-08  3.43   2       \n",
       "2018-09-07  3.40   2       \n",
       "2018-09-03  3.40   2       \n",
       "2018-09-02  3.38   2       \n",
       "2018-09-01  3.37   2       \n",
       "2018-08-31  3.37   2       \n",
       "2018-08-30  3.38   2       \n",
       "2018-08-29  3.38   2       \n",
       "2019-11-30  7.50   0       \n",
       "2019-11-29  7.64   0       \n",
       "2019-11-28  8.31   0       \n",
       "2019-11-27  7.63   0       \n",
       "2019-11-26  7.24   0       \n",
       "2019-11-25  7.22   0       \n",
       "2019-11-24  7.11   0       \n",
       "2019-11-23  7.27   0       \n",
       "2019-11-22  7.10   0       \n",
       "2019-11-21  6.28   0       \n",
       "2019-11-20  6.90   0       \n",
       "2019-11-19  6.39   0       \n",
       "2019-11-18  7.83   0       \n",
       "2019-11-17  8.29   0       \n",
       "2019-11-16  9.46   0       \n",
       "2019-11-15  11.14  0       \n",
       "2019-11-14  12.08  0       \n",
       "2019-11-13  11.58  0       \n",
       "2019-11-12  11.17  0       \n",
       "2019-11-11  9.68   0       \n",
       "2019-11-10  9.34   0       \n",
       "2019-11-09  9.47   0       \n",
       "2019-11-08  8.42   0       \n",
       "2019-11-07  6.97   0       \n",
       "2019-11-06  7.10   0       \n",
       "2019-11-05  5.90   0       \n",
       "2019-11-04  5.67   0       \n",
       "2019-11-03  6.64   0       \n",
       "2019-11-02  7.34   0       \n",
       "2019-11-01  7.55   0       \n",
       "2019-10-31  7.56   0       \n",
       "2019-10-30  6.08   0       \n",
       "2019-10-29  6.01   0       \n",
       "2019-10-28  5.98   0       \n",
       "2019-10-27  5.57   0       \n",
       "2019-10-26  4.47   0       \n",
       "2019-10-25  3.90   0       \n",
       "2019-10-24  4.46   0       \n",
       "2019-10-23  3.53   0       \n",
       "2019-10-22  2.84   0       \n",
       "2019-10-21  1.31   0       \n",
       "2019-10-20  1.33   0       \n",
       "2019-10-19  1.39   0       \n",
       "2019-10-18  1.30   0       \n",
       "2019-10-17  1.27   0       \n",
       "2019-10-16  1.27   0       \n",
       "2019-10-15  1.38   0       \n",
       "2019-10-14  1.43   0       \n",
       "2019-10-13  0.94   0       \n",
       "2019-10-12  0.90   0       \n",
       "2019-10-11  0.80   0       \n",
       "2019-10-10  0.78   0       \n",
       "2019-10-09  0.94   0       \n",
       "2019-10-08  0.82   0       \n",
       "2019-10-07  0.91   0       \n",
       "2019-10-06  0.92   0       \n",
       "2019-10-05  0.99   0       \n",
       "2019-10-04  0.93   0       \n",
       "2019-10-03  0.99   0       \n",
       "2019-10-02  1.16   0       \n",
       "2019-10-01  1.16   0       \n",
       "2019-09-30  1.00   0       \n",
       "2019-09-29  1.16   0       \n",
       "2019-09-28  1.34   0       \n",
       "2019-09-27  1.38   0       \n",
       "2019-09-26  1.23   0       \n",
       "2019-09-25  1.29   0       \n",
       "2019-09-24  1.35   0       \n",
       "2019-09-23  1.32   0       \n",
       "2019-09-22  1.29   0       \n",
       "2019-09-21  1.21   0       \n",
       "2019-09-20  1.46   0       \n",
       "2019-09-19  1.37   0       \n",
       "2019-09-18  1.38   0       \n",
       "2019-09-17  1.56   0       \n",
       "2019-09-16  1.54   0       \n",
       "2019-09-15  1.46   0       \n",
       "2019-09-14  1.36   0       \n",
       "2019-09-13  1.22   0       \n",
       "2019-09-12  1.24   0       \n",
       "2019-09-11  1.42   0       \n",
       "2019-09-10  1.43   0       \n",
       "2019-09-09  1.44   0       \n",
       "2019-09-08  1.48   0       \n",
       "2019-09-07  1.66   0       \n",
       "2019-09-06  1.65   0       \n",
       "2018-04-13  4.05   2       \n",
       "2018-04-12  4.02   2       \n",
       "2018-04-11  3.99   2       \n",
       "2018-04-10  3.98   2       \n",
       "2018-04-09  3.87   2       \n",
       "2018-04-08  3.84   2       \n",
       "2018-04-07  3.84   2       \n",
       "2018-04-06  3.73   2       \n",
       "2018-04-05  3.67   2       \n",
       "2018-04-04  3.37   2       \n",
       "2018-04-03  3.33   2       \n",
       "2018-04-02  3.29   2       \n",
       "2018-04-01  3.27   2       \n",
       "2018-03-31  3.16   2       \n",
       "2018-03-30  3.11   2       \n",
       "2018-03-29  2.97   2       \n",
       "2018-03-28  2.91   2       \n",
       "2018-03-27  2.92   2       \n",
       "2018-03-26  2.95   2       \n",
       "2018-03-25  2.95   2       \n",
       "2018-03-24  2.94   2       \n",
       "2018-03-23  2.92   2       \n",
       "2018-03-22  2.89   2       \n",
       "2018-03-21  2.91   2       \n",
       "2018-03-20  2.90   2       \n",
       "2018-03-19  2.92   2       \n",
       "2018-03-18  2.89   2       \n",
       "2018-03-17  2.88   2       \n",
       "2018-03-16  2.86   2       \n",
       "2018-03-14  2.86   2       \n",
       "2018-03-13  2.97   2       \n",
       "2018-03-12  2.99   2       \n",
       "2018-03-11  3.00   2       \n",
       "2018-03-10  2.99   2       \n",
       "2018-03-09  2.97   2       \n",
       "2018-03-05  2.97   2       \n",
       "2018-03-04  2.99   2       \n",
       "2018-03-03  3.01   2       \n",
       "2018-03-02  3.12   2       \n",
       "2018-03-01  3.20   2       \n",
       "2018-02-28  3.38   2       \n",
       "2018-02-26  3.38   2       \n",
       "2018-02-25  3.39   2       \n",
       "2018-02-21  3.39   2       \n",
       "2018-02-20  3.40   2       \n",
       "2018-02-19  3.41   2       \n",
       "2018-02-18  3.37   2       \n",
       "2018-02-17  3.30   2       \n",
       "2018-02-16  2.97   2       \n",
       "2018-02-15  2.76   2       \n",
       "2018-02-13  2.76   2       \n",
       "2018-02-12  2.73   2       \n",
       "2018-02-11  2.72   2       \n",
       "2018-02-06  2.72   2       \n",
       "2018-02-05  2.92   2       \n",
       "2018-02-04  2.94   2       \n",
       "2018-02-03  2.96   2       \n",
       "2018-02-01  2.96   2       \n",
       "2018-01-31  3.02   2       \n",
       "2018-01-30  3.03   2       \n",
       "2018-01-29  3.03   2       \n",
       "2018-01-28  3.17   2       \n",
       "2018-01-27  3.21   2       \n",
       "2018-01-26  3.21   2       \n",
       "2018-01-25  3.18   2       \n",
       "2018-01-24  3.37   2       \n",
       "2018-01-23  3.50   2       \n",
       "2018-01-22  3.83   2       \n",
       "2018-01-21  3.90   2       \n",
       "2018-01-20  3.91   2       \n",
       "2018-01-19  4.07   2       \n",
       "2018-01-18  4.14   2       \n",
       "2018-01-17  4.14   2       \n",
       "2018-01-16  4.29   2       \n",
       "2018-01-15  4.33   2       \n",
       "2018-01-14  4.32   2       \n",
       "2018-01-13  4.32   2       \n",
       "2018-01-12  4.45   2       \n",
       "2018-01-11  4.53   2       \n",
       "2018-01-10  4.68   2       \n",
       "2018-01-09  4.69   2       \n",
       "2018-01-08  4.69   2       \n",
       "2018-01-07  4.67   2       \n",
       "2018-01-06  4.66   2       \n",
       "2017-12-29  4.66   2       \n",
       "2017-12-28  4.65   2       \n",
       "2017-12-24  4.65   2       \n",
       "2017-12-23  4.62   2       \n",
       "2017-12-22  4.61   2       \n",
       "2017-12-21  4.67   2       \n",
       "2017-12-20  4.83   2       \n",
       "2017-12-17  4.83   2       \n",
       "2017-12-16  4.85   2       \n",
       "2017-12-15  4.87   2       \n",
       "2017-12-14  4.90   2       \n",
       "2017-12-12  4.90   2       \n",
       "2017-12-11  4.70   2       \n",
       "2017-12-10  4.66   2       \n",
       "2017-12-07  4.66   2       \n",
       "2017-12-06  4.44   2       \n",
       "2017-12-05  4.44   2       \n",
       "2017-12-04  4.45   2       \n",
       "2017-12-02  4.45   2       \n",
       "2017-12-01  4.46   2       \n",
       "2017-11-29  4.46   2       \n",
       "2017-11-28  4.50   2       \n",
       "2017-11-27  4.50   2       \n",
       "2017-11-26  4.51   2       \n",
       "2017-11-25  4.49   2       \n",
       "2017-11-24  4.48   2       \n",
       "2017-11-23  4.50   2       \n",
       "2017-11-18  4.50   2       \n",
       "2017-11-17  4.47   2       \n",
       "2017-11-15  4.47   2       \n",
       "2017-11-14  4.48   2       \n",
       "2017-11-13  4.49   2       \n",
       "2017-11-12  4.49   2       \n",
       "2017-11-11  4.48   2       \n",
       "2017-11-08  4.48   2       \n",
       "2017-11-07  4.47   2       \n",
       "2017-11-06  4.44   2       \n",
       "2017-11-05  4.46   2       \n",
       "2017-11-04  4.47   2       \n",
       "2017-11-03  4.47   2       \n",
       "2017-11-02  4.46   2       \n",
       "2017-11-01  4.43   2       \n",
       "2017-10-30  4.43   2       \n",
       "2017-10-29  4.42   2       \n",
       "2017-10-28  4.53   2       \n",
       "2017-10-27  4.35   2       \n",
       "2017-10-26  4.32   2       \n",
       "2017-10-22  4.32   2       \n",
       "2017-10-21  4.28   2       \n",
       "2017-10-10  4.28   2       \n",
       "2017-10-09  4.32   2       \n",
       "2017-10-08  4.30   2       \n",
       "2017-10-07  4.28   2       \n",
       "2017-10-06  4.30   2       \n",
       "2017-10-04  4.30   2       \n",
       "2017-10-03  4.35   2       \n",
       "2017-10-02  4.66   2       \n",
       "2017-10-01  4.69   2       \n",
       "2017-09-30  4.69   2       \n",
       "2017-09-29  4.70   2       \n",
       "2017-09-28  4.75   2       \n",
       "2017-09-27  4.78   2       \n",
       "2017-09-26  4.72   2       \n",
       "2017-09-25  4.72   2       \n",
       "2017-09-24  4.60   2       \n",
       "2017-09-23  4.54   2       \n",
       "2017-09-22  4.34   2       \n",
       "2017-09-21  4.33   2       \n",
       "2017-09-20  4.35   2       \n",
       "2017-09-19  4.32   2       \n",
       "2017-09-18  4.35   2       \n",
       "2017-09-17  4.35   2       \n",
       "2017-09-16  4.36   2       \n",
       "2017-09-14  4.36   2       \n",
       "2017-09-13  4.35   2       \n",
       "2017-09-12  4.35   2       \n",
       "2017-09-11  4.34   2       \n",
       "2017-09-10  4.34   2       \n",
       "2017-09-09  4.19   2       \n",
       "2017-09-08  4.23   2       \n",
       "2017-09-07  4.27   2       \n",
       "2017-09-06  4.26   2       \n",
       "2017-09-03  4.26   2       \n",
       "2017-09-02  4.25   2       \n",
       "2017-08-30  4.25   2       \n",
       "2017-08-29  4.26   2       \n",
       "2017-08-28  4.25   2       \n",
       "2017-08-26  4.25   2       \n",
       "2017-08-25  4.24   2       \n",
       "2017-08-22  4.24   2       \n",
       "2017-08-21  4.23   2       \n",
       "2017-08-20  4.38   2       \n",
       "2017-08-18  4.38   2       \n",
       "2017-08-17  4.39   2       \n",
       "2017-08-15  4.39   2       \n",
       "2017-08-14  4.37   2       \n",
       "2017-08-13  4.53   2       \n",
       "2017-08-11  4.53   2       \n",
       "2017-08-10  4.51   2       \n",
       "2017-08-09  4.46   2       \n",
       "2017-08-08  4.45   2       \n",
       "2017-08-07  4.45   2       \n",
       "2017-08-06  4.44   2       \n",
       "2017-08-05  4.43   2       \n",
       "2017-08-04  4.57   2       \n",
       "2017-08-03  4.41   2       \n",
       "2017-08-02  4.37   2       \n",
       "2017-08-01  4.37   2       \n",
       "2017-07-31  4.25   2       \n",
       "2017-07-30  4.24   2       \n",
       "2017-07-29  4.22   2       \n",
       "2017-07-28  4.10   2       \n",
       "2017-07-27  4.04   2       \n",
       "2017-07-26  4.06   2       \n",
       "2017-07-25  4.24   2       \n",
       "2017-07-24  4.18   2       \n",
       "2017-07-23  4.08   2       \n",
       "2017-07-22  3.83   2       \n",
       "2017-07-21  3.62   2       \n",
       "2017-07-20  3.72   2       \n",
       "2017-07-19  3.70   2       \n",
       "2017-07-18  3.74   2       \n",
       "2017-07-17  3.71   2       \n",
       "2017-07-16  3.73   2       \n",
       "2017-07-15  3.53   2       \n",
       "2017-07-14  3.52   2       \n",
       "2017-07-13  3.51   2       \n",
       "2017-07-12  3.51   2       \n",
       "2017-07-11  3.52   2       \n",
       "2017-07-10  3.55   2       \n",
       "2017-07-09  3.49   2       \n",
       "2017-07-08  3.56   2       \n",
       "2017-07-07  3.61   2       \n",
       "2017-07-06  3.77   2       \n",
       "2017-07-05  3.70   2       \n",
       "2017-07-04  3.41   2       \n",
       "2017-07-03  3.28   2       \n",
       "2017-07-02  3.26   2       \n",
       "2017-07-01  3.26   2       \n",
       "2017-06-30  3.10   2       \n",
       "2017-06-29  3.05   2       \n",
       "2020-06-28  5.28   3       \n",
       "2020-06-27  5.40   3       \n",
       "2020-06-26  5.87   3       \n",
       "2020-06-03  5.87   3       \n",
       "2020-06-02  5.51   3       \n",
       "2020-06-01  5.21   3       \n",
       "2020-05-23  5.21   3       \n",
       "2020-05-22  5.36   3       \n",
       "2020-05-21  5.73   3       \n",
       "2020-05-20  5.73   3       \n",
       "2020-05-19  5.37   3       \n",
       "2020-05-18  5.15   3       \n",
       "2020-05-17  5.53   3       \n",
       "2020-05-15  5.53   3       \n",
       "2020-05-14  6.17   3       \n",
       "2020-05-13  6.26   3       \n",
       "2020-05-12  6.84   3       \n",
       "2020-05-11  6.99   3       \n",
       "2020-05-10  6.99   3       \n",
       "2020-05-09  6.35   3       \n",
       "2020-05-08  6.26   3       \n",
       "2020-04-12  6.26   3       \n",
       "2020-04-11  6.78   3       \n",
       "2020-04-10  6.99   3       \n",
       "2020-03-30  6.99   3       \n",
       "2020-03-29  6.97   3       \n",
       "2020-03-28  6.72   3       \n",
       "2020-03-26  6.72   3       \n",
       "2020-03-25  6.69   3       \n",
       "2020-03-24  5.99   3       \n",
       "2020-03-14  5.99   3       \n",
       "2020-03-13  5.69   3       \n",
       "2020-03-12  5.39   3       \n",
       "2020-03-10  5.39   3       \n",
       "2020-03-09  6.67   3       \n",
       "2020-03-08  8.05   3       \n",
       "2020-02-11  8.05   3       \n",
       "2020-02-10  8.13   3       \n",
       "2020-02-09  8.88   3       \n",
       "2020-02-08  9.04   3       \n",
       "2020-01-28  9.04   3       \n",
       "2020-01-27  8.83   3       \n",
       "2020-01-26  8.05   3       \n",
       "2020-01-22  8.05   3       \n",
       "2020-01-21  8.13   3       \n",
       "2020-01-20  9.04   3       \n",
       "2020-01-18  9.04   3       \n",
       "2020-01-17  9.50   3       \n",
       "2020-01-16  10.04  3       \n",
       "2020-01-13  10.04  3       \n",
       "2020-01-12  9.87   3       \n",
       "2020-01-11  8.05   3       \n",
       "2020-01-05  8.05   3       \n",
       "2020-01-04  7.81   3       \n",
       "2020-01-03  7.32   3       \n",
       "2020-01-01  7.32   3       \n",
       "2019-12-31  7.75   3       \n",
       "2019-12-30  9.04   3       \n",
       "2019-12-29  9.04   3       \n",
       "2019-12-28  9.92   3       \n",
       "2019-12-27  10.04  3       \n",
       "2019-12-25  10.04  3       \n",
       "2019-12-24  10.46  3       \n",
       "2019-12-23  11.04  3       \n",
       "2019-12-22  9.68   3       \n",
       "2019-12-21  8.55   3       \n",
       "2019-12-20  7.77   3       \n",
       "2019-12-18  7.77   3       \n",
       "2019-12-17  7.56   3       \n",
       "2019-12-16  7.04   3       \n",
       "2019-12-15  7.04   3       \n",
       "2019-12-14  7.25   3       \n",
       "2019-12-13  7.77   3       \n",
       "2019-12-08  7.77   3       \n",
       "2019-12-07  8.32   3       \n",
       "2019-12-06  8.50   3       \n",
       "2019-12-04  8.50   3       \n",
       "2019-12-03  8.54   3       \n",
       "2019-12-02  9.49   3       \n",
       "2019-12-01  9.49   3       \n",
       "2019-11-30  9.21   3       \n",
       "2019-11-29  7.79   3       \n",
       "2019-11-25  7.79   3       \n",
       "2019-11-24  8.52   3       \n",
       "2019-11-23  8.44   3       \n",
       "2019-11-22  7.96   3       \n",
       "2019-11-21  6.47   3       \n",
       "2019-11-20  6.22   3       \n",
       "2019-11-19  6.80   3       \n",
       "2019-11-18  6.80   3       \n",
       "2019-11-17  7.32   3       \n",
       "2019-11-16  7.53   3       \n",
       "2019-11-11  7.53   3       \n",
       "2019-11-10  7.44   3       \n",
       "2019-11-09  6.74   3       \n",
       "2019-11-08  6.74   3       \n",
       "2019-11-07  6.72   3       \n",
       "2019-11-06  6.14   3       \n",
       "2019-11-05  6.14   3       \n",
       "2019-11-04  6.00   3       \n",
       "2019-11-03  5.03   3       \n",
       "2019-11-02  4.57   3       \n",
       "2019-11-01  4.57   3       \n",
       "2019-10-31  4.79   3       \n",
       "2019-10-30  4.10   3       \n",
       "2019-10-29  4.35   3       \n",
       "2019-10-28  4.12   3       \n",
       "2019-10-27  3.80   3       \n",
       "2019-10-26  3.86   3       \n",
       "2019-10-25  3.99   3       \n",
       "2019-10-24  3.44   3       \n",
       "2019-10-23  2.48   3       \n",
       "2019-10-22  2.25   3       \n",
       "2019-10-21  1.83   3       \n",
       "2019-10-20  1.77   3       \n",
       "2019-09-19  1.77   3       \n",
       "2019-09-18  1.62   3       \n",
       "2019-09-17  1.55   3       \n",
       "2019-09-08  1.55   3       \n",
       "2019-09-07  1.77   3       \n",
       "2019-08-03  1.77   3       \n",
       "2019-08-02  1.83   3       \n",
       "2019-08-01  1.99   3       \n",
       "2019-07-24  1.99   3       \n",
       "2019-07-23  2.05   3       \n",
       "2019-07-22  2.28   3       \n",
       "2019-07-21  2.28   3       \n",
       "2019-07-20  2.20   3       \n",
       "2019-07-19  1.99   3       \n",
       "2019-07-01  1.99   3       \n",
       "2019-06-30  1.85   3       \n",
       "2019-06-29  1.77   3       \n",
       "2019-06-27  1.77   3       \n",
       "2019-06-26  1.82   3       \n",
       "2019-06-25  1.99   3       \n",
       "2019-06-23  1.99   3       \n",
       "2019-06-22  1.62   3       \n",
       "2019-06-21  1.55   3       \n",
       "2019-06-09  1.55   3       \n",
       "2019-06-08  1.64   3       \n",
       "2019-06-07  1.77   3       \n",
       "2019-06-06  1.65   3       \n",
       "2019-06-05  1.33   3       \n",
       "2019-05-31  1.33   3       \n",
       "2019-05-30  1.15   3       \n",
       "2019-05-29  1.11   3       \n",
       "2019-05-26  1.11   3       \n",
       "2019-05-25  1.23   3       \n",
       "2019-05-24  1.30   3       \n",
       "2019-04-29  1.30   3       \n",
       "2019-04-28  1.48   3       \n",
       "2019-04-27  1.52   3       \n",
       "2019-04-24  1.52   3       \n",
       "2019-04-23  1.57   3       \n",
       "2019-04-22  1.74   3       \n",
       "2019-04-20  1.74   3       \n",
       "2019-04-19  1.52   3       \n",
       "2019-04-14  1.52   3       \n",
       "2019-04-13  1.33   3       \n",
       "2019-04-08  1.33   3       \n",
       "2019-04-07  1.14   3       \n",
       "2019-04-05  1.14   3       \n",
       "2019-04-04  0.98   3       \n",
       "2019-03-30  0.98   3       \n",
       "2019-03-29  0.84   3       \n",
       "2019-03-24  0.84   3       \n",
       "2019-03-23  0.72   3       \n",
       "2019-03-20  0.72   3       \n",
       "2019-03-19  0.63   3       \n",
       "2019-03-18  0.60   3       \n",
       "2019-03-17  0.62   3       \n",
       "2019-03-16  0.72   3       \n",
       "2019-03-12  0.72   3       \n",
       "2019-03-11  0.71   3       \n",
       "2019-03-10  0.60   3       \n",
       "2019-03-09  0.46   3       \n",
       "2019-03-08  0.40   3       \n",
       "2019-03-03  0.40   3       \n",
       "2019-03-02  0.33   3       \n",
       "2019-03-01  0.31   3       \n",
       "2019-02-28  0.21   3       \n",
       "2019-02-27  0.23   3       \n",
       "2019-02-26  0.23   3       \n",
       "2019-02-25  0.25   3       \n",
       "2019-02-24  0.36   3       \n",
       "2019-02-13  0.36   3       \n",
       "2019-02-12  0.45   3       \n",
       "2019-02-10  0.45   3       \n",
       "2019-02-09  0.54   3       \n",
       "2019-02-01  0.54   3       \n",
       "2019-01-31  0.62   3       \n",
       "2019-01-30  0.63   3       \n",
       "2019-01-22  0.63   3       \n",
       "2019-01-21  0.57   3       \n",
       "2019-01-20  0.51   3       \n",
       "2019-01-11  0.51   3       \n",
       "2019-01-10  0.60   3       \n",
       "2018-12-31  0.60   3       \n",
       "2018-12-30  0.51   3       \n",
       "2018-12-29  0.49   3       \n",
       "2018-12-25  0.49   3       \n",
       "2018-12-24  0.44   3       \n",
       "2018-12-23  0.49   3       \n",
       "2018-12-16  0.49   3       \n",
       "2018-12-15  0.57   3       \n",
       "2018-12-14  0.58   3       \n",
       "2018-12-13  0.68   3       \n",
       "2018-12-12  0.70   3       \n",
       "2018-12-08  0.70   3       \n",
       "2018-12-07  0.78   3       \n",
       "2018-12-06  0.82   3       \n",
       "2018-12-05  0.89   3       \n",
       "2018-12-04  1.01   3       \n",
       "2018-11-16  1.01   3       \n",
       "2018-11-15  1.19   3       \n",
       "2018-11-14  1.20   3       \n",
       "2018-11-09  1.20   3       \n",
       "2018-11-08  1.31   3       \n",
       "2018-11-07  1.39   3       \n",
       "2018-10-03  1.39   3       \n",
       "2018-10-02  1.48   3       \n",
       "2018-10-01  1.64   3       \n",
       "2018-09-30  1.73   3       \n",
       "2018-09-23  1.73   3       \n",
       "2018-09-22  1.82   3       \n",
       "2018-09-21  1.95   3       \n",
       "2018-09-14  1.95   3       \n",
       "2018-09-13  2.12   3       \n",
       "2018-09-12  2.24   3       \n",
       "2018-09-11  2.24   3       \n",
       "2018-09-10  2.18   3       \n",
       "2018-09-09  1.95   3       \n",
       "2018-09-08  1.95   3       \n",
       "2018-09-07  1.88   3       \n",
       "2018-08-28  3.39   2       \n",
       "2018-08-27  3.41   2       \n",
       "2018-08-26  3.41   2       \n",
       "2018-08-25  3.44   2       \n",
       "2018-08-24  3.60   2       \n",
       "2018-08-23  3.63   2       \n",
       "2018-08-22  3.63   2       \n",
       "2018-08-21  3.62   2       \n",
       "2018-08-20  3.67   2       \n",
       "2018-08-19  3.67   2       \n",
       "2018-08-18  3.84   2       \n",
       "2018-08-17  3.85   2       \n",
       "2018-08-16  3.85   2       \n",
       "2018-08-15  3.82   2       \n",
       "2018-08-14  3.66   2       \n",
       "2018-08-12  3.66   2       \n",
       "2018-08-11  3.68   2       \n",
       "2018-08-10  3.66   2       \n",
       "2018-08-09  3.65   2       \n",
       "2018-08-08  3.65   2       \n",
       "2018-08-07  3.67   2       \n",
       "2018-08-03  3.67   2       \n",
       "2018-08-02  3.73   2       \n",
       "2018-08-01  3.64   2       \n",
       "2018-07-31  3.75   2       \n",
       "2018-07-30  3.69   2       \n",
       "2018-07-29  3.65   2       \n",
       "2018-07-28  3.65   2       \n",
       "2018-07-27  3.69   2       \n",
       "2018-07-26  3.69   2       \n",
       "2018-07-25  3.68   2       \n",
       "2018-07-24  3.68   2       \n",
       "2018-07-23  3.56   2       \n",
       "2018-07-22  3.66   2       \n",
       "2018-07-21  3.69   2       \n",
       "2018-07-20  3.56   2       \n",
       "2018-07-19  3.51   2       \n",
       "2018-07-09  3.51   2       \n",
       "2018-07-08  3.49   2       \n",
       "2018-07-07  3.47   2       \n",
       "2018-07-06  3.30   2       \n",
       "2018-07-05  3.38   2       \n",
       "2018-07-04  3.45   2       \n",
       "2018-07-03  3.46   2       \n",
       "2018-07-02  3.64   2       \n",
       "2018-06-21  3.64   2       \n",
       "2018-06-20  3.65   2       \n",
       "2018-06-13  3.65   2       \n",
       "2018-06-12  3.67   2       \n",
       "2018-06-11  3.80   2       \n",
       "2018-06-10  3.86   2       \n",
       "2018-06-05  3.86   2       \n",
       "2018-06-04  3.87   2       \n",
       "2018-06-03  3.89   2       \n",
       "2018-06-02  3.87   2       \n",
       "2018-05-31  3.87   2       \n",
       "2018-05-30  3.90   2       \n",
       "2018-05-29  3.92   2       \n",
       "2018-05-28  3.93   2       \n",
       "2018-05-27  4.01   2       \n",
       "2018-05-26  4.04   2       \n",
       "2018-05-23  4.04   2       \n",
       "2018-05-22  4.03   2       \n",
       "2018-05-16  4.03   2       \n",
       "2018-05-15  3.99   2       \n",
       "2018-05-14  3.97   2       \n",
       "2018-05-12  3.97   2       \n",
       "2018-05-11  3.99   2       \n",
       "2018-05-10  4.24   2       \n",
       "2018-05-09  4.25   2       \n",
       "2018-05-08  4.25   2       \n",
       "2018-05-07  4.24   2       \n",
       "2018-05-06  4.23   2       \n",
       "2018-05-05  4.24   2       \n",
       "2018-04-29  4.24   2       \n",
       "2018-04-28  4.25   2       \n",
       "2018-04-27  4.27   2       \n",
       "2018-04-25  4.27   2       \n",
       "2018-04-24  4.26   2       \n",
       "2018-04-23  4.25   2       \n",
       "2018-04-22  4.22   2       \n",
       "2018-04-20  4.22   2       \n",
       "2018-04-19  4.25   2       \n",
       "2018-04-18  4.21   2       \n",
       "2018-04-17  4.19   2       \n",
       "2018-04-16  4.19   2       \n",
       "2018-04-15  4.07   2       \n",
       "2018-04-14  4.04   2       \n",
       "2019-10-05  2.65   1       \n",
       "2019-10-04  2.65   1       \n",
       "2019-10-03  2.63   1       \n",
       "2019-10-02  2.64   1       \n",
       "2019-10-01  2.62   1       \n",
       "2019-09-30  2.62   1       \n",
       "2019-09-29  2.63   1       \n",
       "2019-09-28  2.64   1       \n",
       "2019-09-27  2.64   1       \n",
       "2019-09-26  2.62   1       \n",
       "2019-09-25  2.74   1       \n",
       "2019-09-24  2.82   1       \n",
       "2019-09-22  2.82   1       \n",
       "2019-09-21  2.84   1       \n",
       "2019-09-20  2.81   1       \n",
       "2019-09-19  2.82   1       \n",
       "2019-09-18  2.80   1       \n",
       "2019-09-17  2.81   1       \n",
       "2019-09-16  2.82   1       \n",
       "2019-09-15  2.81   1       \n",
       "2019-09-14  2.80   1       \n",
       "2019-09-13  2.79   1       \n",
       "2019-09-12  2.79   1       \n",
       "2019-09-11  2.77   1       \n",
       "2019-09-10  2.77   1       \n",
       "2019-09-09  2.78   1       \n",
       "2019-09-08  3.07   1       \n",
       "2019-09-07  3.07   1       \n",
       "2019-09-06  2.95   1       \n",
       "2019-09-05  2.95   1       \n",
       "2019-09-04  2.96   1       \n",
       "2018-07-09  1.67   3       \n",
       "2018-07-08  1.59   3       \n",
       "2018-07-07  1.45   3       \n",
       "2018-07-06  1.38   3       \n",
       "2018-07-05  1.42   3       \n",
       "2018-07-04  1.42   3       \n",
       "2018-07-03  1.28   3       \n",
       "2018-07-02  1.33   3       \n",
       "2018-07-01  1.39   3       \n",
       "2018-06-26  1.39   3       \n",
       "2018-06-25  2.24   3       \n",
       "2018-06-24  2.28   3       \n",
       "2018-06-21  2.28   3       \n",
       "2018-06-20  2.06   3       \n",
       "2018-06-19  1.99   3       \n",
       "2018-06-13  1.99   3       \n",
       "2018-06-12  2.14   3       \n",
       "2018-06-11  2.28   3       \n",
       "2018-06-10  2.05   3       \n",
       "2018-06-09  1.99   3       \n",
       "2018-06-07  1.99   3       \n",
       "2018-06-06  2.45   3       \n",
       "2018-06-05  2.47   3       \n",
       "2018-06-02  2.47   3       \n",
       "2018-06-01  2.25   3       \n",
       "2018-05-31  2.18   3       \n",
       "2018-05-24  2.18   3       \n",
       "2018-05-23  1.69   3       \n",
       "2018-05-22  1.72   3       \n",
       "2018-05-21  1.85   3       \n",
       "2018-05-20  1.85   3       \n",
       "2018-05-19  2.08   3       \n",
       "2018-05-18  2.40   3       \n",
       "2018-05-16  2.40   3       \n",
       "2018-05-15  2.69   3       \n",
       "2018-05-05  2.69   3       \n",
       "2018-05-04  2.81   3       \n",
       "2018-05-03  3.09   3       \n",
       "2018-05-02  3.42   3       \n",
       "2018-05-01  3.73   3       \n",
       "2018-04-27  3.73   3       \n",
       "2018-04-26  3.83   3       \n",
       "2018-04-25  4.12   3       \n",
       "2018-04-23  4.12   3       \n",
       "2018-04-22  3.94   3       \n",
       "2018-04-21  3.73   3       \n",
       "2018-04-15  3.73   3       \n",
       "2018-04-14  3.39   3       \n",
       "2018-04-13  2.98   3       \n",
       "2018-04-11  2.98   3       \n",
       "2018-04-10  2.95   3       \n",
       "2018-04-09  2.66   3       \n",
       "2018-04-07  2.66   3       \n",
       "2018-04-06  2.78   3       \n",
       "2018-04-05  2.95   3       \n",
       "2018-04-02  2.95   3       \n",
       "2018-04-01  3.13   3       \n",
       "2018-03-31  3.37   3       \n",
       "2018-03-30  2.95   3       \n",
       "2018-03-29  2.71   3       \n",
       "2018-03-28  2.63   3       \n",
       "2018-03-21  2.63   3       \n",
       "2018-03-20  2.26   3       \n",
       "2018-03-19  2.05   3       \n",
       "2018-03-18  2.05   3       \n",
       "2018-03-17  1.86   3       \n",
       "2018-03-16  1.76   3       \n",
       "2018-03-13  1.76   3       \n",
       "2018-03-12  1.87   3       \n",
       "2018-03-11  2.27   3       \n",
       "2018-03-10  2.27   3       \n",
       "2018-03-09  2.48   3       \n",
       "2018-03-08  2.56   3       \n",
       "2018-03-04  2.56   3       \n",
       "2018-03-03  2.81   3       \n",
       "2018-03-02  2.85   3       \n",
       "2018-03-01  2.85   3       \n",
       "2018-02-28  2.86   3       \n",
       "2018-02-27  3.16   3       \n",
       "2018-02-26  3.53   3       \n",
       "2018-02-24  3.53   3       \n",
       "2018-02-23  2.97   3       \n",
       "2018-02-22  2.89   3       \n",
       "2018-02-21  3.11   3       \n",
       "2018-02-19  3.11   3       \n",
       "2018-02-18  2.72   3       \n",
       "2018-02-17  2.51   3       \n",
       "2018-02-16  2.24   3       \n",
       "2018-02-15  2.40   3       \n",
       "2018-02-14  2.53   3       \n",
       "2018-02-13  2.53   3       \n",
       "2018-02-12  2.76   3       \n",
       "2018-02-11  2.82   3       \n",
       "2018-02-10  2.63   3       \n",
       "2018-02-09  1.99   3       \n",
       "2018-02-08  1.95   3       \n",
       "2018-02-07  2.10   3       \n",
       "2018-02-06  2.24   3       \n",
       "2018-02-05  1.99   3       \n",
       "2018-02-04  1.95   3       \n",
       "2018-02-02  1.95   3       \n",
       "2018-02-01  2.22   3       \n",
       "2018-01-31  2.24   3       \n",
       "2018-01-19  2.24   3       \n",
       "2018-01-18  2.23   3       \n",
       "2018-01-17  2.17   3       \n",
       "2018-01-16  2.24   3       \n",
       "2018-01-15  2.11   3       \n",
       "2018-01-14  2.01   3       \n",
       "2018-01-13  2.24   3       \n",
       "2018-01-12  1.97   3       \n",
       "2018-01-11  1.93   3       \n",
       "2018-01-09  1.93   3       \n",
       "2018-01-08  1.51   3       \n",
       "2018-01-07  1.45   3       \n",
       "2018-01-05  1.45   3       \n",
       "2018-01-04  1.24   3       \n",
       "2018-01-03  1.04   3       \n",
       "2018-01-02  1.08   3       \n",
       "2018-01-01  1.11   3       \n",
       "2017-12-31  0.97   3       \n",
       "2017-12-30  1.04   3       \n",
       "2017-12-29  1.12   3       \n",
       "2017-12-28  1.25   3       \n",
       "2017-12-27  1.34   3       \n",
       "2017-12-26  1.72   3       \n",
       "2017-12-25  1.91   3       \n",
       "2017-12-21  1.91   3       \n",
       "2017-12-20  1.66   3       \n",
       "2017-12-19  1.65   3       \n",
       "2017-12-18  1.82   3       \n",
       "2017-12-17  1.87   3       \n",
       "2017-12-15  1.87   3       \n",
       "2017-12-14  1.58   3       \n",
       "2017-12-13  1.54   3       \n",
       "2017-12-02  1.54   3       \n",
       "2017-12-01  1.61   3       \n",
       "2017-11-30  1.85   3       \n",
       "2017-11-29  1.90   3       \n",
       "2017-11-27  1.90   3       \n",
       "2017-11-26  2.15   3       \n",
       "2017-11-25  2.19   3       \n",
       "2017-11-21  2.19   3       \n",
       "2017-11-20  2.43   3       \n",
       "2017-11-19  2.48   3       \n",
       "2017-11-17  2.48   3       \n",
       "2017-11-16  2.63   3       \n",
       "2017-11-15  2.77   3       \n",
       "2017-11-14  2.82   3       \n",
       "2017-11-13  3.06   3       \n",
       "2017-11-11  3.06   3       \n",
       "2017-11-10  2.94   3       \n",
       "2017-11-09  2.48   3       \n",
       "2017-11-02  2.48   3       \n",
       "2017-11-01  2.29   3       \n",
       "2017-10-31  2.19   3       \n",
       "2017-10-26  2.19   3       \n",
       "2017-10-25  2.15   3       \n",
       "2017-10-24  1.90   3       \n",
       "2017-10-18  1.90   3       \n",
       "2017-10-17  1.91   3       \n",
       "2017-10-16  2.19   3       \n",
       "2017-10-10  2.19   3       \n",
       "2017-10-09  2.47   3       \n",
       "2017-10-08  2.58   3       \n",
       "2017-10-07  2.10   3       \n",
       "2017-10-06  2.23   3       \n",
       "2017-10-05  2.29   3       \n",
       "2017-10-01  2.29   3       \n",
       "2017-09-30  2.57   3       \n",
       "2017-09-29  2.74   3       \n",
       "2017-09-28  3.02   3       \n",
       "2017-09-23  3.02   3       \n",
       "2017-09-22  3.07   3       \n",
       "2017-09-21  3.38   3       \n",
       "2017-09-10  3.38   3       \n",
       "2017-09-09  2.99   3       \n",
       "2017-09-08  3.88   3       \n",
       "2017-09-07  3.96   3       \n",
       "2017-08-11  3.96   3       \n",
       "2017-08-10  4.24   3       \n",
       "2017-08-09  4.35   3       \n",
       "2017-07-28  4.35   3       \n",
       "2017-07-27  4.02   3       \n",
       "2017-07-26  3.96   3       \n",
       "2017-07-25  3.96   3       \n",
       "2017-07-24  3.63   3       \n",
       "2017-07-23  3.57   3       \n",
       "2017-07-22  3.57   3       \n",
       "2017-07-21  2.98   3       \n",
       "2017-07-20  2.86   3       \n",
       "2017-07-09  2.86   3       \n",
       "2017-07-08  2.61   3       \n",
       "2017-07-07  2.57   3       \n",
       "2017-07-05  2.57   3       \n",
       "2017-07-04  2.41   3       \n",
       "2017-07-03  2.05   3       \n",
       "2017-07-02  1.83   3       \n",
       "2017-07-01  2.28   3       \n",
       "2017-06-29  2.53   3       \n",
       "2020-06-28  5.99   4       \n",
       "2020-05-05  5.99   4       \n",
       "2020-05-04  6.54   4       \n",
       "2020-05-03  6.72   4       \n",
       "2020-04-28  6.72   4       \n",
       "2020-04-27  6.39   4       \n",
       "2020-04-26  5.99   4       \n",
       "2020-04-25  5.99   4       \n",
       "2020-04-24  6.19   4       \n",
       "2020-04-23  6.79   4       \n",
       "2020-04-22  6.79   4       \n",
       "2020-04-21  7.89   4       \n",
       "2020-04-20  8.25   4       \n",
       "2020-04-11  8.25   4       \n",
       "2020-04-10  8.83   4       \n",
       "2020-04-09  8.98   4       \n",
       "2020-04-02  8.98   4       \n",
       "2020-04-01  8.45   4       \n",
       "2020-03-31  7.99   4       \n",
       "2020-03-30  7.99   4       \n",
       "2020-03-29  8.08   4       \n",
       "2020-03-28  8.99   4       \n",
       "2020-03-15  8.99   4       \n",
       "2020-03-14  8.95   4       \n",
       "2020-03-13  9.49   4       \n",
       "2020-03-12  9.99   4       \n",
       "2020-01-28  9.99   4       \n",
       "2020-01-27  9.78   4       \n",
       "2020-01-26  8.99   4       \n",
       "2020-01-25  9.12   4       \n",
       "2020-01-24  9.99   4       \n",
       "2020-01-18  9.99   4       \n",
       "2020-01-17  9.86   4       \n",
       "2020-01-16  8.99   4       \n",
       "2020-01-14  8.99   4       \n",
       "2020-01-13  9.57   4       \n",
       "2020-01-12  9.99   4       \n",
       "2019-12-25  9.99   4       \n",
       "2019-12-24  9.81   4       \n",
       "2019-12-23  9.69   4       \n",
       "2019-12-15  9.69   4       \n",
       "2019-12-14  10.61  4       \n",
       "2019-12-13  10.69  4       \n",
       "2019-12-12  10.44  4       \n",
       "2019-12-11  8.71   4       \n",
       "2019-11-30  8.71   4       \n",
       "2019-11-29  8.09   4       \n",
       "2019-11-28  7.72   4       \n",
       "2019-11-25  7.72   4       \n",
       "2019-11-24  8.18   4       \n",
       "2019-11-23  8.45   4       \n",
       "2019-11-22  8.86   4       \n",
       "2019-11-21  9.44   4       \n",
       "2019-11-20  8.85   4       \n",
       "2019-11-19  8.73   4       \n",
       "2019-11-18  8.73   4       \n",
       "2019-11-17  8.15   4       \n",
       "2019-11-16  8.00   4       \n",
       "2019-11-15  8.00   4       \n",
       "2019-11-14  7.42   4       \n",
       "2019-11-13  7.27   4       \n",
       "2019-11-12  7.27   4       \n",
       "2019-11-11  6.69   4       \n",
       "2019-11-10  6.54   4       \n",
       "2019-11-09  6.54   4       \n",
       "2019-11-08  6.07   4       \n",
       "2019-11-07  5.95   4       \n",
       "2019-11-06  5.95   4       \n",
       "2019-11-05  5.59   4       \n",
       "2019-11-04  5.49   4       \n",
       "2019-11-01  5.49   4       \n",
       "2019-10-31  5.12   4       \n",
       "2019-10-30  4.85   4       \n",
       "2019-10-29  4.85   4       \n",
       "2019-10-28  4.64   4       \n",
       "2019-10-27  4.39   4       \n",
       "2019-10-26  4.39   4       \n",
       "2019-10-25  4.02   4       \n",
       "2019-10-24  3.34   4       \n",
       "2019-10-23  3.08   4       \n",
       "2019-10-22  2.93   4       \n",
       "2019-10-21  2.79   4       \n",
       "2019-08-30  2.79   4       \n",
       "2019-08-29  2.73   4       \n",
       "2019-08-28  2.50   4       \n",
       "2019-08-27  2.58   4       \n",
       "2019-08-26  2.79   4       \n",
       "2019-07-01  2.79   4       \n",
       "2019-06-30  2.96   4       \n",
       "2019-06-29  3.06   4       \n",
       "2019-05-14  3.06   4       \n",
       "2019-05-13  2.99   4       \n",
       "2019-05-12  2.77   4       \n",
       "2019-05-11  2.77   4       \n",
       "2019-05-10  2.70   4       \n",
       "2019-05-09  2.48   4       \n",
       "2019-05-07  2.48   4       \n",
       "2019-05-06  2.37   4       \n",
       "2019-05-05  2.19   4       \n",
       "2019-05-04  2.19   4       \n",
       "2019-05-03  2.12   4       \n",
       "2019-05-02  1.97   4       \n",
       "2019-05-01  1.97   4       \n",
       "2019-04-30  1.82   4       \n",
       "2019-04-29  1.53   4       \n",
       "2019-04-17  1.53   4       \n",
       "2019-04-16  1.36   4       \n",
       "2019-04-15  1.31   4       \n",
       "2019-02-25  1.31   4       \n",
       "2019-02-24  1.16   4       \n",
       "2019-02-23  1.16   4       \n",
       "2019-02-22  1.19   4       \n",
       "2019-02-16  1.19   4       \n",
       "2019-02-15  1.43   4       \n",
       "2019-02-14  1.57   4       \n",
       "2019-02-05  1.57   4       \n",
       "2019-02-04  1.79   4       \n",
       "2019-01-11  1.79   4       \n",
       "2019-01-10  1.60   4       \n",
       "2019-01-09  1.57   4       \n",
       "2018-12-30  1.57   4       \n",
       "2018-12-29  1.59   4       \n",
       "2018-12-28  1.79   4       \n",
       "2018-12-22  1.79   4       \n",
       "2018-12-21  2.32   4       \n",
       "2018-12-20  2.58   4       \n",
       "2018-10-24  2.58   4       \n",
       "2018-10-23  2.75   4       \n",
       "2018-10-22  3.16   4       \n",
       "2018-09-26  3.16   4       \n",
       "2018-09-25  2.93   4       \n",
       "2018-09-24  2.80   4       \n",
       "2018-09-05  2.80   4       \n",
       "2018-09-04  2.69   4       \n",
       "2018-09-03  2.51   4       \n",
       "2018-06-14  2.51   4       \n",
       "2018-06-13  2.75   4       \n",
       "2018-06-12  2.80   4       \n",
       "2018-05-17  2.80   4       \n",
       "2018-05-16  2.96   4       \n",
       "2018-05-15  3.09   4       \n",
       "2018-04-10  3.09   4       \n",
       "2018-04-09  2.80   4       \n",
       "2018-04-08  2.73   4       \n",
       "2018-03-23  2.73   4       \n",
       "2018-03-22  2.80   4       \n",
       "2018-03-21  3.02   4       \n",
       "2018-02-21  3.02   4       \n",
       "2018-02-20  3.05   4       \n",
       "2018-02-19  3.41   4       \n",
       "2018-02-15  3.41   4       \n",
       "2018-02-14  3.36   4       \n",
       "2018-02-13  2.87   4       \n",
       "2018-02-12  3.05   4       \n",
       "2018-02-06  3.05   4       \n",
       "2018-02-05  2.93   4       \n",
       "2018-02-04  2.76   4       \n",
       "2018-01-21  2.76   4       \n",
       "2018-01-20  2.64   4       \n",
       "2018-01-19  2.18   4       \n",
       "2018-01-12  2.18   4       \n",
       "2018-01-11  2.06   4       \n",
       "2018-01-10  1.89   4       \n",
       "2018-01-09  1.89   4       \n",
       "2018-01-08  1.90   4       \n",
       "2018-01-07  1.71   4       \n",
       "2018-01-06  1.69   4       \n",
       "2017-12-28  1.69   4       \n",
       "2017-12-27  1.56   4       \n",
       "2017-12-26  1.47   4       \n",
       "2017-12-22  1.47   4       \n",
       "2017-12-21  1.67   4       \n",
       "2017-12-20  1.69   4       \n",
       "2017-12-17  1.69   4       \n",
       "2017-12-16  1.74   4       \n",
       "2017-12-15  1.56   4       \n",
       "2017-12-14  1.51   4       \n",
       "2017-12-13  1.69   4       \n",
       "2017-12-12  1.51   4       \n",
       "2017-12-11  1.47   4       \n",
       "2017-12-07  1.47   4       \n",
       "2017-12-06  1.63   4       \n",
       "2017-12-05  1.66   4       \n",
       "2017-12-04  1.27   4       \n",
       "2017-12-03  1.25   4       \n",
       "2017-12-02  1.25   4       \n",
       "2017-12-01  1.06   4       \n",
       "2017-11-30  1.11   4       \n",
       "2017-11-29  1.21   4       \n",
       "2017-11-28  1.12   4       \n",
       "2017-11-27  1.10   4       \n",
       "2017-11-26  1.00   4       \n",
       "2017-11-25  0.96   4       \n",
       "2017-11-24  0.99   4       \n",
       "2017-11-23  1.08   4       \n",
       "2017-11-22  1.02   4       \n",
       "2017-11-21  1.16   4       \n",
       "2017-11-20  1.31   4       \n",
       "2017-11-19  1.28   4       \n",
       "2017-11-18  1.31   4       \n",
       "2020-06-28  6.79   5       \n",
       "2020-04-25  6.79   5       \n",
       "2020-04-24  6.85   5       \n",
       "2020-04-23  7.04   5       \n",
       "2020-04-02  7.04   5       \n",
       "2020-04-01  6.67   5       \n",
       "2020-03-31  5.58   5       \n",
       "2020-03-30  5.41   5       \n",
       "2020-03-29  5.02   5       \n",
       "2020-03-28  5.53   5       \n",
       "2020-03-27  5.53   5       \n",
       "2020-03-26  5.56   5       \n",
       "2020-03-25  6.26   5       \n",
       "2020-03-24  5.68   5       \n",
       "2020-03-23  5.53   5       \n",
       "2020-03-22  5.53   5       \n",
       "2020-03-21  6.08   5       \n",
       "2020-03-20  6.53   5       \n",
       "2020-03-19  6.99   5       \n",
       "2020-03-13  6.99   5       \n",
       "2020-03-12  7.29   5       \n",
       "2020-03-11  7.72   5       \n",
       "2020-03-10  7.72   5       \n",
       "2020-03-09  8.44   5       \n",
       "2020-03-08  9.44   5       \n",
       "2020-02-22  9.44   5       \n",
       "2020-02-21  10.40  5       \n",
       "2020-02-20  10.44  5       \n",
       "2020-02-02  10.44  5       \n",
       "2020-02-01  11.40  5       \n",
       "2020-01-31  11.44  5       \n",
       "2019-11-25  11.44  5       \n",
       "2019-11-24  10.73  5       \n",
       "2019-11-23  10.44  5       \n",
       "2019-11-22  10.44  5       \n",
       "2019-11-21  9.73   5       \n",
       "2019-11-20  9.44   5       \n",
       "2019-11-19  9.44   5       \n",
       "2019-11-18  8.94   5       \n",
       "2019-11-17  8.73   5       \n",
       "2019-11-16  8.73   5       \n",
       "2019-11-15  8.21   5       \n",
       "2019-11-14  8.00   5       \n",
       "2019-11-13  8.00   5       \n",
       "2019-11-12  7.48   5       \n",
       "2019-11-11  7.27   5       \n",
       "2019-11-10  7.27   5       \n",
       "2019-11-09  6.75   5       \n",
       "2019-11-08  6.54   5       \n",
       "2019-11-07  6.54   5       \n",
       "2019-11-06  6.12   5       \n",
       "2019-11-05  5.95   5       \n",
       "2019-11-04  5.95   5       \n",
       "2019-11-03  5.68   5       \n",
       "2019-11-02  5.49   5       \n",
       "2019-11-01  5.49   5       \n",
       "2019-10-31  5.23   5       \n",
       "2019-10-30  4.85   5       \n",
       "2019-10-29  4.58   5       \n",
       "2019-10-28  4.58   5       \n",
       "2019-10-27  4.41   5       \n",
       "2019-10-26  4.19   5       \n",
       "2019-10-25  4.19   5       \n",
       "2019-10-24  4.27   5       \n",
       "2019-10-23  4.38   5       \n",
       "2019-07-03  4.38   5       \n",
       "2019-07-02  4.02   5       \n",
       "2019-07-01  3.99   5       \n",
       "2018-12-22  3.99   5       \n",
       "2018-12-21  4.14   5       \n",
       "2018-12-20  4.38   5       \n",
       "2018-08-24  4.38   5       \n",
       "2018-08-23  3.73   5       \n",
       "2018-08-22  3.60   5       \n",
       "2018-07-28  3.60   5       \n",
       "2018-07-27  3.63   5       \n",
       "2018-07-26  3.99   5       \n",
       "2018-06-18  3.99   5       \n",
       "2018-06-17  8.45   5       \n",
       "2018-06-16  9.94   5       \n",
       "2018-03-17  9.94   5       \n",
       "2018-03-16  10.19  5       \n",
       "2018-03-15  10.44  5       \n",
       "2018-02-28  10.44  5       \n",
       "2018-02-27  10.11  5       \n",
       "2018-02-26  9.44   5       \n",
       "2018-02-15  9.44   5       \n",
       "2018-02-14  9.38   5       \n",
       "2018-02-13  8.72   5       \n",
       "2017-10-06  8.72   5       \n",
       "2017-10-05  8.57   5       \n",
       "2017-10-04  7.99   5       \n",
       "2017-09-09  7.99   5       \n",
       "2017-09-08  10.74  5       \n",
       "2017-06-29  10.99  5       \n",
       "2018-09-06  1.69   3       \n",
       "2018-08-11  1.69   3       \n",
       "2018-08-10  1.53   3       \n",
       "2018-08-09  1.51   3       \n",
       "2018-08-08  1.71   3       \n",
       "2018-07-30  1.71   3       \n",
       "2018-07-29  1.93   3       \n",
       "2018-07-28  1.83   3       \n",
       "2018-07-27  1.67   3       \n",
       "2018-07-26  1.69   3       \n",
       "2018-07-25  1.89   3       \n",
       "2018-07-22  1.89   3       \n",
       "2018-07-21  1.50   3       \n",
       "2018-07-20  1.45   3       \n",
       "2018-07-16  1.45   3       \n",
       "2018-07-15  1.67   3       \n",
       "2018-07-14  1.67   3       \n",
       "2018-07-13  1.87   3       \n",
       "2018-07-12  2.15   3       \n",
       "2018-07-11  2.15   3       \n",
       "2018-07-10  2.09   3       \n",
       "2020-07-17  3.46   0       \n",
       "2020-06-29  3.58   0       \n",
       "2020-06-29  7.06   1       \n",
       "2020-06-29  6.28   2       \n",
       "2020-06-29  5.28   3       \n",
       "2020-06-29  5.99   4       \n",
       "2020-06-29  6.79   5       \n",
       "2020-07-16  3.61   0       \n",
       "2020-07-15  3.37   0       \n",
       "2020-07-14  3.48   0       \n",
       "2020-07-13  3.01   0       \n",
       "2020-07-12  2.35   0       \n",
       "2020-07-11  2.52   0       \n",
       "2020-07-10  2.66   0       \n",
       "2020-07-09  2.81   0       \n",
       "2020-07-08  2.87   0       \n",
       "2020-07-07  2.92   0       \n",
       "2020-07-06  3.25   0       \n",
       "2020-07-05  3.45   0       \n",
       "2020-07-04  3.20   0       \n",
       "2020-07-03  3.21   0       \n",
       "2020-07-02  3.23   0       \n",
       "2020-07-01  3.35   0       \n",
       "2020-06-30  3.71   0       \n",
       "2020-07-17  5.93   1       \n",
       "2020-07-16  6.11   1       \n",
       "2020-07-13  6.11   1       \n",
       "2020-07-12  6.09   1       \n",
       "2020-07-11  5.82   1       \n",
       "2020-07-10  6.13   1       \n",
       "2020-07-09  6.40   1       \n",
       "2020-07-08  6.41   1       \n",
       "2020-07-07  6.50   1       \n",
       "2020-07-03  6.50   1       \n",
       "2020-07-02  6.64   1       \n",
       "2020-07-01  6.92   1       \n",
       "2020-06-30  7.06   1       \n",
       "2020-07-17  5.59   2       \n",
       "2020-07-16  6.12   2       \n",
       "2020-07-15  6.28   2       \n",
       "2020-07-17  5.28   3       \n",
       "2020-07-17  5.99   4       \n",
       "2020-07-17  6.79   5       \n",
       "2020-08-27  3.91   0       \n",
       "2020-08-26  3.90   0       \n",
       "2020-08-25  4.25   0       \n",
       "2020-08-24  4.07   0       \n",
       "2020-08-23  4.30   0       \n",
       "2020-08-22  4.27   0       \n",
       "2020-08-21  4.27   0       \n",
       "2020-08-20  4.06   0       \n",
       "2020-08-19  3.77   0       \n",
       "2020-08-18  3.96   0       \n",
       "2020-08-17  4.42   0       \n",
       "2020-08-16  4.17   0       \n",
       "2020-08-15  3.67   0       \n",
       "2020-08-14  3.39   0       \n",
       "2020-08-13  3.70   0       \n",
       "2020-08-12  3.77   0       \n",
       "2020-08-11  3.48   0       \n",
       "2020-08-10  3.56   0       \n",
       "2020-08-09  3.40   0       \n",
       "2020-08-08  4.86   0       \n",
       "2020-08-07  4.98   0       \n",
       "2020-08-06  4.82   0       \n",
       "2020-08-05  4.75   0       \n",
       "2020-08-04  4.53   0       \n",
       "2020-08-03  2.81   0       \n",
       "2020-08-02  2.78   0       \n",
       "2020-08-01  2.98   0       \n",
       "2020-07-31  3.26   0       \n",
       "2020-07-30  3.29   0       \n",
       "2020-07-29  3.34   0       \n",
       "2020-07-28  3.21   0       \n",
       "2020-07-27  3.37   0       \n",
       "2020-07-26  3.27   0       \n",
       "2020-07-25  2.97   0       \n",
       "2020-07-24  2.84   0       \n",
       "2020-07-23  2.68   0       \n",
       "2020-07-22  2.78   0       \n",
       "2020-07-21  2.84   0       \n",
       "2020-07-20  3.08   0       \n",
       "2020-07-19  3.13   0       \n",
       "2020-07-18  3.31   0       \n",
       "2020-08-27  6.40   1       \n",
       "2020-08-26  6.36   1       \n",
       "2020-08-25  6.36   1       \n",
       "2020-08-24  6.91   1       \n",
       "2020-08-23  6.57   1       \n",
       "2020-08-22  6.46   1       \n",
       "2020-08-20  6.46   1       \n",
       "2020-08-19  6.48   1       \n",
       "2020-08-18  6.49   1       \n",
       "2020-08-17  6.49   1       \n",
       "2020-08-16  6.41   1       \n",
       "2020-08-15  6.02   1       \n",
       "2020-08-12  6.02   1       \n",
       "2020-08-11  6.25   1       \n",
       "2020-08-10  6.68   1       \n",
       "2020-08-09  6.72   1       \n",
       "2020-08-08  6.78   1       \n",
       "2020-08-07  6.22   1       \n",
       "2020-08-06  6.18   1       \n",
       "2020-08-05  6.18   1       \n",
       "2020-08-04  5.78   1       \n",
       "2020-08-03  5.32   1       \n",
       "2020-08-02  5.30   1       \n",
       "2020-08-01  5.40   1       \n",
       "2020-07-31  5.54   1       \n",
       "2020-07-29  5.54   1       \n",
       "2020-07-28  5.60   1       \n",
       "2020-07-27  5.79   1       \n",
       "2020-07-26  5.42   1       \n",
       "2020-07-25  5.06   1       \n",
       "2020-07-24  5.28   1       \n",
       "2020-07-23  5.34   1       \n",
       "2020-07-22  5.14   1       \n",
       "2020-07-21  5.47   1       \n",
       "2020-07-20  5.76   1       \n",
       "2020-07-19  5.82   1       \n",
       "2020-07-18  5.82   1       \n",
       "2020-08-27  5.74   2       \n",
       "2020-08-26  6.04   2       \n",
       "2020-08-21  6.04   2       \n",
       "2020-08-20  6.05   2       \n",
       "2020-08-19  6.12   2       \n",
       "2020-08-17  6.12   2       \n",
       "2020-08-16  6.06   2       \n",
       "2020-08-15  5.92   2       \n",
       "2020-08-13  5.92   2       \n",
       "2020-08-12  5.83   2       \n",
       "2020-08-10  5.83   2       \n",
       "2020-08-09  5.76   2       \n",
       "2020-08-08  5.66   2       \n",
       "2020-08-07  5.37   2       \n",
       "2020-08-06  5.13   2       \n",
       "2020-08-05  5.09   2       \n",
       "2020-08-04  4.76   2       \n",
       "2020-08-03  3.89   2       \n",
       "2020-08-02  3.97   2       \n",
       "2020-08-01  4.02   2       \n",
       "2020-07-31  4.10   2       \n",
       "2020-07-29  4.10   2       \n",
       "2020-07-28  4.08   2       \n",
       "2020-07-27  4.07   2       \n",
       "2020-07-26  4.06   2       \n",
       "2020-07-25  4.17   2       \n",
       "2020-07-24  4.24   2       \n",
       "2020-07-23  4.28   2       \n",
       "2020-07-22  4.28   2       \n",
       "2020-07-21  4.49   2       \n",
       "2020-07-20  5.00   2       \n",
       "2020-07-19  5.18   2       \n",
       "2020-07-18  5.20   2       \n",
       "2020-08-27  4.45   3       \n",
       "2020-08-26  4.45   3       \n",
       "2020-08-25  4.87   3       \n",
       "2020-08-24  4.91   3       \n",
       "2020-08-10  4.91   3       \n",
       "2020-08-09  4.64   3       \n",
       "2020-08-08  4.45   3       \n",
       "2020-08-01  4.45   3       \n",
       "2020-07-31  4.43   3       \n",
       "2020-07-30  3.99   3       \n",
       "2020-07-24  3.99   3       \n",
       "2020-07-23  4.47   3       \n",
       "2020-07-22  4.82   3       \n",
       "2020-07-21  5.17   3       \n",
       "2020-07-20  5.28   3       \n",
       "2020-08-27  4.87   4       \n",
       "2020-08-02  4.87   4       \n",
       "2020-08-01  5.06   4       \n",
       "2020-07-31  5.33   4       \n",
       "2020-07-29  5.33   4       \n",
       "2020-07-28  5.77   4       \n",
       "2020-07-27  5.99   4       \n",
       "2020-08-27  6.79   5       \n",
       "2020-09-21  2.70   0       \n",
       "2020-09-12  5.39   2       \n",
       "2020-09-11  5.44   2       \n",
       "2020-09-10  5.44   2       \n",
       "2020-09-09  5.59   2       \n",
       "2020-09-08  5.90   2       \n",
       "2020-08-31  5.90   2       \n",
       "2020-08-30  5.89   2       \n",
       "2020-08-29  5.79   2       \n",
       "2020-08-28  5.79   2       \n",
       "2020-09-21  3.99   3       \n",
       "2020-09-03  3.99   3       \n",
       "2020-09-02  4.20   3       \n",
       "2020-09-01  4.45   3       \n",
       "2020-09-21  4.87   4       \n",
       "2020-09-21  6.79   5       \n",
       "2020-09-20  2.68   0       \n",
       "2020-09-19  2.93   0       \n",
       "2020-09-18  2.97   0       \n",
       "2020-09-17  2.84   0       \n",
       "2020-09-16  3.19   0       \n",
       "2020-09-15  3.22   0       \n",
       "2020-09-14  3.16   0       \n",
       "2020-09-13  3.18   0       \n",
       "2020-09-12  3.31   0       \n",
       "2020-09-11  3.09   0       \n",
       "2020-09-10  2.86   0       \n",
       "2020-09-09  2.75   0       \n",
       "2020-09-08  2.95   0       \n",
       "2020-09-07  3.10   0       \n",
       "2020-09-06  3.32   0       \n",
       "2020-09-05  3.08   0       \n",
       "2020-09-04  2.88   0       \n",
       "2020-09-03  2.81   0       \n",
       "2020-09-02  3.14   0       \n",
       "2020-09-01  3.24   0       \n",
       "2020-08-31  3.79   0       \n",
       "2020-08-30  3.97   0       \n",
       "2020-08-29  4.06   0       \n",
       "2020-08-28  4.08   0       \n",
       "2020-09-21  5.39   1       \n",
       "2020-09-20  5.40   1       \n",
       "2020-09-19  5.44   1       \n",
       "2020-09-18  5.52   1       \n",
       "2020-09-17  5.45   1       \n",
       "2020-09-16  5.37   1       \n",
       "2020-09-15  5.35   1       \n",
       "2020-09-14  5.86   1       \n",
       "2020-09-13  5.95   1       \n",
       "2020-09-12  6.10   1       \n",
       "2020-09-11  6.13   1       \n",
       "2020-09-10  6.13   1       \n",
       "2020-09-09  6.17   1       \n",
       "2020-09-08  6.20   1       \n",
       "2020-09-07  6.20   1       \n",
       "2020-09-06  6.38   1       \n",
       "2020-09-03  6.38   1       \n",
       "2020-09-02  6.37   1       \n",
       "2020-09-01  6.36   1       \n",
       "2020-08-31  6.48   1       \n",
       "2020-08-30  6.42   1       \n",
       "2020-08-29  6.46   1       \n",
       "2020-08-28  6.43   1       \n",
       "2020-09-21  5.35   2       \n",
       "2020-09-19  5.35   2       \n",
       "2020-09-18  5.34   2       \n",
       "2020-09-17  5.29   2       \n",
       "2020-09-16  5.37   2       \n",
       "2020-09-15  5.36   2       \n",
       "2020-09-13  5.36   2       \n",
       "2020-09-29  2.80   0       \n",
       "2020-09-28  2.87   0       \n",
       "2020-09-27  3.11   0       \n",
       "2020-09-26  2.98   0       \n",
       "2020-09-25  2.84   0       \n",
       "2020-09-24  2.56   0       \n",
       "2020-09-23  2.60   0       \n",
       "2020-09-22  2.68   0       \n",
       "2020-09-29  5.49   1       \n",
       "2020-09-28  5.50   1       \n",
       "2020-09-27  5.49   1       \n",
       "2020-09-26  5.46   1       \n",
       "2020-09-25  5.43   1       \n",
       "2020-09-23  5.43   1       \n",
       "2020-09-22  5.39   1       \n",
       "2020-09-29  4.93   2       \n",
       "2020-09-28  4.93   2       \n",
       "2020-09-27  4.95   2       \n",
       "2020-09-26  5.43   2       \n",
       "2020-09-25  5.36   2       \n",
       "2020-09-23  5.36   2       \n",
       "2020-09-22  5.35   2       \n",
       "2020-09-29  3.99   3       \n",
       "2020-09-29  4.99   4       \n",
       "2020-09-28  4.87   4       \n",
       "2020-09-29  6.79   5       "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goatdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('''SELECT carddate,AVG(pshipq) as AVGE,percentile_cont(0.5) WITHIN GROUP (ORDER BY pshipq) as MED\n",
    "    FROM (SELECT DISTINCT title,cardname,carddate,(price+shipping)/COALESCE(cardquantity,maybecardquantity,1) as pshipq FROM public.transactions\n",
    "\tWHERE possiblybad IS FALSE \n",
    "    AND cardname ILIKE %s\n",
    "\tAND isfoil IS FALSE\n",
    "\tAND (cardlanguage='english' or cardlanguage IS NULL)\n",
    "\tAND cardset NOT IN('LEA','LEB','U')\n",
    "\tAND (isemblem IS NULL OR isemblem IS FALSE)\n",
    "\tAND (lotis IS FALSE OR lotis IS NULL)\n",
    "\tAND (isboosterbox IS NULL OR isboosterbox IS FALSE)\n",
    "\tAND cardspecial IS NULL\n",
    "\tAND issleeve IS NULL\n",
    "\tAND isdeck IS NULL\n",
    "\tAND isplaymat IS NULL\n",
    "\tAND isultra IS NULL\n",
    "\tAND (istoken IS NULL OR istoken IS FALSE)\n",
    "\tAND (iscommanderdeck IS NULL OR iscommanderdeck IS FALSE)\n",
    "\tAND isgraded IS NULL\n",
    "\tAND (othercards IS NULL OR transplit IS TRUE)\n",
    "\tAND saletype IN('normal')\n",
    "\tAND price+shipping<19) as foo GROUP BY carddate ORDER BY carddate ASC''',(analyname,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebayrows=cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ebaydf=pd.DataFrame(ebayrows,columns=[\"carddate\",\"avg\",\"med\"]) \n",
    "ebaydf=pd.DataFrame(ebayrows,columns=[\"carddate\",\"AVG\",\"MED\"]).set_index(\"carddate\")\n",
    "ebaydf['MINAVGMED']=ebaydf.apply(lambda row: np.minimum(row.AVG,row.MED), axis = 1)\n",
    "ebaydf=ebaydf.drop(columns=['AVG','MED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            MINAVGMED\n",
      "carddate             \n",
      "2019-10-21  5.245000 \n",
      "2019-10-22  6.000000 \n",
      "2019-10-23  5.290000 \n",
      "2019-10-24  6.295000 \n",
      "2019-10-25  6.118000 \n",
      "2019-10-26  8.940001 \n",
      "2019-10-27  7.243333 \n",
      "2019-10-28  6.833333 \n",
      "2019-10-30  6.880000 \n",
      "2019-10-31  5.485000 \n",
      "2019-11-01  7.175000 \n",
      "2019-11-02  7.620000 \n",
      "2019-11-03  7.820000 \n",
      "2019-11-04  7.475000 \n",
      "2019-11-05  8.495000 \n",
      "2019-11-06  9.520000 \n",
      "2019-11-07  7.923333 \n",
      "2019-11-08  7.755000 \n",
      "2019-11-11  10.680000\n",
      "2019-11-12  9.186250 \n",
      "2019-11-13  10.340000\n",
      "2019-11-14  10.790000\n",
      "2019-11-15  9.811666 \n",
      "2019-11-16  10.059999\n",
      "2019-11-17  10.495000\n",
      "2019-11-18  11.377778\n",
      "2019-11-19  11.677500\n",
      "2019-11-21  11.790000\n",
      "2019-11-22  10.300000\n",
      "2019-11-23  10.000000\n",
      "2019-11-24  4.330000 \n",
      "2019-11-25  11.763333\n",
      "2019-11-26  9.990000 \n",
      "2019-11-28  11.975000\n",
      "2019-11-29  11.000000\n",
      "2019-11-30  11.506000\n",
      "2019-12-01  14.480000\n",
      "2019-12-02  12.740000\n",
      "2019-12-03  11.000000\n",
      "2019-12-04  11.790000\n",
      "2019-12-05  11.490000\n",
      "2019-12-07  11.590000\n",
      "2019-12-09  11.576667\n",
      "2019-12-10  13.500000\n",
      "2019-12-11  10.882500\n",
      "2019-12-12  11.990000\n",
      "2019-12-13  12.485000\n",
      "2019-12-15  11.420000\n",
      "2019-12-17  12.236000\n",
      "2019-12-18  11.990000\n",
      "2019-12-20  12.750000\n",
      "2019-12-21  11.355000\n",
      "2019-12-22  12.890000\n",
      "2019-12-23  13.745000\n",
      "2019-12-26  12.559999\n",
      "2019-12-27  12.750000\n",
      "2019-12-28  14.996667\n",
      "2019-12-30  13.820000\n",
      "2019-12-31  14.337500\n",
      "2020-01-02  14.990000\n",
      "2020-01-03  14.440001\n",
      "2020-01-04  13.000000\n",
      "2020-01-05  12.375000\n",
      "2020-01-06  14.500000\n",
      "2020-01-07  12.500000\n",
      "2020-01-08  12.750000\n",
      "2020-01-09  12.000000\n",
      "2020-01-10  14.385000\n",
      "2020-01-11  11.740000\n",
      "2020-01-12  12.646000\n",
      "2020-01-13  15.000000\n",
      "2020-01-14  12.333333\n",
      "2020-01-15  11.485000\n",
      "2020-01-17  13.500000\n",
      "2020-01-18  12.500000\n",
      "2020-01-19  11.495000\n",
      "2020-01-20  13.250000\n",
      "2020-01-21  12.500000\n",
      "2020-01-22  11.738000\n",
      "2020-01-23  11.990000\n",
      "2020-01-24  10.500000\n",
      "2020-01-25  13.250000\n",
      "2020-01-26  12.000000\n",
      "2020-01-27  10.500000\n",
      "2020-01-28  12.000000\n",
      "2020-01-29  12.940000\n",
      "2020-01-30  13.000000\n",
      "2020-02-01  9.500000 \n",
      "2020-02-02  11.657500\n",
      "2020-02-03  13.500000\n",
      "2020-02-04  12.960000\n",
      "2020-02-05  12.160000\n",
      "2020-02-06  9.655000 \n",
      "2020-02-07  12.600000\n",
      "2020-02-09  11.000000\n",
      "2020-02-11  12.990000\n",
      "2020-02-12  11.000000\n",
      "2020-02-14  13.950000\n",
      "2020-02-15  13.350000\n",
      "2020-02-16  12.033333\n",
      "2020-02-17  14.110000\n",
      "2020-02-19  14.990000\n",
      "2020-02-22  12.116667\n",
      "2020-02-24  12.000000\n",
      "2020-02-25  10.500000\n",
      "2020-02-29  11.500000\n",
      "2020-03-01  11.990000\n",
      "2020-03-02  12.000000\n",
      "2020-03-04  10.547500\n",
      "2020-03-05  17.189999\n",
      "2020-03-06  10.012500\n",
      "2020-03-07  11.900000\n",
      "2020-03-08  11.490000\n",
      "2020-03-09  9.615000 \n",
      "2020-03-10  13.000000\n",
      "2020-03-11  10.615000\n",
      "2020-03-13  8.285000 \n",
      "2020-03-14  8.330000 \n",
      "2020-03-15  9.190000 \n",
      "2020-03-16  6.500000 \n",
      "2020-03-17  8.400000 \n",
      "2020-03-20  6.700000 \n",
      "2020-03-21  9.720000 \n",
      "2020-03-22  7.950000 \n",
      "2020-03-23  7.280000 \n",
      "2020-03-24  5.150000 \n",
      "2020-03-29  6.980000 \n",
      "2020-03-30  7.372500 \n",
      "2020-03-31  5.500000 \n",
      "2020-04-01  7.000000 \n",
      "2020-04-02  6.750000 \n",
      "2020-04-03  6.860000 \n",
      "2020-04-05  6.702500 \n",
      "2020-04-06  7.743333 \n",
      "2020-04-08  6.000000 \n",
      "2020-04-09  9.226000 \n",
      "2020-04-10  6.750000 \n",
      "2020-04-11  6.250000 \n",
      "2020-04-12  5.990000 \n",
      "2020-04-15  6.745000 \n",
      "2020-04-16  6.800000 \n",
      "2020-04-17  5.670000 \n",
      "2020-04-18  7.750000 \n",
      "2020-04-19  8.225000 \n",
      "2020-04-20  7.005000 \n",
      "2020-04-21  7.000000 \n",
      "2020-04-22  6.990000 \n",
      "2020-04-23  8.750000 \n",
      "2020-04-24  6.695000 \n",
      "2020-04-25  6.225000 \n",
      "2020-04-26  6.990000 \n",
      "2020-04-27  6.262000 \n",
      "2020-04-28  8.100000 \n",
      "2020-04-29  5.990000 \n",
      "2020-04-30  6.390000 \n",
      "2020-05-01  6.830833 \n",
      "2020-05-02  9.135000 \n",
      "2020-05-03  8.515000 \n",
      "2020-05-04  5.605000 \n",
      "2020-05-05  7.740000 \n",
      "2020-05-06  7.615000 \n",
      "2020-05-07  7.000000 \n",
      "2020-05-08  7.830000 \n",
      "2020-05-10  7.095000 \n",
      "2020-05-12  8.290000 \n",
      "2020-05-13  8.370000 \n",
      "2020-05-14  11.990000\n",
      "2020-05-15  9.975000 \n",
      "2020-05-16  6.483333 \n",
      "2020-05-17  7.330000 \n",
      "2020-05-18  5.870000 \n",
      "2020-05-19  6.800000 \n",
      "2020-05-20  9.970000 \n",
      "2020-05-21  5.750000 \n",
      "2020-05-22  9.655000 \n",
      "2020-05-23  5.970000 \n",
      "2020-05-24  6.495000 \n",
      "2020-05-25  7.500000 \n",
      "2020-05-27  5.990000 \n",
      "2020-05-28  7.000000 \n",
      "2020-05-29  5.915000 \n",
      "2020-06-01  7.215000 \n",
      "2020-06-02  6.890000 \n",
      "2020-06-03  8.210000 \n",
      "2020-06-04  7.005000 \n",
      "2020-06-05  6.250000 \n",
      "2020-06-06  6.990000 \n",
      "2020-06-07  6.933333 \n",
      "2020-06-08  7.220000 \n",
      "2020-06-09  6.990000 \n",
      "2020-06-10  6.307500 \n",
      "2020-06-11  6.245000 \n",
      "2020-06-14  4.240000 \n",
      "2020-06-15  7.745000 \n",
      "2020-06-16  6.370000 \n",
      "2020-06-19  1.750000 \n",
      "2020-06-20  2.990000 \n",
      "2020-06-21  4.990000 \n",
      "2020-06-23  6.740000 \n",
      "2020-06-25  4.690000 \n",
      "2020-06-26  5.500000 \n",
      "2020-06-27  5.990000 \n",
      "2020-06-28  3.750000 \n",
      "2020-06-29  4.995000 \n",
      "2020-07-01  5.000000 \n",
      "2020-07-03  6.240000 \n",
      "2020-07-04  5.970000 \n",
      "2020-07-06  5.122500 \n",
      "2020-07-08  2.000000 \n",
      "2020-07-10  3.787500 \n",
      "2020-07-11  6.385000 \n",
      "2020-07-14  5.120000 \n",
      "2020-07-15  5.490000 \n",
      "2020-07-16  8.465000 \n",
      "2020-07-17  6.280000 \n",
      "2020-07-19  7.870000 \n",
      "2020-07-20  5.870000 \n",
      "2020-07-23  5.950000 \n",
      "2020-07-24  7.250000 \n",
      "2020-07-25  6.775000 \n",
      "2020-07-26  5.500000 \n",
      "2020-07-27  5.485000 \n",
      "2020-07-30  6.500000 \n",
      "2020-07-31  6.960000 \n",
      "2020-08-02  6.390000 \n",
      "2020-08-04  5.736667 \n",
      "2020-08-06  5.480000 \n",
      "2020-08-07  5.490000 \n",
      "2020-08-08  5.331667 \n",
      "2020-08-12  5.990000 \n",
      "2020-08-13  4.750000 \n",
      "2020-08-14  9.740000 \n",
      "2020-08-16  2.785000 \n",
      "2020-08-18  5.965000 \n",
      "2020-08-22  6.040000 \n",
      "2020-08-23  7.325000 \n",
      "2020-08-25  7.562500 \n",
      "2020-08-26  6.000000 \n",
      "2020-08-27  5.197500 \n",
      "2020-08-29  6.000000 \n",
      "2020-08-30  6.580000 \n",
      "2020-09-01  6.990000 \n",
      "2020-09-02  5.990000 \n",
      "2020-09-03  6.540000 \n",
      "2020-09-04  6.250000 \n",
      "2020-09-05  6.025000 \n",
      "2020-09-06  4.275000 \n",
      "2020-09-07  8.000000 \n",
      "2020-09-09  8.000000 \n",
      "2020-09-10  6.875000 \n",
      "2020-09-11  6.245000 \n",
      "2020-09-14  10.000000\n",
      "2020-09-16  5.975000 \n",
      "2020-09-17  5.750000 \n",
      "2020-09-18  5.500000 \n",
      "2020-09-20  5.915000 \n",
      "2020-09-21  7.910000 \n",
      "2020-09-22  6.140000 \n",
      "2020-09-26  5.990000 \n",
      "2020-09-28  2.925000 \n",
      "2020-09-30  4.990000 \n",
      "2020-10-01  5.750000 \n"
     ]
    }
   ],
   "source": [
    "print(ebaydf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('''SELECT entrydate1 as entrydate,SUM(cardquant)/AVG(aggcount) as ratio FROM(\n",
    "SELECT DISTINCT cardname0,entrydate1,cardquant,aggcount,tourntitle FROM\n",
    "(SELECT cardname as cardname0,entrydate as entrydate0,cardquant,tourntitle FROM public.tourninfo WHERE cardname ILIKE %s) AS foo1,\n",
    "(SELECT entrydate as entrydate1,SUM(cardquant) as aggcount FROM public.tourninfo GROUP BY entrydate) AS bar\n",
    "WHERE entrydate0=entrydate1) AS foo\n",
    "GROUP BY cardname0,entrydate1''',(analyname,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournrows=cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourndf=pd.DataFrame(tournrows,columns=[\"carddate\",\"ratio\"]).set_index(\"carddate\")\n",
    "tourndf=tourndf.astype({'ratio': 'float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ratio\n",
      "carddate            \n",
      "2019-10-27  0.003750\n",
      "2019-10-28  0.006046\n",
      "2019-10-31  0.008246\n",
      "2019-11-01  0.001667\n",
      "2019-11-03  0.005000\n",
      "2019-11-04  0.005972\n",
      "2019-11-07  0.005557\n",
      "2019-11-09  0.005838\n",
      "2019-11-10  0.009163\n",
      "2019-11-11  0.005086\n",
      "2019-11-14  0.006987\n",
      "2019-11-15  0.011250\n",
      "2019-11-17  0.003769\n",
      "2019-11-18  0.005506\n",
      "2019-11-21  0.005520\n",
      "2019-11-24  0.007222\n",
      "2019-11-25  0.007543\n",
      "2019-11-26  0.007923\n",
      "2019-11-28  0.005228\n",
      "2019-11-29  0.007503\n",
      "2019-11-30  0.003333\n",
      "2019-12-01  0.002271\n",
      "2019-12-02  0.003489\n",
      "2019-12-05  0.006306\n",
      "2019-12-07  0.005000\n",
      "2019-12-08  0.010014\n",
      "2019-12-10  0.004680\n",
      "2019-12-12  0.005186\n",
      "2019-12-14  0.002667\n",
      "2019-12-15  0.011314\n",
      "2019-12-16  0.007181\n",
      "2019-12-17  0.019210\n",
      "2019-12-18  0.011429\n",
      "2019-12-19  0.004089\n",
      "2019-12-20  0.001905\n",
      "2019-12-21  0.011439\n",
      "2019-12-22  0.010797\n",
      "2019-12-23  0.004660\n",
      "2019-12-24  0.003955\n",
      "2019-12-25  0.005333\n",
      "2019-12-26  0.002945\n",
      "2019-12-27  0.001250\n",
      "2019-12-28  0.005330\n",
      "2019-12-29  0.004167\n",
      "2019-12-30  0.002291\n",
      "2019-12-31  0.000702\n",
      "2020-01-01  0.001876\n",
      "2020-01-02  0.002001\n",
      "2020-01-03  0.005614\n",
      "2020-01-04  0.006219\n",
      "2020-01-05  0.009184\n",
      "2020-01-06  0.003828\n",
      "2020-01-07  0.005492\n",
      "2020-01-08  0.007304\n",
      "2020-01-09  0.003891\n",
      "2020-01-10  0.006377\n",
      "2020-01-11  0.002857\n",
      "2020-01-12  0.006155\n",
      "2020-01-13  0.004002\n",
      "2020-01-14  0.007622\n",
      "2020-01-15  0.005060\n",
      "2020-01-16  0.006832\n",
      "2020-01-17  0.005591\n",
      "2020-01-18  0.009412\n",
      "2020-01-19  0.009170\n",
      "2020-01-20  0.005265\n",
      "2020-01-21  0.005641\n",
      "2020-01-22  0.002353\n",
      "2020-01-23  0.003791\n",
      "2020-01-24  0.007083\n",
      "2020-01-25  0.005869\n",
      "2020-01-26  0.012358\n",
      "2020-01-27  0.004243\n",
      "2020-01-28  0.009905\n",
      "2020-01-29  0.005336\n",
      "2020-01-30  0.006325\n",
      "2020-02-01  0.005725\n",
      "2020-02-02  0.006091\n",
      "2020-02-03  0.005079\n",
      "2020-02-04  0.004713\n",
      "2020-02-06  0.006667\n",
      "2020-02-07  0.003337\n",
      "2020-02-08  0.003498\n",
      "2020-02-09  0.004469\n",
      "2020-02-10  0.004199\n",
      "2020-02-12  0.003333\n",
      "2020-02-13  0.003811\n",
      "2020-02-14  0.003639\n",
      "2020-02-15  0.002473\n",
      "2020-02-16  0.002666\n",
      "2020-02-17  0.004693\n",
      "2020-02-18  0.008000\n",
      "2020-02-19  0.003556\n",
      "2020-02-20  0.003334\n",
      "2020-02-22  0.003810\n",
      "2020-02-23  0.001994\n",
      "2020-02-24  0.002899\n",
      "2020-02-25  0.002668\n",
      "2020-02-27  0.004241\n",
      "2020-02-28  0.001975\n",
      "2020-02-29  0.002667\n",
      "2020-03-01  0.002435\n",
      "2020-03-02  0.002994\n",
      "2020-03-04  0.000494\n",
      "2020-03-05  0.004127\n",
      "2020-03-06  0.002319\n",
      "2020-03-07  0.001212\n",
      "2020-03-08  0.002919\n",
      "2020-03-09  0.005786\n",
      "2020-03-10  0.002133\n",
      "2020-03-11  0.007312\n",
      "2020-03-12  0.005458\n",
      "2020-03-13  0.006126\n",
      "2020-03-14  0.008757\n",
      "2020-03-15  0.003810\n",
      "2020-03-16  0.002581\n",
      "2020-03-17  0.003200\n",
      "2020-03-18  0.002020\n",
      "2020-03-19  0.003165\n",
      "2020-03-20  0.001778\n",
      "2020-03-21  0.002083\n",
      "2020-03-22  0.001667\n",
      "2020-03-23  0.002754\n",
      "2020-03-26  0.002910\n",
      "2020-03-28  0.002597\n",
      "2020-03-30  0.002549\n",
      "2020-03-31  0.001441\n",
      "2020-04-02  0.003972\n",
      "2020-04-04  0.003158\n",
      "2020-04-06  0.004267\n",
      "2020-04-07  0.003619\n",
      "2020-04-08  0.001481\n",
      "2020-04-09  0.003200\n",
      "2020-04-10  0.000556\n",
      "2020-04-11  0.002667\n",
      "2020-04-13  0.004444\n",
      "2020-04-16  0.002370\n",
      "2020-04-17  0.001767\n",
      "2020-04-20  0.004063\n",
      "2020-04-21  0.004678\n",
      "2020-04-22  0.002388\n",
      "2020-04-23  0.010000\n",
      "2020-04-24  0.004425\n",
      "2020-04-25  0.006202\n",
      "2020-04-26  0.003917\n",
      "2020-04-27  0.007182\n",
      "2020-04-28  0.009877\n",
      "2020-04-29  0.002853\n",
      "2020-04-30  0.009864\n",
      "2020-05-01  0.005427\n",
      "2020-05-02  0.006250\n",
      "2020-05-03  0.004753\n",
      "2020-05-04  0.006190\n",
      "2020-05-05  0.002332\n",
      "2020-05-06  0.007238\n",
      "2020-05-07  0.008254\n",
      "2020-05-08  0.004853\n",
      "2020-05-09  0.006299\n",
      "2020-05-10  0.008847\n",
      "2020-05-11  0.010591\n",
      "2020-05-12  0.011673\n",
      "2020-05-13  0.009206\n",
      "2020-05-14  0.015054\n",
      "2020-05-15  0.009061\n",
      "2020-05-16  0.010448\n",
      "2020-05-17  0.013600\n",
      "2020-05-18  0.011435\n",
      "2020-05-19  0.006780\n",
      "2020-05-20  0.006604\n",
      "2020-05-21  0.011828\n",
      "2020-05-22  0.011765\n",
      "2020-05-23  0.009612\n",
      "2020-05-24  0.011019\n",
      "2020-05-25  0.004350\n",
      "2020-05-26  0.009394\n",
      "2020-05-28  0.008832\n",
      "2020-05-29  0.008594\n",
      "2020-05-30  0.013740\n",
      "2020-06-01  0.011273\n",
      "2020-06-03  0.009249\n",
      "2020-06-04  0.004619\n",
      "2020-06-05  0.006978\n",
      "2020-06-06  0.009127\n",
      "2020-06-07  0.006048\n",
      "2020-06-08  0.003941\n",
      "2020-06-09  0.002575\n",
      "2020-06-11  0.008520\n",
      "2020-06-13  0.004878\n",
      "2020-06-14  0.003689\n",
      "2020-06-15  0.008790\n",
      "2020-06-16  0.005594\n",
      "2020-06-17  0.005755\n",
      "2020-06-18  0.012603\n",
      "2020-06-20  0.005242\n",
      "2020-06-21  0.004918\n",
      "2020-06-22  0.010958\n",
      "2020-06-24  0.003896\n",
      "2020-06-25  0.004317\n",
      "2020-06-27  0.002846\n",
      "2020-06-29  0.008284\n",
      "2020-07-01  0.009467\n",
      "2020-07-02  0.004380\n",
      "2020-07-06  0.005461\n",
      "2020-07-09  0.008186\n",
      "2020-07-11  0.004065\n",
      "2020-07-12  0.001639\n",
      "2020-07-13  0.002717\n",
      "2020-07-16  0.007833\n",
      "2020-07-18  0.002500\n",
      "2020-07-20  0.004165\n",
      "2020-07-22  0.002956\n",
      "2020-07-23  0.011145\n",
      "2020-07-24  0.000952\n",
      "2020-07-25  0.000417\n",
      "2020-07-26  0.002016\n",
      "2020-07-27  0.003354\n",
      "2020-07-30  0.003754\n",
      "2020-08-01  0.002049\n",
      "2020-08-02  0.003226\n",
      "2020-08-03  0.000692\n",
      "2020-08-05  0.012587\n",
      "2020-08-06  0.007494\n",
      "2020-08-08  0.005741\n",
      "2020-08-09  0.004002\n",
      "2020-08-10  0.007080\n",
      "2020-08-11  0.008333\n",
      "2020-08-13  0.005949\n",
      "2020-08-15  0.010937\n",
      "2020-08-16  0.010085\n",
      "2020-08-17  0.009156\n",
      "2020-08-18  0.009889\n",
      "2020-08-19  0.016343\n",
      "2020-08-20  0.007507\n",
      "2020-08-21  0.003089\n",
      "2020-08-24  0.006494\n",
      "2020-08-25  0.005755\n",
      "2020-08-27  0.012325\n",
      "2020-08-28  0.012560\n",
      "2020-08-29  0.009274\n",
      "2020-08-30  0.010317\n",
      "2020-08-31  0.009076\n",
      "2020-09-01  0.011102\n",
      "2020-09-02  0.010292\n",
      "2020-09-03  0.008475\n",
      "2020-09-05  0.011019\n",
      "2020-09-06  0.004920\n",
      "2020-09-07  0.007251\n",
      "2020-09-08  0.005755\n",
      "2020-09-10  0.005355\n",
      "2020-09-12  0.010813\n",
      "2020-09-13  0.011024\n",
      "2020-09-14  0.016552\n",
      "2020-09-15  0.019868\n",
      "2020-09-17  0.015625\n",
      "2020-09-18  0.005495\n",
      "2020-09-19  0.004689\n",
      "2020-09-20  0.006855\n",
      "2020-09-21  0.006433\n"
     ]
    }
   ],
   "source": [
    "print(tourndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End training: 2020-09-29 00:00:00, End testing: 2020-09-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "freq='D'\n",
    "prediction_length=1\n",
    "context_length=7\n",
    "\n",
    "start_date = pd.Timestamp(\"2019-10-23\", freq=freq)\n",
    "end_training = pd.Timestamp(\"2020-09-29\", freq=freq)\n",
    "end_dataset = pd.Timestamp(\"2020-09-29\", freq=freq)\n",
    "mid_dataset=pd.Timestamp(\"2020-09-15\",freq=freq)\n",
    "end_testing = end_training + prediction_length*end_training.freq\n",
    "print(f'End training: {end_training}, End testing: {end_testing}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "goatdfarr.insert(0,ebaydf)\n",
    "goatdfarr.append(tourndf)\n",
    "overalldf=pd.concat(goatdfarr,axis=1,sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "overalldf.index=pd.to_datetime(overalldf.index,infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "overalldf.index.name='carddate'\n",
    "overalldf=pd.concat([overalldf['MINAVGMED'].fillna(\"NaN\"),\n",
    "                     overalldf.loc[:, overalldf.columns.difference(['MINAVGMED'])].fillna(method='ffill').fillna(method='bfill')],\n",
    "                    axis=1,join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_feat_arr=[]\n",
    "for col in overalldf.columns[1:]:\n",
    "    dynamic_feat_arr.append(list(overalldf[start_date-timedelta(1):end_training-timedelta(1)][col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=[{\"start\":str(start_date),\n",
    "               \"target\":list(overalldf[start_date:end_training]['MINAVGMED']),\n",
    "               \"dynamic_feat\":dynamic_feat_arr}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dicts_to_file(\"train.json\", training_data)\n",
    "# write_dicts_to_s3(f'{s3_data_path}/test/test.json',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    \n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print('File s3://{}/{} already exists.\\nSet override to upload anyway.\\n'.format(s3_bucket, s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file to s3://mtgmlbucket/mtgml-notebook/data/train/train.json\n"
     ]
    }
   ],
   "source": [
    "copy_to_s3(\"train.json\", s3_data_path + \"/train/train.json\")\n",
    "#copy_to_s3(\"test.json\", s3_data_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2019-10-23 00:00:00\", \"target\": [5.28999996185303, 6.29499983787537, 6.11800007820129, 8....\n"
     ]
    }
   ],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MINAVGMED</th>\n",
       "      <th>price0</th>\n",
       "      <th>price1</th>\n",
       "      <th>price2</th>\n",
       "      <th>price3</th>\n",
       "      <th>price4</th>\n",
       "      <th>price5</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carddate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-06-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.14</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.05</td>\n",
       "      <td>2.53</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.10</td>\n",
       "      <td>2.53</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.28</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.06</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.28</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.57</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.54</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2.57</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.79</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.77</td>\n",
       "      <td>2.57</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.84</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.61</td>\n",
       "      <td>2.57</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.56</td>\n",
       "      <td>2.61</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.16</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.49</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.91</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.55</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.42</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.52</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.51</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.11</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.51</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.52</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.47</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.53</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.63</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.73</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.71</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.74</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.72</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.62</td>\n",
       "      <td>2.98</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.74</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.83</td>\n",
       "      <td>3.57</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.88</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.08</td>\n",
       "      <td>3.57</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.16</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.18</td>\n",
       "      <td>3.63</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.97</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.24</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.89</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.06</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.04</td>\n",
       "      <td>4.02</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.79</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.66</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.22</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.24</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.41</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.35</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.37</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.09</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.37</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.44</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.41</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.76</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.57</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.43</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.44</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.49</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.46</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.66</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.51</td>\n",
       "      <td>4.24</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.94</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.53</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.97</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.53</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.18</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.53</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.83</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.37</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.84</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.39</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.64</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.39</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.47</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.39</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.23</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.38</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.18</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.38</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.12</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.38</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.93</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.23</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.24</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.27</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.24</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.27</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.24</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.63</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.24</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.49</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.79</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.56</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.26</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.68</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.81</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.35</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.72</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.26</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.15</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.26</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.05</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.26</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.26</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.57</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.27</td>\n",
       "      <td>3.96</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.05</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.23</td>\n",
       "      <td>3.88</td>\n",
       "      <td>1.31</td>\n",
       "      <td>10.74</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.19</td>\n",
       "      <td>2.99</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.44</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.34</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.54</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.34</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.34</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.35</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.35</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.36</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.36</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.14</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.36</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.35</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.35</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.32</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.96</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.35</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.18</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.33</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.39</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.34</td>\n",
       "      <td>3.07</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.57</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.54</td>\n",
       "      <td>3.02</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.32</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.60</td>\n",
       "      <td>3.02</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.12</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.72</td>\n",
       "      <td>3.02</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.72</td>\n",
       "      <td>3.02</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.78</td>\n",
       "      <td>3.02</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.75</td>\n",
       "      <td>3.02</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.70</td>\n",
       "      <td>2.74</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.34</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.69</td>\n",
       "      <td>2.57</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.69</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.13</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.66</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.92</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.57</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.88</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.58</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.32</td>\n",
       "      <td>2.47</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.39</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.69</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.28</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.28</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.28</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.28</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.28</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.32</td>\n",
       "      <td>2.15</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.32</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.92</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.53</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.42</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.36</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.43</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.43</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.47</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.43</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.46</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.68</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.47</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.57</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.47</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.72</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.46</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.44</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.47</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.31</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.48</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.18</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.48</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.15</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.48</td>\n",
       "      <td>2.94</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.12</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.48</td>\n",
       "      <td>3.06</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.57</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.49</td>\n",
       "      <td>3.06</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.49</td>\n",
       "      <td>3.06</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.27</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.48</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.47</td>\n",
       "      <td>2.77</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.47</td>\n",
       "      <td>2.63</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.47</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.63</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.32</td>\n",
       "      <td>1.29</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.28</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.34</td>\n",
       "      <td>1.27</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2.43</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.31</td>\n",
       "      <td>1.21</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.16</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.22</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.02</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.30</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.08</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.35</td>\n",
       "      <td>4.48</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.33</td>\n",
       "      <td>4.49</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.96</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.48</td>\n",
       "      <td>4.51</td>\n",
       "      <td>2.15</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.58</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.10</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.56</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.12</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.53</td>\n",
       "      <td>4.46</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.21</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.04</td>\n",
       "      <td>1.54</td>\n",
       "      <td>4.46</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.11</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.13</td>\n",
       "      <td>1.60</td>\n",
       "      <td>4.46</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.06</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.57</td>\n",
       "      <td>4.45</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.25</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.41</td>\n",
       "      <td>4.45</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.25</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.45</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.27</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.61</td>\n",
       "      <td>4.44</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.66</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.74</td>\n",
       "      <td>4.44</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.63</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.06</td>\n",
       "      <td>1.92</td>\n",
       "      <td>4.66</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.47</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.97</td>\n",
       "      <td>4.66</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.47</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.03</td>\n",
       "      <td>1.90</td>\n",
       "      <td>4.66</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.47</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.06</td>\n",
       "      <td>1.83</td>\n",
       "      <td>4.66</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.47</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1.87</td>\n",
       "      <td>4.70</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.47</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1.92</td>\n",
       "      <td>4.90</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.51</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1.97</td>\n",
       "      <td>4.90</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.69</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.93</td>\n",
       "      <td>4.90</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.51</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2.08</td>\n",
       "      <td>4.87</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.56</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.08</td>\n",
       "      <td>4.85</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.74</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.07</td>\n",
       "      <td>2.08</td>\n",
       "      <td>4.83</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.69</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.97</td>\n",
       "      <td>2.08</td>\n",
       "      <td>4.83</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.69</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.08</td>\n",
       "      <td>4.83</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.69</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.12</td>\n",
       "      <td>4.83</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.69</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.85</td>\n",
       "      <td>2.07</td>\n",
       "      <td>4.67</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.67</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.99</td>\n",
       "      <td>4.61</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.47</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.88</td>\n",
       "      <td>1.99</td>\n",
       "      <td>4.62</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.47</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.99</td>\n",
       "      <td>4.65</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.47</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.91</td>\n",
       "      <td>4.65</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.47</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1.90</td>\n",
       "      <td>4.65</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.47</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.90</td>\n",
       "      <td>4.65</td>\n",
       "      <td>1.34</td>\n",
       "      <td>1.56</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.90</td>\n",
       "      <td>4.65</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.69</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.63</td>\n",
       "      <td>1.90</td>\n",
       "      <td>4.66</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.69</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.84</td>\n",
       "      <td>4.66</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.69</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.63</td>\n",
       "      <td>4.66</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.69</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.63</td>\n",
       "      <td>4.66</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.69</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.73</td>\n",
       "      <td>4.66</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.69</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.66</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.69</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.76</td>\n",
       "      <td>4.66</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.69</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.77</td>\n",
       "      <td>4.66</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.69</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.66</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.69</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.91</td>\n",
       "      <td>4.67</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.71</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.01</td>\n",
       "      <td>4.69</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.90</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.22</td>\n",
       "      <td>4.69</td>\n",
       "      <td>1.93</td>\n",
       "      <td>1.89</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2.32</td>\n",
       "      <td>4.68</td>\n",
       "      <td>1.93</td>\n",
       "      <td>1.89</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.53</td>\n",
       "      <td>4.53</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2.06</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.27</td>\n",
       "      <td>2.52</td>\n",
       "      <td>4.45</td>\n",
       "      <td>1.97</td>\n",
       "      <td>2.18</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.54</td>\n",
       "      <td>4.32</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.78</td>\n",
       "      <td>2.61</td>\n",
       "      <td>4.32</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.18</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.78</td>\n",
       "      <td>4.33</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2.18</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.69</td>\n",
       "      <td>2.60</td>\n",
       "      <td>4.29</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.45</td>\n",
       "      <td>4.14</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.18</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.65</td>\n",
       "      <td>4.14</td>\n",
       "      <td>2.23</td>\n",
       "      <td>2.18</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.86</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.37</td>\n",
       "      <td>2.89</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.64</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.76</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.76</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.48</td>\n",
       "      <td>3.83</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.76</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.53</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.76</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2.63</td>\n",
       "      <td>3.37</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.76</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.04</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.18</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.76</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.21</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.76</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.62</td>\n",
       "      <td>3.21</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.76</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.17</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.76</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.07</td>\n",
       "      <td>2.48</td>\n",
       "      <td>3.03</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.76</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.07</td>\n",
       "      <td>2.48</td>\n",
       "      <td>3.03</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.76</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.45</td>\n",
       "      <td>3.02</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.76</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.23</td>\n",
       "      <td>2.47</td>\n",
       "      <td>2.96</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.76</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.96</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.76</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.96</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.76</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.94</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.76</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.29</td>\n",
       "      <td>2.69</td>\n",
       "      <td>2.92</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.93</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.24</td>\n",
       "      <td>3.05</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.97</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.10</td>\n",
       "      <td>3.05</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.97</td>\n",
       "      <td>2.27</td>\n",
       "      <td>2.72</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.05</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.72</td>\n",
       "      <td>1.99</td>\n",
       "      <td>3.05</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.63</td>\n",
       "      <td>3.05</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.82</td>\n",
       "      <td>3.05</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.73</td>\n",
       "      <td>2.73</td>\n",
       "      <td>2.76</td>\n",
       "      <td>3.05</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.54</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.76</td>\n",
       "      <td>2.53</td>\n",
       "      <td>2.87</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.69</td>\n",
       "      <td>2.76</td>\n",
       "      <td>2.53</td>\n",
       "      <td>3.36</td>\n",
       "      <td>9.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.68</td>\n",
       "      <td>2.91</td>\n",
       "      <td>2.76</td>\n",
       "      <td>2.40</td>\n",
       "      <td>3.41</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.71</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.97</td>\n",
       "      <td>2.24</td>\n",
       "      <td>3.41</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.78</td>\n",
       "      <td>2.82</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.41</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.82</td>\n",
       "      <td>2.86</td>\n",
       "      <td>3.37</td>\n",
       "      <td>2.72</td>\n",
       "      <td>3.41</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.87</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3.41</td>\n",
       "      <td>3.11</td>\n",
       "      <td>3.41</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.48</td>\n",
       "      <td>2.55</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.11</td>\n",
       "      <td>3.05</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.44</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.39</td>\n",
       "      <td>3.11</td>\n",
       "      <td>3.02</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.27</td>\n",
       "      <td>2.49</td>\n",
       "      <td>3.39</td>\n",
       "      <td>2.89</td>\n",
       "      <td>3.02</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.09</td>\n",
       "      <td>2.48</td>\n",
       "      <td>3.39</td>\n",
       "      <td>2.97</td>\n",
       "      <td>3.02</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.41</td>\n",
       "      <td>3.39</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.02</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.38</td>\n",
       "      <td>3.39</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.02</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2.33</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.02</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.23</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.16</td>\n",
       "      <td>3.02</td>\n",
       "      <td>10.11</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.38</td>\n",
       "      <td>2.86</td>\n",
       "      <td>3.02</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2.23</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2.85</td>\n",
       "      <td>3.02</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.82</td>\n",
       "      <td>2.21</td>\n",
       "      <td>3.12</td>\n",
       "      <td>2.85</td>\n",
       "      <td>3.02</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.96</td>\n",
       "      <td>2.22</td>\n",
       "      <td>3.01</td>\n",
       "      <td>2.81</td>\n",
       "      <td>3.02</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.88</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.99</td>\n",
       "      <td>2.56</td>\n",
       "      <td>3.02</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.79</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.97</td>\n",
       "      <td>2.56</td>\n",
       "      <td>3.02</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.97</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.97</td>\n",
       "      <td>2.56</td>\n",
       "      <td>3.02</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.05</td>\n",
       "      <td>2.33</td>\n",
       "      <td>2.97</td>\n",
       "      <td>2.56</td>\n",
       "      <td>3.02</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.04</td>\n",
       "      <td>2.32</td>\n",
       "      <td>2.97</td>\n",
       "      <td>2.56</td>\n",
       "      <td>3.02</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.97</td>\n",
       "      <td>2.48</td>\n",
       "      <td>3.02</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2.23</td>\n",
       "      <td>2.99</td>\n",
       "      <td>2.27</td>\n",
       "      <td>3.02</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.24</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.27</td>\n",
       "      <td>3.02</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.99</td>\n",
       "      <td>1.87</td>\n",
       "      <td>3.02</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.31</td>\n",
       "      <td>2.97</td>\n",
       "      <td>1.76</td>\n",
       "      <td>3.02</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.76</td>\n",
       "      <td>3.02</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.79</td>\n",
       "      <td>2.27</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.76</td>\n",
       "      <td>3.02</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.31</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.76</td>\n",
       "      <td>3.02</td>\n",
       "      <td>10.19</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.88</td>\n",
       "      <td>1.86</td>\n",
       "      <td>3.02</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.33</td>\n",
       "      <td>2.89</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.02</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.92</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.02</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.90</td>\n",
       "      <td>2.26</td>\n",
       "      <td>3.02</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.91</td>\n",
       "      <td>2.63</td>\n",
       "      <td>3.02</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.89</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.53</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2.92</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.94</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.69</td>\n",
       "      <td>2.71</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.92</td>\n",
       "      <td>2.92</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.78</td>\n",
       "      <td>2.89</td>\n",
       "      <td>2.91</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.83</td>\n",
       "      <td>2.90</td>\n",
       "      <td>2.97</td>\n",
       "      <td>2.71</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.71</td>\n",
       "      <td>2.83</td>\n",
       "      <td>3.11</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2.89</td>\n",
       "      <td>3.16</td>\n",
       "      <td>3.37</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.13</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.63</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.29</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.47</td>\n",
       "      <td>3.49</td>\n",
       "      <td>3.33</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.38</td>\n",
       "      <td>3.48</td>\n",
       "      <td>3.37</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.16</td>\n",
       "      <td>3.71</td>\n",
       "      <td>3.67</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.73</td>\n",
       "      <td>2.78</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.31</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.22</td>\n",
       "      <td>3.54</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.73</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.67</td>\n",
       "      <td>3.68</td>\n",
       "      <td>3.87</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.99</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.54</td>\n",
       "      <td>3.88</td>\n",
       "      <td>4.02</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.37</td>\n",
       "      <td>3.85</td>\n",
       "      <td>4.05</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.21</td>\n",
       "      <td>3.72</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.39</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.23</td>\n",
       "      <td>3.48</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.38</td>\n",
       "      <td>3.46</td>\n",
       "      <td>4.19</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.38</td>\n",
       "      <td>3.44</td>\n",
       "      <td>4.19</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.38</td>\n",
       "      <td>3.32</td>\n",
       "      <td>4.21</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.30</td>\n",
       "      <td>3.26</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.24</td>\n",
       "      <td>3.33</td>\n",
       "      <td>4.22</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.11</td>\n",
       "      <td>3.34</td>\n",
       "      <td>4.22</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.37</td>\n",
       "      <td>4.22</td>\n",
       "      <td>3.94</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.81</td>\n",
       "      <td>3.41</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.12</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.60</td>\n",
       "      <td>3.42</td>\n",
       "      <td>4.26</td>\n",
       "      <td>4.12</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.94</td>\n",
       "      <td>3.49</td>\n",
       "      <td>4.27</td>\n",
       "      <td>4.12</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.10</td>\n",
       "      <td>3.53</td>\n",
       "      <td>4.27</td>\n",
       "      <td>3.83</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.53</td>\n",
       "      <td>4.27</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.53</td>\n",
       "      <td>3.53</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.46</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4.24</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.53</td>\n",
       "      <td>3.28</td>\n",
       "      <td>4.24</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.93</td>\n",
       "      <td>3.24</td>\n",
       "      <td>4.24</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.53</td>\n",
       "      <td>3.26</td>\n",
       "      <td>4.24</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.29</td>\n",
       "      <td>3.44</td>\n",
       "      <td>4.24</td>\n",
       "      <td>3.09</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.47</td>\n",
       "      <td>4.24</td>\n",
       "      <td>2.81</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.44</td>\n",
       "      <td>3.40</td>\n",
       "      <td>4.24</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.27</td>\n",
       "      <td>3.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.53</td>\n",
       "      <td>3.01</td>\n",
       "      <td>4.24</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.86</td>\n",
       "      <td>2.74</td>\n",
       "      <td>4.25</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.74</td>\n",
       "      <td>4.25</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.94</td>\n",
       "      <td>2.57</td>\n",
       "      <td>4.24</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.37</td>\n",
       "      <td>3.99</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.79</td>\n",
       "      <td>2.32</td>\n",
       "      <td>3.97</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.84</td>\n",
       "      <td>2.29</td>\n",
       "      <td>3.97</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.31</td>\n",
       "      <td>3.97</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.74</td>\n",
       "      <td>2.34</td>\n",
       "      <td>3.99</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.50</td>\n",
       "      <td>4.03</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.96</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.07</td>\n",
       "      <td>2.47</td>\n",
       "      <td>4.03</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.26</td>\n",
       "      <td>4.03</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.28</td>\n",
       "      <td>4.03</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.05</td>\n",
       "      <td>2.29</td>\n",
       "      <td>4.03</td>\n",
       "      <td>1.85</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.97</td>\n",
       "      <td>2.31</td>\n",
       "      <td>4.03</td>\n",
       "      <td>1.85</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.36</td>\n",
       "      <td>4.03</td>\n",
       "      <td>1.72</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.56</td>\n",
       "      <td>4.04</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.96</td>\n",
       "      <td>2.78</td>\n",
       "      <td>4.04</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.84</td>\n",
       "      <td>2.81</td>\n",
       "      <td>4.04</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.75</td>\n",
       "      <td>4.04</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.61</td>\n",
       "      <td>4.01</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.93</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2.53</td>\n",
       "      <td>3.92</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.53</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.57</td>\n",
       "      <td>3.87</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.21</td>\n",
       "      <td>2.64</td>\n",
       "      <td>3.87</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.64</td>\n",
       "      <td>3.87</td>\n",
       "      <td>2.47</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.66</td>\n",
       "      <td>3.89</td>\n",
       "      <td>2.47</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.88</td>\n",
       "      <td>2.71</td>\n",
       "      <td>3.87</td>\n",
       "      <td>2.47</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.74</td>\n",
       "      <td>3.86</td>\n",
       "      <td>2.47</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.41</td>\n",
       "      <td>2.64</td>\n",
       "      <td>3.86</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.62</td>\n",
       "      <td>3.86</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.63</td>\n",
       "      <td>3.86</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.66</td>\n",
       "      <td>3.86</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.70</td>\n",
       "      <td>3.86</td>\n",
       "      <td>2.05</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.84</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.80</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.67</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.82</td>\n",
       "      <td>2.59</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.75</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.51</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.52</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.51</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.55</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.51</td>\n",
       "      <td>9.94</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.79</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.51</td>\n",
       "      <td>8.45</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.65</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.64</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.71</td>\n",
       "      <td>3.64</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.71</td>\n",
       "      <td>3.64</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.37</td>\n",
       "      <td>2.71</td>\n",
       "      <td>3.64</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.46</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.64</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.18</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.64</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.64</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.64</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.64</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.38</td>\n",
       "      <td>2.78</td>\n",
       "      <td>3.64</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.91</td>\n",
       "      <td>3.64</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.43</td>\n",
       "      <td>2.94</td>\n",
       "      <td>3.64</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.59</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.46</td>\n",
       "      <td>1.28</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2.96</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.63</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.99</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.38</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.79</td>\n",
       "      <td>2.96</td>\n",
       "      <td>3.47</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.79</td>\n",
       "      <td>3.49</td>\n",
       "      <td>1.59</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.81</td>\n",
       "      <td>3.51</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.52</td>\n",
       "      <td>2.77</td>\n",
       "      <td>3.51</td>\n",
       "      <td>2.09</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.44</td>\n",
       "      <td>2.70</td>\n",
       "      <td>3.51</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.67</td>\n",
       "      <td>3.51</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.19</td>\n",
       "      <td>2.67</td>\n",
       "      <td>3.51</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2.67</td>\n",
       "      <td>3.51</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.20</td>\n",
       "      <td>2.67</td>\n",
       "      <td>3.51</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.67</td>\n",
       "      <td>3.51</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2.71</td>\n",
       "      <td>3.51</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.16</td>\n",
       "      <td>2.83</td>\n",
       "      <td>3.51</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2.82</td>\n",
       "      <td>3.51</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.56</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.19</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.87</td>\n",
       "      <td>3.66</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.56</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.91</td>\n",
       "      <td>3.68</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.86</td>\n",
       "      <td>3.68</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.84</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.89</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.63</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.89</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2.81</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.78</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.64</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.26</td>\n",
       "      <td>2.72</td>\n",
       "      <td>3.73</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.14</td>\n",
       "      <td>2.66</td>\n",
       "      <td>3.67</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.67</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.59</td>\n",
       "      <td>3.67</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.41</td>\n",
       "      <td>2.55</td>\n",
       "      <td>3.67</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.44</td>\n",
       "      <td>2.56</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.38</td>\n",
       "      <td>2.83</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.51</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.41</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.66</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.68</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.66</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.91</td>\n",
       "      <td>3.66</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.61</td>\n",
       "      <td>3.05</td>\n",
       "      <td>3.66</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.51</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.82</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.85</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.77</td>\n",
       "      <td>3.85</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.41</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.84</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.51</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.67</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3.67</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.36</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.43</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.63</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.56</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.63</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.78</td>\n",
       "      <td>3.44</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.62</td>\n",
       "      <td>2.79</td>\n",
       "      <td>3.41</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.41</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.37</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.39</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2.85</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.36</td>\n",
       "      <td>2.85</td>\n",
       "      <td>3.37</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.47</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.37</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.93</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.91</td>\n",
       "      <td>3.09</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.69</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.01</td>\n",
       "      <td>3.11</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.04</td>\n",
       "      <td>3.13</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.12</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1.88</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.97</td>\n",
       "      <td>3.12</td>\n",
       "      <td>3.43</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.23</td>\n",
       "      <td>3.09</td>\n",
       "      <td>3.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.09</td>\n",
       "      <td>3.29</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.36</td>\n",
       "      <td>3.11</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.19</td>\n",
       "      <td>3.29</td>\n",
       "      <td>3.33</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.81</td>\n",
       "      <td>3.29</td>\n",
       "      <td>3.33</td>\n",
       "      <td>2.12</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.72</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.80</td>\n",
       "      <td>3.33</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.83</td>\n",
       "      <td>3.33</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.87</td>\n",
       "      <td>3.35</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.88</td>\n",
       "      <td>3.35</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.80</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.70</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.62</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.65</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.36</td>\n",
       "      <td>1.82</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.52</td>\n",
       "      <td>3.51</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.41</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.41</td>\n",
       "      <td>3.58</td>\n",
       "      <td>3.53</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.52</td>\n",
       "      <td>1.73</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.15</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.49</td>\n",
       "      <td>1.73</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.48</td>\n",
       "      <td>1.73</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.48</td>\n",
       "      <td>1.73</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.09</td>\n",
       "      <td>3.14</td>\n",
       "      <td>3.48</td>\n",
       "      <td>1.73</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.14</td>\n",
       "      <td>3.48</td>\n",
       "      <td>1.64</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.08</td>\n",
       "      <td>3.14</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1.48</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.15</td>\n",
       "      <td>3.14</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.14</td>\n",
       "      <td>3.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.06</td>\n",
       "      <td>3.01</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2.89</td>\n",
       "      <td>3.35</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.85</td>\n",
       "      <td>3.35</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2.85</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.78</td>\n",
       "      <td>2.78</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.77</td>\n",
       "      <td>2.76</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.77</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.66</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.71</td>\n",
       "      <td>3.31</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.83</td>\n",
       "      <td>2.71</td>\n",
       "      <td>3.31</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.81</td>\n",
       "      <td>2.77</td>\n",
       "      <td>3.31</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.83</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.31</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.53</td>\n",
       "      <td>3.31</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.58</td>\n",
       "      <td>2.33</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.63</td>\n",
       "      <td>2.39</td>\n",
       "      <td>3.35</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.72</td>\n",
       "      <td>2.26</td>\n",
       "      <td>3.35</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.96</td>\n",
       "      <td>3.35</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.94</td>\n",
       "      <td>3.31</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.85</td>\n",
       "      <td>3.31</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.75</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.54</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.54</td>\n",
       "      <td>3.11</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.51</td>\n",
       "      <td>3.11</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.47</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.93</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.27</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.03</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.85</td>\n",
       "      <td>2.77</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2.72</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2.72</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2.72</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2.72</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2.72</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.31</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2.77</td>\n",
       "      <td>1.20</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.86</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.20</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.84</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.20</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.88</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.20</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2.78</td>\n",
       "      <td>1.20</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2.76</td>\n",
       "      <td>1.20</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1.19</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2.62</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2.62</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.03</td>\n",
       "      <td>2.62</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2.62</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2.62</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2.62</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2.64</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.12</td>\n",
       "      <td>2.66</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.14</td>\n",
       "      <td>2.64</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.15</td>\n",
       "      <td>2.52</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.15</td>\n",
       "      <td>2.52</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.15</td>\n",
       "      <td>2.52</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.52</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.52</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.52</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.18</td>\n",
       "      <td>2.51</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.20</td>\n",
       "      <td>2.51</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.20</td>\n",
       "      <td>2.61</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.19</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.19</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.20</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.78</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.15</td>\n",
       "      <td>2.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.15</td>\n",
       "      <td>2.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2.53</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.58</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.57</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.38</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.38</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.03</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.03</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2.32</td>\n",
       "      <td>4.14</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.59</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.60</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.43</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.16</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.16</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.39</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.32</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.39</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.34</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.32</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.76</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.36</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.88</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.53</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.53</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.53</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.96</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.53</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.12</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.53</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2.02</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.53</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.73</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.53</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.02</td>\n",
       "      <td>1.76</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.53</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.51</td>\n",
       "      <td>2.02</td>\n",
       "      <td>1.76</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.53</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.40</td>\n",
       "      <td>2.04</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.53</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.58</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.53</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.59</td>\n",
       "      <td>2.04</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.53</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.53</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.44</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.82</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.97</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.43</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.97</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.16</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.12</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.19</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.19</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.05</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.19</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.46</td>\n",
       "      <td>2.23</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.37</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.28</td>\n",
       "      <td>2.32</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.48</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.28</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.48</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.21</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.48</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2.49</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.70</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.15</td>\n",
       "      <td>2.58</td>\n",
       "      <td>2.28</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.77</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.20</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.77</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2.51</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.99</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.79</td>\n",
       "      <td>2.54</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.31</td>\n",
       "      <td>2.86</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.56</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.14</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.56</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.03</td>\n",
       "      <td>2.86</td>\n",
       "      <td>2.56</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.12</td>\n",
       "      <td>2.83</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.18</td>\n",
       "      <td>2.76</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.21</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.59</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.20</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.61</td>\n",
       "      <td>1.23</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.31</td>\n",
       "      <td>2.68</td>\n",
       "      <td>2.62</td>\n",
       "      <td>1.11</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.77</td>\n",
       "      <td>2.67</td>\n",
       "      <td>1.11</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2.72</td>\n",
       "      <td>1.11</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.58</td>\n",
       "      <td>2.72</td>\n",
       "      <td>1.11</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.46</td>\n",
       "      <td>2.76</td>\n",
       "      <td>2.71</td>\n",
       "      <td>1.15</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.56</td>\n",
       "      <td>2.78</td>\n",
       "      <td>2.71</td>\n",
       "      <td>1.33</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.71</td>\n",
       "      <td>1.33</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.38</td>\n",
       "      <td>2.71</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1.33</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.69</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1.33</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.79</td>\n",
       "      <td>2.74</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1.33</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.62</td>\n",
       "      <td>2.77</td>\n",
       "      <td>2.68</td>\n",
       "      <td>1.33</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.77</td>\n",
       "      <td>2.71</td>\n",
       "      <td>1.65</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>2.72</td>\n",
       "      <td>1.77</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.77</td>\n",
       "      <td>2.73</td>\n",
       "      <td>1.64</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.78</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.31</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.79</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.52</td>\n",
       "      <td>2.74</td>\n",
       "      <td>2.62</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.51</td>\n",
       "      <td>3.01</td>\n",
       "      <td>2.67</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.45</td>\n",
       "      <td>3.06</td>\n",
       "      <td>2.73</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.26</td>\n",
       "      <td>3.12</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.34</td>\n",
       "      <td>3.31</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.41</td>\n",
       "      <td>3.34</td>\n",
       "      <td>3.03</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.44</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.07</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.46</td>\n",
       "      <td>3.57</td>\n",
       "      <td>3.07</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.51</td>\n",
       "      <td>3.23</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.47</td>\n",
       "      <td>3.36</td>\n",
       "      <td>3.29</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.64</td>\n",
       "      <td>3.69</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1.62</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.65</td>\n",
       "      <td>3.76</td>\n",
       "      <td>3.49</td>\n",
       "      <td>1.99</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.38</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.54</td>\n",
       "      <td>1.99</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.29</td>\n",
       "      <td>3.48</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.99</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.15</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.52</td>\n",
       "      <td>1.82</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.17</td>\n",
       "      <td>3.56</td>\n",
       "      <td>3.37</td>\n",
       "      <td>1.77</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.77</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.12</td>\n",
       "      <td>3.69</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.77</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.32</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.56</td>\n",
       "      <td>1.85</td>\n",
       "      <td>2.96</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.46</td>\n",
       "      <td>4.05</td>\n",
       "      <td>3.99</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.68</td>\n",
       "      <td>3.99</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.02</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.70</td>\n",
       "      <td>3.70</td>\n",
       "      <td>4.03</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.61</td>\n",
       "      <td>3.70</td>\n",
       "      <td>4.03</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.65</td>\n",
       "      <td>3.71</td>\n",
       "      <td>4.08</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.58</td>\n",
       "      <td>3.68</td>\n",
       "      <td>4.07</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.14</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.40</td>\n",
       "      <td>3.64</td>\n",
       "      <td>4.17</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.26</td>\n",
       "      <td>3.63</td>\n",
       "      <td>4.18</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.11</td>\n",
       "      <td>3.62</td>\n",
       "      <td>4.19</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.23</td>\n",
       "      <td>3.63</td>\n",
       "      <td>4.21</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.63</td>\n",
       "      <td>4.19</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.16</td>\n",
       "      <td>3.63</td>\n",
       "      <td>4.19</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.07</td>\n",
       "      <td>3.61</td>\n",
       "      <td>4.19</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.33</td>\n",
       "      <td>3.62</td>\n",
       "      <td>4.22</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.97</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.24</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.26</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.16</td>\n",
       "      <td>3.24</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.02</td>\n",
       "      <td>3.24</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.49</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3.51</td>\n",
       "      <td>3.49</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.91</td>\n",
       "      <td>3.56</td>\n",
       "      <td>3.49</td>\n",
       "      <td>2.05</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>3.54</td>\n",
       "      <td>3.49</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.91</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.49</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3.54</td>\n",
       "      <td>3.54</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.94</td>\n",
       "      <td>3.54</td>\n",
       "      <td>3.46</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.94</td>\n",
       "      <td>3.52</td>\n",
       "      <td>3.21</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.52</td>\n",
       "      <td>3.21</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.48</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.19</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.19</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.19</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.09</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2.79</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2.73</td>\n",
       "      <td>2.98</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.84</td>\n",
       "      <td>2.71</td>\n",
       "      <td>2.88</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.78</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2.87</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.87</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.14</td>\n",
       "      <td>2.71</td>\n",
       "      <td>2.93</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.71</td>\n",
       "      <td>2.93</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.69</td>\n",
       "      <td>2.93</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.32</td>\n",
       "      <td>2.69</td>\n",
       "      <td>2.93</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.46</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.93</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.57</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.51</td>\n",
       "      <td>2.78</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.79</td>\n",
       "      <td>2.88</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.79</td>\n",
       "      <td>2.88</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.44</td>\n",
       "      <td>2.79</td>\n",
       "      <td>2.88</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.77</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.91</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.31</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.91</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.19</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.91</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.16</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.92</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2.61</td>\n",
       "      <td>2.92</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.61</td>\n",
       "      <td>2.96</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.50</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.59</td>\n",
       "      <td>2.62</td>\n",
       "      <td>2.97</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.73</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.01</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.72</td>\n",
       "      <td>2.93</td>\n",
       "      <td>3.12</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.12</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.04</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.32</td>\n",
       "      <td>2.98</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.27</td>\n",
       "      <td>2.96</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.43</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.94</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.99</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.07</td>\n",
       "      <td>2.99</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.48</td>\n",
       "      <td>3.07</td>\n",
       "      <td>2.99</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.44</td>\n",
       "      <td>2.78</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.43</td>\n",
       "      <td>2.77</td>\n",
       "      <td>2.97</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.77</td>\n",
       "      <td>2.96</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2.79</td>\n",
       "      <td>2.96</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2.79</td>\n",
       "      <td>2.97</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.36</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.96</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.46</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2.96</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.82</td>\n",
       "      <td>2.96</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.56</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2.96</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.38</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.96</td>\n",
       "      <td>1.62</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.37</td>\n",
       "      <td>2.82</td>\n",
       "      <td>2.96</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.46</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2.96</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.21</td>\n",
       "      <td>2.84</td>\n",
       "      <td>2.96</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.29</td>\n",
       "      <td>2.82</td>\n",
       "      <td>3.02</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.32</td>\n",
       "      <td>2.82</td>\n",
       "      <td>3.09</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.82</td>\n",
       "      <td>3.09</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.29</td>\n",
       "      <td>2.74</td>\n",
       "      <td>2.99</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.62</td>\n",
       "      <td>2.79</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.38</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.79</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.79</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.16</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.79</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.62</td>\n",
       "      <td>2.79</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.16</td>\n",
       "      <td>2.62</td>\n",
       "      <td>2.79</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.16</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.44</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.44</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.44</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.44</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.91</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.43</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.78</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2.62</td>\n",
       "      <td>2.45</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.43</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.44</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.38</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.27</td>\n",
       "      <td>2.69</td>\n",
       "      <td>2.37</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.27</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.37</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.83</td>\n",
       "      <td>2.37</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.87</td>\n",
       "      <td>2.37</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.33</td>\n",
       "      <td>3.12</td>\n",
       "      <td>2.37</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-21</th>\n",
       "      <td>5.245</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.13</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.79</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-22</th>\n",
       "      <td>6</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.23</td>\n",
       "      <td>2.77</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-23</th>\n",
       "      <td>5.29</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.37</td>\n",
       "      <td>2.48</td>\n",
       "      <td>3.08</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-24</th>\n",
       "      <td>6.295</td>\n",
       "      <td>4.46</td>\n",
       "      <td>4.95</td>\n",
       "      <td>4.78</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3.34</td>\n",
       "      <td>4.27</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-25</th>\n",
       "      <td>6.118</td>\n",
       "      <td>3.90</td>\n",
       "      <td>6.93</td>\n",
       "      <td>5.49</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.02</td>\n",
       "      <td>4.19</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-26</th>\n",
       "      <td>8.94</td>\n",
       "      <td>4.47</td>\n",
       "      <td>7.97</td>\n",
       "      <td>5.75</td>\n",
       "      <td>3.86</td>\n",
       "      <td>4.39</td>\n",
       "      <td>4.19</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-27</th>\n",
       "      <td>7.24333</td>\n",
       "      <td>5.57</td>\n",
       "      <td>8.17</td>\n",
       "      <td>5.79</td>\n",
       "      <td>3.80</td>\n",
       "      <td>4.39</td>\n",
       "      <td>4.41</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-28</th>\n",
       "      <td>6.83333</td>\n",
       "      <td>5.98</td>\n",
       "      <td>8.22</td>\n",
       "      <td>6.12</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4.64</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.006046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6.01</td>\n",
       "      <td>7.61</td>\n",
       "      <td>6.17</td>\n",
       "      <td>4.35</td>\n",
       "      <td>4.85</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.006046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-30</th>\n",
       "      <td>6.88</td>\n",
       "      <td>6.08</td>\n",
       "      <td>7.97</td>\n",
       "      <td>6.35</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.85</td>\n",
       "      <td>4.85</td>\n",
       "      <td>0.006046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>5.485</td>\n",
       "      <td>7.56</td>\n",
       "      <td>9.51</td>\n",
       "      <td>7.50</td>\n",
       "      <td>4.79</td>\n",
       "      <td>5.12</td>\n",
       "      <td>5.23</td>\n",
       "      <td>0.008246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>7.175</td>\n",
       "      <td>7.55</td>\n",
       "      <td>9.90</td>\n",
       "      <td>7.82</td>\n",
       "      <td>4.57</td>\n",
       "      <td>5.49</td>\n",
       "      <td>5.49</td>\n",
       "      <td>0.001667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-02</th>\n",
       "      <td>7.62</td>\n",
       "      <td>7.34</td>\n",
       "      <td>8.92</td>\n",
       "      <td>7.93</td>\n",
       "      <td>4.57</td>\n",
       "      <td>5.49</td>\n",
       "      <td>5.49</td>\n",
       "      <td>0.001667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-03</th>\n",
       "      <td>7.82</td>\n",
       "      <td>6.64</td>\n",
       "      <td>8.84</td>\n",
       "      <td>7.83</td>\n",
       "      <td>5.03</td>\n",
       "      <td>5.49</td>\n",
       "      <td>5.68</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-04</th>\n",
       "      <td>7.475</td>\n",
       "      <td>5.67</td>\n",
       "      <td>8.91</td>\n",
       "      <td>7.96</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5.49</td>\n",
       "      <td>5.95</td>\n",
       "      <td>0.005972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-05</th>\n",
       "      <td>8.495</td>\n",
       "      <td>5.90</td>\n",
       "      <td>9.01</td>\n",
       "      <td>8.03</td>\n",
       "      <td>6.14</td>\n",
       "      <td>5.59</td>\n",
       "      <td>5.95</td>\n",
       "      <td>0.005972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-06</th>\n",
       "      <td>9.52</td>\n",
       "      <td>7.10</td>\n",
       "      <td>9.60</td>\n",
       "      <td>8.73</td>\n",
       "      <td>6.14</td>\n",
       "      <td>5.95</td>\n",
       "      <td>6.12</td>\n",
       "      <td>0.005972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-07</th>\n",
       "      <td>7.92333</td>\n",
       "      <td>6.97</td>\n",
       "      <td>10.02</td>\n",
       "      <td>9.18</td>\n",
       "      <td>6.72</td>\n",
       "      <td>5.95</td>\n",
       "      <td>6.54</td>\n",
       "      <td>0.005557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-08</th>\n",
       "      <td>7.755</td>\n",
       "      <td>8.42</td>\n",
       "      <td>10.07</td>\n",
       "      <td>9.14</td>\n",
       "      <td>6.74</td>\n",
       "      <td>6.07</td>\n",
       "      <td>6.54</td>\n",
       "      <td>0.005557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.47</td>\n",
       "      <td>10.34</td>\n",
       "      <td>9.65</td>\n",
       "      <td>6.74</td>\n",
       "      <td>6.54</td>\n",
       "      <td>6.75</td>\n",
       "      <td>0.005838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.34</td>\n",
       "      <td>11.57</td>\n",
       "      <td>9.77</td>\n",
       "      <td>7.44</td>\n",
       "      <td>6.54</td>\n",
       "      <td>7.27</td>\n",
       "      <td>0.009163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-11</th>\n",
       "      <td>10.68</td>\n",
       "      <td>9.68</td>\n",
       "      <td>13.09</td>\n",
       "      <td>10.29</td>\n",
       "      <td>7.53</td>\n",
       "      <td>6.69</td>\n",
       "      <td>7.27</td>\n",
       "      <td>0.005086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-12</th>\n",
       "      <td>9.18625</td>\n",
       "      <td>11.17</td>\n",
       "      <td>12.92</td>\n",
       "      <td>10.60</td>\n",
       "      <td>7.53</td>\n",
       "      <td>7.27</td>\n",
       "      <td>7.48</td>\n",
       "      <td>0.005086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-13</th>\n",
       "      <td>10.34</td>\n",
       "      <td>11.58</td>\n",
       "      <td>12.86</td>\n",
       "      <td>11.81</td>\n",
       "      <td>7.53</td>\n",
       "      <td>7.27</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.005086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-14</th>\n",
       "      <td>10.79</td>\n",
       "      <td>12.08</td>\n",
       "      <td>13.50</td>\n",
       "      <td>12.35</td>\n",
       "      <td>7.53</td>\n",
       "      <td>7.42</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.006987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-15</th>\n",
       "      <td>9.81167</td>\n",
       "      <td>11.14</td>\n",
       "      <td>13.36</td>\n",
       "      <td>12.29</td>\n",
       "      <td>7.53</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0.011250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-16</th>\n",
       "      <td>10.06</td>\n",
       "      <td>9.46</td>\n",
       "      <td>12.04</td>\n",
       "      <td>11.88</td>\n",
       "      <td>7.53</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.73</td>\n",
       "      <td>0.011250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-17</th>\n",
       "      <td>10.495</td>\n",
       "      <td>8.29</td>\n",
       "      <td>11.93</td>\n",
       "      <td>11.49</td>\n",
       "      <td>7.32</td>\n",
       "      <td>8.15</td>\n",
       "      <td>8.73</td>\n",
       "      <td>0.003769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-18</th>\n",
       "      <td>11.3778</td>\n",
       "      <td>7.83</td>\n",
       "      <td>11.94</td>\n",
       "      <td>11.50</td>\n",
       "      <td>6.80</td>\n",
       "      <td>8.73</td>\n",
       "      <td>8.94</td>\n",
       "      <td>0.005506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-19</th>\n",
       "      <td>11.6775</td>\n",
       "      <td>6.39</td>\n",
       "      <td>12.06</td>\n",
       "      <td>11.48</td>\n",
       "      <td>6.80</td>\n",
       "      <td>8.73</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.005506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6.90</td>\n",
       "      <td>12.01</td>\n",
       "      <td>11.41</td>\n",
       "      <td>6.22</td>\n",
       "      <td>8.85</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.005506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-21</th>\n",
       "      <td>11.79</td>\n",
       "      <td>6.28</td>\n",
       "      <td>12.16</td>\n",
       "      <td>11.48</td>\n",
       "      <td>6.47</td>\n",
       "      <td>9.44</td>\n",
       "      <td>9.73</td>\n",
       "      <td>0.005520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-22</th>\n",
       "      <td>10.3</td>\n",
       "      <td>7.10</td>\n",
       "      <td>12.80</td>\n",
       "      <td>11.72</td>\n",
       "      <td>7.96</td>\n",
       "      <td>8.86</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.005520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-23</th>\n",
       "      <td>10</td>\n",
       "      <td>7.27</td>\n",
       "      <td>12.66</td>\n",
       "      <td>11.49</td>\n",
       "      <td>8.44</td>\n",
       "      <td>8.45</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.005520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-24</th>\n",
       "      <td>4.33</td>\n",
       "      <td>7.11</td>\n",
       "      <td>12.52</td>\n",
       "      <td>11.29</td>\n",
       "      <td>8.52</td>\n",
       "      <td>8.18</td>\n",
       "      <td>10.73</td>\n",
       "      <td>0.007222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-25</th>\n",
       "      <td>11.7633</td>\n",
       "      <td>7.22</td>\n",
       "      <td>13.12</td>\n",
       "      <td>11.38</td>\n",
       "      <td>7.79</td>\n",
       "      <td>7.72</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.007543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-26</th>\n",
       "      <td>9.99</td>\n",
       "      <td>7.24</td>\n",
       "      <td>12.94</td>\n",
       "      <td>11.45</td>\n",
       "      <td>7.79</td>\n",
       "      <td>7.72</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.007923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.63</td>\n",
       "      <td>12.87</td>\n",
       "      <td>11.38</td>\n",
       "      <td>7.79</td>\n",
       "      <td>7.72</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.007923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-28</th>\n",
       "      <td>11.975</td>\n",
       "      <td>8.31</td>\n",
       "      <td>12.76</td>\n",
       "      <td>11.38</td>\n",
       "      <td>7.79</td>\n",
       "      <td>7.72</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.005228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-29</th>\n",
       "      <td>11</td>\n",
       "      <td>7.64</td>\n",
       "      <td>12.50</td>\n",
       "      <td>11.38</td>\n",
       "      <td>7.79</td>\n",
       "      <td>8.09</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.007503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>11.506</td>\n",
       "      <td>7.50</td>\n",
       "      <td>12.17</td>\n",
       "      <td>11.38</td>\n",
       "      <td>9.21</td>\n",
       "      <td>8.71</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.003333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>14.48</td>\n",
       "      <td>6.63</td>\n",
       "      <td>12.17</td>\n",
       "      <td>11.38</td>\n",
       "      <td>9.49</td>\n",
       "      <td>8.71</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.002271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-02</th>\n",
       "      <td>12.74</td>\n",
       "      <td>6.48</td>\n",
       "      <td>12.20</td>\n",
       "      <td>11.38</td>\n",
       "      <td>9.49</td>\n",
       "      <td>8.71</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.003489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-03</th>\n",
       "      <td>11</td>\n",
       "      <td>7.46</td>\n",
       "      <td>12.19</td>\n",
       "      <td>11.40</td>\n",
       "      <td>8.54</td>\n",
       "      <td>8.71</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.003489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-04</th>\n",
       "      <td>11.79</td>\n",
       "      <td>8.45</td>\n",
       "      <td>12.12</td>\n",
       "      <td>11.45</td>\n",
       "      <td>8.50</td>\n",
       "      <td>8.71</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.003489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-05</th>\n",
       "      <td>11.49</td>\n",
       "      <td>8.24</td>\n",
       "      <td>12.35</td>\n",
       "      <td>11.44</td>\n",
       "      <td>8.50</td>\n",
       "      <td>8.71</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.006306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8.55</td>\n",
       "      <td>11.93</td>\n",
       "      <td>11.44</td>\n",
       "      <td>8.50</td>\n",
       "      <td>8.71</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.006306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-07</th>\n",
       "      <td>11.59</td>\n",
       "      <td>8.25</td>\n",
       "      <td>11.93</td>\n",
       "      <td>11.41</td>\n",
       "      <td>8.32</td>\n",
       "      <td>8.71</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8.62</td>\n",
       "      <td>11.89</td>\n",
       "      <td>11.40</td>\n",
       "      <td>7.77</td>\n",
       "      <td>8.71</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.010014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-09</th>\n",
       "      <td>11.5767</td>\n",
       "      <td>8.06</td>\n",
       "      <td>11.68</td>\n",
       "      <td>11.40</td>\n",
       "      <td>7.77</td>\n",
       "      <td>8.71</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.010014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-10</th>\n",
       "      <td>13.5</td>\n",
       "      <td>7.47</td>\n",
       "      <td>11.25</td>\n",
       "      <td>11.39</td>\n",
       "      <td>7.77</td>\n",
       "      <td>8.71</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.004680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-11</th>\n",
       "      <td>10.8825</td>\n",
       "      <td>8.26</td>\n",
       "      <td>10.85</td>\n",
       "      <td>11.38</td>\n",
       "      <td>7.77</td>\n",
       "      <td>8.71</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.004680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12</th>\n",
       "      <td>11.99</td>\n",
       "      <td>8.24</td>\n",
       "      <td>10.86</td>\n",
       "      <td>11.38</td>\n",
       "      <td>7.77</td>\n",
       "      <td>10.44</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.005186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-13</th>\n",
       "      <td>12.485</td>\n",
       "      <td>8.48</td>\n",
       "      <td>10.96</td>\n",
       "      <td>10.86</td>\n",
       "      <td>7.77</td>\n",
       "      <td>10.69</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.005186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8.06</td>\n",
       "      <td>10.88</td>\n",
       "      <td>10.68</td>\n",
       "      <td>7.25</td>\n",
       "      <td>10.61</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.002667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-15</th>\n",
       "      <td>11.42</td>\n",
       "      <td>7.93</td>\n",
       "      <td>10.80</td>\n",
       "      <td>10.58</td>\n",
       "      <td>7.04</td>\n",
       "      <td>9.69</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.011314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.96</td>\n",
       "      <td>11.27</td>\n",
       "      <td>10.58</td>\n",
       "      <td>7.04</td>\n",
       "      <td>9.69</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.007181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-17</th>\n",
       "      <td>12.236</td>\n",
       "      <td>9.43</td>\n",
       "      <td>11.49</td>\n",
       "      <td>10.60</td>\n",
       "      <td>7.56</td>\n",
       "      <td>9.69</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.019210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-18</th>\n",
       "      <td>11.99</td>\n",
       "      <td>9.17</td>\n",
       "      <td>11.54</td>\n",
       "      <td>10.63</td>\n",
       "      <td>7.77</td>\n",
       "      <td>9.69</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.011429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8.40</td>\n",
       "      <td>11.52</td>\n",
       "      <td>10.63</td>\n",
       "      <td>7.77</td>\n",
       "      <td>9.69</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.004089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-20</th>\n",
       "      <td>12.75</td>\n",
       "      <td>9.00</td>\n",
       "      <td>11.47</td>\n",
       "      <td>10.69</td>\n",
       "      <td>7.77</td>\n",
       "      <td>9.69</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.001905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-21</th>\n",
       "      <td>11.355</td>\n",
       "      <td>10.14</td>\n",
       "      <td>11.48</td>\n",
       "      <td>10.63</td>\n",
       "      <td>8.55</td>\n",
       "      <td>9.69</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.011439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-22</th>\n",
       "      <td>12.89</td>\n",
       "      <td>11.18</td>\n",
       "      <td>11.67</td>\n",
       "      <td>10.38</td>\n",
       "      <td>9.68</td>\n",
       "      <td>9.69</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.010797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-23</th>\n",
       "      <td>13.745</td>\n",
       "      <td>10.08</td>\n",
       "      <td>12.44</td>\n",
       "      <td>10.80</td>\n",
       "      <td>11.04</td>\n",
       "      <td>9.69</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.004660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8.72</td>\n",
       "      <td>12.59</td>\n",
       "      <td>11.08</td>\n",
       "      <td>10.46</td>\n",
       "      <td>9.81</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.003955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.79</td>\n",
       "      <td>13.09</td>\n",
       "      <td>10.90</td>\n",
       "      <td>10.04</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.005333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-26</th>\n",
       "      <td>12.56</td>\n",
       "      <td>7.78</td>\n",
       "      <td>13.25</td>\n",
       "      <td>10.95</td>\n",
       "      <td>10.04</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.002945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>12.75</td>\n",
       "      <td>7.83</td>\n",
       "      <td>13.60</td>\n",
       "      <td>10.96</td>\n",
       "      <td>10.04</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.001250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-28</th>\n",
       "      <td>14.9967</td>\n",
       "      <td>7.53</td>\n",
       "      <td>13.29</td>\n",
       "      <td>10.96</td>\n",
       "      <td>9.92</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.005330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6.36</td>\n",
       "      <td>13.22</td>\n",
       "      <td>10.96</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>13.82</td>\n",
       "      <td>5.94</td>\n",
       "      <td>13.14</td>\n",
       "      <td>10.96</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.002291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>14.3375</td>\n",
       "      <td>5.74</td>\n",
       "      <td>13.13</td>\n",
       "      <td>10.94</td>\n",
       "      <td>7.75</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.000702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.77</td>\n",
       "      <td>13.12</td>\n",
       "      <td>10.94</td>\n",
       "      <td>7.32</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.001876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02</th>\n",
       "      <td>14.99</td>\n",
       "      <td>5.02</td>\n",
       "      <td>13.10</td>\n",
       "      <td>10.97</td>\n",
       "      <td>7.32</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.002001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>14.44</td>\n",
       "      <td>5.20</td>\n",
       "      <td>13.10</td>\n",
       "      <td>10.93</td>\n",
       "      <td>7.32</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.005614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-04</th>\n",
       "      <td>13</td>\n",
       "      <td>5.62</td>\n",
       "      <td>13.11</td>\n",
       "      <td>10.93</td>\n",
       "      <td>7.81</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.006219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-05</th>\n",
       "      <td>12.375</td>\n",
       "      <td>5.58</td>\n",
       "      <td>13.23</td>\n",
       "      <td>10.91</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.009184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-06</th>\n",
       "      <td>14.5</td>\n",
       "      <td>5.69</td>\n",
       "      <td>12.72</td>\n",
       "      <td>10.91</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.003828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-07</th>\n",
       "      <td>12.5</td>\n",
       "      <td>6.02</td>\n",
       "      <td>12.29</td>\n",
       "      <td>10.90</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.005492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-08</th>\n",
       "      <td>12.75</td>\n",
       "      <td>7.43</td>\n",
       "      <td>11.96</td>\n",
       "      <td>10.89</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.007304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-09</th>\n",
       "      <td>12</td>\n",
       "      <td>9.28</td>\n",
       "      <td>12.07</td>\n",
       "      <td>10.40</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.003891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-10</th>\n",
       "      <td>14.385</td>\n",
       "      <td>10.45</td>\n",
       "      <td>11.94</td>\n",
       "      <td>10.34</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.006377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-11</th>\n",
       "      <td>11.74</td>\n",
       "      <td>9.61</td>\n",
       "      <td>11.92</td>\n",
       "      <td>10.38</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.002857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-12</th>\n",
       "      <td>12.646</td>\n",
       "      <td>9.64</td>\n",
       "      <td>11.87</td>\n",
       "      <td>11.15</td>\n",
       "      <td>9.87</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.006155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-13</th>\n",
       "      <td>15</td>\n",
       "      <td>9.20</td>\n",
       "      <td>11.97</td>\n",
       "      <td>11.26</td>\n",
       "      <td>10.04</td>\n",
       "      <td>9.57</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.004002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-14</th>\n",
       "      <td>12.3333</td>\n",
       "      <td>8.53</td>\n",
       "      <td>12.09</td>\n",
       "      <td>11.25</td>\n",
       "      <td>10.04</td>\n",
       "      <td>8.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.007622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-15</th>\n",
       "      <td>11.485</td>\n",
       "      <td>8.48</td>\n",
       "      <td>11.98</td>\n",
       "      <td>11.22</td>\n",
       "      <td>10.04</td>\n",
       "      <td>8.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.005060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.79</td>\n",
       "      <td>11.74</td>\n",
       "      <td>11.14</td>\n",
       "      <td>10.04</td>\n",
       "      <td>8.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.006832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-17</th>\n",
       "      <td>13.5</td>\n",
       "      <td>7.97</td>\n",
       "      <td>12.67</td>\n",
       "      <td>11.22</td>\n",
       "      <td>9.50</td>\n",
       "      <td>9.86</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.005591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-18</th>\n",
       "      <td>12.5</td>\n",
       "      <td>7.50</td>\n",
       "      <td>12.66</td>\n",
       "      <td>11.22</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.009412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-19</th>\n",
       "      <td>11.495</td>\n",
       "      <td>6.74</td>\n",
       "      <td>12.66</td>\n",
       "      <td>11.52</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.009170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-20</th>\n",
       "      <td>13.25</td>\n",
       "      <td>7.03</td>\n",
       "      <td>12.29</td>\n",
       "      <td>11.47</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.005265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-21</th>\n",
       "      <td>12.5</td>\n",
       "      <td>8.20</td>\n",
       "      <td>11.98</td>\n",
       "      <td>11.45</td>\n",
       "      <td>8.13</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.005641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-22</th>\n",
       "      <td>11.738</td>\n",
       "      <td>8.20</td>\n",
       "      <td>12.22</td>\n",
       "      <td>11.27</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.002353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-23</th>\n",
       "      <td>11.99</td>\n",
       "      <td>8.64</td>\n",
       "      <td>11.86</td>\n",
       "      <td>11.23</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.003791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-24</th>\n",
       "      <td>10.5</td>\n",
       "      <td>9.95</td>\n",
       "      <td>12.06</td>\n",
       "      <td>11.23</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.007083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-25</th>\n",
       "      <td>13.25</td>\n",
       "      <td>10.42</td>\n",
       "      <td>12.56</td>\n",
       "      <td>11.20</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.12</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.005869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-26</th>\n",
       "      <td>12</td>\n",
       "      <td>10.63</td>\n",
       "      <td>12.53</td>\n",
       "      <td>10.93</td>\n",
       "      <td>8.05</td>\n",
       "      <td>8.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.012358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-27</th>\n",
       "      <td>10.5</td>\n",
       "      <td>9.39</td>\n",
       "      <td>12.54</td>\n",
       "      <td>10.88</td>\n",
       "      <td>8.83</td>\n",
       "      <td>9.78</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.004243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-28</th>\n",
       "      <td>12</td>\n",
       "      <td>8.30</td>\n",
       "      <td>11.95</td>\n",
       "      <td>10.96</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.009905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-29</th>\n",
       "      <td>12.94</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.82</td>\n",
       "      <td>10.97</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.005336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-30</th>\n",
       "      <td>13</td>\n",
       "      <td>7.34</td>\n",
       "      <td>11.78</td>\n",
       "      <td>10.91</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.006325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6.77</td>\n",
       "      <td>11.88</td>\n",
       "      <td>10.91</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.006325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-01</th>\n",
       "      <td>9.5</td>\n",
       "      <td>6.15</td>\n",
       "      <td>11.84</td>\n",
       "      <td>10.91</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.99</td>\n",
       "      <td>11.40</td>\n",
       "      <td>0.005725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-02</th>\n",
       "      <td>11.6575</td>\n",
       "      <td>6.84</td>\n",
       "      <td>11.74</td>\n",
       "      <td>10.78</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.006091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-03</th>\n",
       "      <td>13.5</td>\n",
       "      <td>6.36</td>\n",
       "      <td>11.71</td>\n",
       "      <td>10.19</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.005079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-04</th>\n",
       "      <td>12.96</td>\n",
       "      <td>6.45</td>\n",
       "      <td>10.04</td>\n",
       "      <td>10.19</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.004713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-05</th>\n",
       "      <td>12.16</td>\n",
       "      <td>7.25</td>\n",
       "      <td>9.90</td>\n",
       "      <td>10.19</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.004713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-06</th>\n",
       "      <td>9.655</td>\n",
       "      <td>7.63</td>\n",
       "      <td>9.93</td>\n",
       "      <td>10.19</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.006667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-07</th>\n",
       "      <td>12.6</td>\n",
       "      <td>7.08</td>\n",
       "      <td>9.94</td>\n",
       "      <td>10.19</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.33</td>\n",
       "      <td>9.93</td>\n",
       "      <td>10.21</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-09</th>\n",
       "      <td>11</td>\n",
       "      <td>6.90</td>\n",
       "      <td>10.06</td>\n",
       "      <td>10.21</td>\n",
       "      <td>8.88</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.004469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6.44</td>\n",
       "      <td>10.72</td>\n",
       "      <td>10.19</td>\n",
       "      <td>8.13</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.004199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-11</th>\n",
       "      <td>12.99</td>\n",
       "      <td>6.29</td>\n",
       "      <td>10.72</td>\n",
       "      <td>10.19</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.004199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-12</th>\n",
       "      <td>11</td>\n",
       "      <td>6.44</td>\n",
       "      <td>10.73</td>\n",
       "      <td>10.19</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6.41</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.19</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-14</th>\n",
       "      <td>13.95</td>\n",
       "      <td>6.07</td>\n",
       "      <td>10.73</td>\n",
       "      <td>10.19</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-15</th>\n",
       "      <td>13.35</td>\n",
       "      <td>5.26</td>\n",
       "      <td>10.72</td>\n",
       "      <td>10.19</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.002473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-16</th>\n",
       "      <td>12.0333</td>\n",
       "      <td>5.20</td>\n",
       "      <td>10.74</td>\n",
       "      <td>10.19</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.002666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-17</th>\n",
       "      <td>14.11</td>\n",
       "      <td>5.18</td>\n",
       "      <td>11.01</td>\n",
       "      <td>10.19</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.004693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.16</td>\n",
       "      <td>11.05</td>\n",
       "      <td>10.19</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-19</th>\n",
       "      <td>14.99</td>\n",
       "      <td>5.07</td>\n",
       "      <td>11.13</td>\n",
       "      <td>10.19</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.94</td>\n",
       "      <td>11.42</td>\n",
       "      <td>10.19</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.44</td>\n",
       "      <td>0.003334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.37</td>\n",
       "      <td>11.42</td>\n",
       "      <td>10.19</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.40</td>\n",
       "      <td>0.003334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-22</th>\n",
       "      <td>12.1167</td>\n",
       "      <td>4.23</td>\n",
       "      <td>11.42</td>\n",
       "      <td>10.15</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.003810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.27</td>\n",
       "      <td>11.35</td>\n",
       "      <td>10.11</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.001994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-24</th>\n",
       "      <td>12</td>\n",
       "      <td>4.35</td>\n",
       "      <td>11.04</td>\n",
       "      <td>10.11</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.002899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-25</th>\n",
       "      <td>10.5</td>\n",
       "      <td>4.31</td>\n",
       "      <td>11.05</td>\n",
       "      <td>10.11</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.002668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.75</td>\n",
       "      <td>11.05</td>\n",
       "      <td>10.11</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.002668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.87</td>\n",
       "      <td>11.05</td>\n",
       "      <td>10.11</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.004241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.99</td>\n",
       "      <td>11.02</td>\n",
       "      <td>10.04</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.001975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-29</th>\n",
       "      <td>11.5</td>\n",
       "      <td>5.39</td>\n",
       "      <td>10.92</td>\n",
       "      <td>10.03</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.002667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01</th>\n",
       "      <td>11.99</td>\n",
       "      <td>5.20</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.03</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.002435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-02</th>\n",
       "      <td>12</td>\n",
       "      <td>4.30</td>\n",
       "      <td>10.68</td>\n",
       "      <td>10.03</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.002994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.88</td>\n",
       "      <td>9.96</td>\n",
       "      <td>9.94</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.002994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-04</th>\n",
       "      <td>10.5475</td>\n",
       "      <td>4.51</td>\n",
       "      <td>9.84</td>\n",
       "      <td>9.72</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.000494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-05</th>\n",
       "      <td>17.19</td>\n",
       "      <td>4.34</td>\n",
       "      <td>9.79</td>\n",
       "      <td>9.72</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.004127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-06</th>\n",
       "      <td>10.0125</td>\n",
       "      <td>4.17</td>\n",
       "      <td>9.79</td>\n",
       "      <td>9.70</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.002319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-07</th>\n",
       "      <td>11.9</td>\n",
       "      <td>4.50</td>\n",
       "      <td>9.79</td>\n",
       "      <td>9.64</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.001212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-08</th>\n",
       "      <td>11.49</td>\n",
       "      <td>4.92</td>\n",
       "      <td>9.79</td>\n",
       "      <td>9.64</td>\n",
       "      <td>8.05</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.002919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-09</th>\n",
       "      <td>9.615</td>\n",
       "      <td>4.61</td>\n",
       "      <td>9.68</td>\n",
       "      <td>9.60</td>\n",
       "      <td>6.67</td>\n",
       "      <td>9.99</td>\n",
       "      <td>8.44</td>\n",
       "      <td>0.005786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-10</th>\n",
       "      <td>13</td>\n",
       "      <td>4.76</td>\n",
       "      <td>9.66</td>\n",
       "      <td>9.49</td>\n",
       "      <td>5.39</td>\n",
       "      <td>9.99</td>\n",
       "      <td>7.72</td>\n",
       "      <td>0.002133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-11</th>\n",
       "      <td>10.615</td>\n",
       "      <td>5.71</td>\n",
       "      <td>9.66</td>\n",
       "      <td>9.49</td>\n",
       "      <td>5.39</td>\n",
       "      <td>9.99</td>\n",
       "      <td>7.72</td>\n",
       "      <td>0.007312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6.37</td>\n",
       "      <td>9.54</td>\n",
       "      <td>9.47</td>\n",
       "      <td>5.39</td>\n",
       "      <td>9.99</td>\n",
       "      <td>7.29</td>\n",
       "      <td>0.005458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-13</th>\n",
       "      <td>8.285</td>\n",
       "      <td>6.47</td>\n",
       "      <td>9.09</td>\n",
       "      <td>8.96</td>\n",
       "      <td>5.69</td>\n",
       "      <td>9.49</td>\n",
       "      <td>6.99</td>\n",
       "      <td>0.006126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-14</th>\n",
       "      <td>8.33</td>\n",
       "      <td>6.67</td>\n",
       "      <td>8.64</td>\n",
       "      <td>8.45</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.95</td>\n",
       "      <td>6.99</td>\n",
       "      <td>0.008757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-15</th>\n",
       "      <td>9.19</td>\n",
       "      <td>6.46</td>\n",
       "      <td>8.29</td>\n",
       "      <td>8.35</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "      <td>6.99</td>\n",
       "      <td>0.003810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-16</th>\n",
       "      <td>6.5</td>\n",
       "      <td>5.75</td>\n",
       "      <td>7.91</td>\n",
       "      <td>8.29</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "      <td>6.99</td>\n",
       "      <td>0.002581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-17</th>\n",
       "      <td>8.4</td>\n",
       "      <td>5.43</td>\n",
       "      <td>7.89</td>\n",
       "      <td>8.56</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "      <td>6.99</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.06</td>\n",
       "      <td>7.89</td>\n",
       "      <td>8.56</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "      <td>6.99</td>\n",
       "      <td>0.002020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.12</td>\n",
       "      <td>7.89</td>\n",
       "      <td>8.47</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "      <td>6.99</td>\n",
       "      <td>0.003165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-20</th>\n",
       "      <td>6.7</td>\n",
       "      <td>4.50</td>\n",
       "      <td>7.89</td>\n",
       "      <td>8.09</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "      <td>6.53</td>\n",
       "      <td>0.001778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-21</th>\n",
       "      <td>9.72</td>\n",
       "      <td>4.53</td>\n",
       "      <td>7.89</td>\n",
       "      <td>8.01</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "      <td>6.08</td>\n",
       "      <td>0.002083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-22</th>\n",
       "      <td>7.95</td>\n",
       "      <td>4.28</td>\n",
       "      <td>7.79</td>\n",
       "      <td>6.31</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "      <td>5.53</td>\n",
       "      <td>0.001667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-23</th>\n",
       "      <td>7.28</td>\n",
       "      <td>4.22</td>\n",
       "      <td>7.87</td>\n",
       "      <td>4.99</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "      <td>5.53</td>\n",
       "      <td>0.002754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-24</th>\n",
       "      <td>5.15</td>\n",
       "      <td>3.96</td>\n",
       "      <td>7.97</td>\n",
       "      <td>4.77</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "      <td>5.68</td>\n",
       "      <td>0.002754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.93</td>\n",
       "      <td>7.67</td>\n",
       "      <td>4.64</td>\n",
       "      <td>6.69</td>\n",
       "      <td>8.99</td>\n",
       "      <td>6.26</td>\n",
       "      <td>0.002754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.54</td>\n",
       "      <td>7.63</td>\n",
       "      <td>4.30</td>\n",
       "      <td>6.72</td>\n",
       "      <td>8.99</td>\n",
       "      <td>5.56</td>\n",
       "      <td>0.002910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.73</td>\n",
       "      <td>7.63</td>\n",
       "      <td>4.29</td>\n",
       "      <td>6.72</td>\n",
       "      <td>8.99</td>\n",
       "      <td>5.53</td>\n",
       "      <td>0.002910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.51</td>\n",
       "      <td>7.60</td>\n",
       "      <td>4.82</td>\n",
       "      <td>6.72</td>\n",
       "      <td>8.99</td>\n",
       "      <td>5.53</td>\n",
       "      <td>0.002597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-29</th>\n",
       "      <td>6.98</td>\n",
       "      <td>5.07</td>\n",
       "      <td>7.19</td>\n",
       "      <td>5.98</td>\n",
       "      <td>6.97</td>\n",
       "      <td>8.08</td>\n",
       "      <td>5.02</td>\n",
       "      <td>0.002597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-30</th>\n",
       "      <td>7.3725</td>\n",
       "      <td>5.32</td>\n",
       "      <td>7.06</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.99</td>\n",
       "      <td>7.99</td>\n",
       "      <td>5.41</td>\n",
       "      <td>0.002549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-31</th>\n",
       "      <td>5.5</td>\n",
       "      <td>4.70</td>\n",
       "      <td>7.05</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.99</td>\n",
       "      <td>7.99</td>\n",
       "      <td>5.58</td>\n",
       "      <td>0.001441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-01</th>\n",
       "      <td>7</td>\n",
       "      <td>4.63</td>\n",
       "      <td>7.05</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.99</td>\n",
       "      <td>8.45</td>\n",
       "      <td>6.67</td>\n",
       "      <td>0.001441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-02</th>\n",
       "      <td>6.75</td>\n",
       "      <td>4.42</td>\n",
       "      <td>7.05</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.99</td>\n",
       "      <td>8.98</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.003972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-03</th>\n",
       "      <td>6.86</td>\n",
       "      <td>4.52</td>\n",
       "      <td>7.05</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.99</td>\n",
       "      <td>8.98</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.003972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.53</td>\n",
       "      <td>7.05</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.99</td>\n",
       "      <td>8.98</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.003158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-05</th>\n",
       "      <td>6.7025</td>\n",
       "      <td>4.29</td>\n",
       "      <td>6.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.99</td>\n",
       "      <td>8.98</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.003158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-06</th>\n",
       "      <td>7.74333</td>\n",
       "      <td>4.79</td>\n",
       "      <td>6.60</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.99</td>\n",
       "      <td>8.98</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.004267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.49</td>\n",
       "      <td>6.61</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.99</td>\n",
       "      <td>8.98</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.003619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-08</th>\n",
       "      <td>6</td>\n",
       "      <td>4.85</td>\n",
       "      <td>6.34</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.99</td>\n",
       "      <td>8.98</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.001481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-09</th>\n",
       "      <td>9.226</td>\n",
       "      <td>4.77</td>\n",
       "      <td>6.32</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.99</td>\n",
       "      <td>8.98</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-10</th>\n",
       "      <td>6.75</td>\n",
       "      <td>4.91</td>\n",
       "      <td>6.33</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.99</td>\n",
       "      <td>8.83</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.000556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-11</th>\n",
       "      <td>6.25</td>\n",
       "      <td>4.53</td>\n",
       "      <td>6.16</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.78</td>\n",
       "      <td>8.25</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.002667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-12</th>\n",
       "      <td>5.99</td>\n",
       "      <td>3.85</td>\n",
       "      <td>5.94</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.26</td>\n",
       "      <td>8.25</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.002667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.68</td>\n",
       "      <td>5.75</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.26</td>\n",
       "      <td>8.25</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.004444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.66</td>\n",
       "      <td>5.77</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.26</td>\n",
       "      <td>8.25</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.004444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-15</th>\n",
       "      <td>6.745</td>\n",
       "      <td>3.60</td>\n",
       "      <td>5.81</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.26</td>\n",
       "      <td>8.25</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.004444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-16</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.75</td>\n",
       "      <td>5.81</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.26</td>\n",
       "      <td>8.25</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.002370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-17</th>\n",
       "      <td>5.67</td>\n",
       "      <td>3.80</td>\n",
       "      <td>5.82</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.26</td>\n",
       "      <td>8.25</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.001767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-18</th>\n",
       "      <td>7.75</td>\n",
       "      <td>3.92</td>\n",
       "      <td>5.85</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.26</td>\n",
       "      <td>8.25</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.001767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-19</th>\n",
       "      <td>8.225</td>\n",
       "      <td>3.83</td>\n",
       "      <td>5.64</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.26</td>\n",
       "      <td>8.25</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.001767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-20</th>\n",
       "      <td>7.005</td>\n",
       "      <td>3.83</td>\n",
       "      <td>5.53</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.26</td>\n",
       "      <td>8.25</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.004063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-21</th>\n",
       "      <td>7</td>\n",
       "      <td>3.67</td>\n",
       "      <td>5.56</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.26</td>\n",
       "      <td>7.89</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.004678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-22</th>\n",
       "      <td>6.99</td>\n",
       "      <td>3.88</td>\n",
       "      <td>5.58</td>\n",
       "      <td>6.22</td>\n",
       "      <td>6.26</td>\n",
       "      <td>6.79</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.002388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-23</th>\n",
       "      <td>8.75</td>\n",
       "      <td>3.90</td>\n",
       "      <td>5.71</td>\n",
       "      <td>6.28</td>\n",
       "      <td>6.26</td>\n",
       "      <td>6.79</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-24</th>\n",
       "      <td>6.695</td>\n",
       "      <td>4.02</td>\n",
       "      <td>5.86</td>\n",
       "      <td>6.28</td>\n",
       "      <td>6.26</td>\n",
       "      <td>6.19</td>\n",
       "      <td>6.85</td>\n",
       "      <td>0.004425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-25</th>\n",
       "      <td>6.225</td>\n",
       "      <td>4.36</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.51</td>\n",
       "      <td>6.26</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-26</th>\n",
       "      <td>6.99</td>\n",
       "      <td>4.61</td>\n",
       "      <td>6.33</td>\n",
       "      <td>6.57</td>\n",
       "      <td>6.26</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.003917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-27</th>\n",
       "      <td>6.262</td>\n",
       "      <td>4.70</td>\n",
       "      <td>5.80</td>\n",
       "      <td>6.57</td>\n",
       "      <td>6.26</td>\n",
       "      <td>6.39</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.007182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-28</th>\n",
       "      <td>8.1</td>\n",
       "      <td>4.64</td>\n",
       "      <td>5.73</td>\n",
       "      <td>6.57</td>\n",
       "      <td>6.26</td>\n",
       "      <td>6.72</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.009877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-29</th>\n",
       "      <td>5.99</td>\n",
       "      <td>4.73</td>\n",
       "      <td>5.83</td>\n",
       "      <td>6.57</td>\n",
       "      <td>6.26</td>\n",
       "      <td>6.72</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.002853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-30</th>\n",
       "      <td>6.39</td>\n",
       "      <td>4.87</td>\n",
       "      <td>5.89</td>\n",
       "      <td>6.57</td>\n",
       "      <td>6.26</td>\n",
       "      <td>6.72</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.009864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01</th>\n",
       "      <td>6.83083</td>\n",
       "      <td>4.82</td>\n",
       "      <td>5.84</td>\n",
       "      <td>6.57</td>\n",
       "      <td>6.26</td>\n",
       "      <td>6.72</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.005427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-02</th>\n",
       "      <td>9.135</td>\n",
       "      <td>4.84</td>\n",
       "      <td>6.13</td>\n",
       "      <td>6.57</td>\n",
       "      <td>6.26</td>\n",
       "      <td>6.72</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-03</th>\n",
       "      <td>8.515</td>\n",
       "      <td>5.18</td>\n",
       "      <td>6.28</td>\n",
       "      <td>6.57</td>\n",
       "      <td>6.26</td>\n",
       "      <td>6.72</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.004753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-04</th>\n",
       "      <td>5.605</td>\n",
       "      <td>5.40</td>\n",
       "      <td>6.27</td>\n",
       "      <td>6.57</td>\n",
       "      <td>6.26</td>\n",
       "      <td>6.54</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-05</th>\n",
       "      <td>7.74</td>\n",
       "      <td>5.71</td>\n",
       "      <td>6.21</td>\n",
       "      <td>6.57</td>\n",
       "      <td>6.26</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.002332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-06</th>\n",
       "      <td>7.615</td>\n",
       "      <td>5.37</td>\n",
       "      <td>5.97</td>\n",
       "      <td>6.35</td>\n",
       "      <td>6.26</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.007238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-07</th>\n",
       "      <td>7</td>\n",
       "      <td>5.08</td>\n",
       "      <td>6.08</td>\n",
       "      <td>6.03</td>\n",
       "      <td>6.26</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.008254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-08</th>\n",
       "      <td>7.83</td>\n",
       "      <td>5.94</td>\n",
       "      <td>6.36</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.26</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.004853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6.28</td>\n",
       "      <td>6.69</td>\n",
       "      <td>6.11</td>\n",
       "      <td>6.35</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-10</th>\n",
       "      <td>7.095</td>\n",
       "      <td>5.92</td>\n",
       "      <td>6.92</td>\n",
       "      <td>6.32</td>\n",
       "      <td>6.99</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.008847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.30</td>\n",
       "      <td>6.96</td>\n",
       "      <td>6.28</td>\n",
       "      <td>6.99</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.010591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-12</th>\n",
       "      <td>8.29</td>\n",
       "      <td>4.41</td>\n",
       "      <td>7.13</td>\n",
       "      <td>6.28</td>\n",
       "      <td>6.84</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.011673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-13</th>\n",
       "      <td>8.37</td>\n",
       "      <td>4.87</td>\n",
       "      <td>7.25</td>\n",
       "      <td>6.28</td>\n",
       "      <td>6.26</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.009206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-14</th>\n",
       "      <td>11.99</td>\n",
       "      <td>5.35</td>\n",
       "      <td>7.23</td>\n",
       "      <td>6.28</td>\n",
       "      <td>6.17</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.015054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-15</th>\n",
       "      <td>9.975</td>\n",
       "      <td>5.69</td>\n",
       "      <td>7.14</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.53</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.009061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-16</th>\n",
       "      <td>6.48333</td>\n",
       "      <td>5.95</td>\n",
       "      <td>7.15</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.53</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.010448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-17</th>\n",
       "      <td>7.33</td>\n",
       "      <td>5.55</td>\n",
       "      <td>7.20</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.53</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-18</th>\n",
       "      <td>5.87</td>\n",
       "      <td>5.40</td>\n",
       "      <td>7.31</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.15</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.011435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-19</th>\n",
       "      <td>6.8</td>\n",
       "      <td>5.52</td>\n",
       "      <td>7.31</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.37</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-20</th>\n",
       "      <td>9.97</td>\n",
       "      <td>5.60</td>\n",
       "      <td>7.08</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.73</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-21</th>\n",
       "      <td>5.75</td>\n",
       "      <td>5.39</td>\n",
       "      <td>6.77</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.73</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.011828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-22</th>\n",
       "      <td>9.655</td>\n",
       "      <td>4.73</td>\n",
       "      <td>6.77</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.36</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.011765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-23</th>\n",
       "      <td>5.97</td>\n",
       "      <td>5.56</td>\n",
       "      <td>7.67</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.21</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.009612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-24</th>\n",
       "      <td>6.495</td>\n",
       "      <td>4.99</td>\n",
       "      <td>7.68</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.21</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.011019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-25</th>\n",
       "      <td>7.5</td>\n",
       "      <td>5.17</td>\n",
       "      <td>7.67</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.21</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.004350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.32</td>\n",
       "      <td>7.70</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.21</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.009394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-27</th>\n",
       "      <td>5.99</td>\n",
       "      <td>4.86</td>\n",
       "      <td>7.68</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.21</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.009394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-28</th>\n",
       "      <td>7</td>\n",
       "      <td>4.47</td>\n",
       "      <td>7.67</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.21</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.008832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-29</th>\n",
       "      <td>5.915</td>\n",
       "      <td>4.47</td>\n",
       "      <td>7.67</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.21</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.008594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.25</td>\n",
       "      <td>7.67</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.21</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.013740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.87</td>\n",
       "      <td>7.67</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.21</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.013740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-01</th>\n",
       "      <td>7.215</td>\n",
       "      <td>3.99</td>\n",
       "      <td>7.66</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.21</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.011273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-02</th>\n",
       "      <td>6.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>7.44</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.51</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.011273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-03</th>\n",
       "      <td>8.21</td>\n",
       "      <td>3.82</td>\n",
       "      <td>7.44</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.009249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-04</th>\n",
       "      <td>7.005</td>\n",
       "      <td>4.24</td>\n",
       "      <td>7.50</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.004619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-05</th>\n",
       "      <td>6.25</td>\n",
       "      <td>4.55</td>\n",
       "      <td>7.50</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-06</th>\n",
       "      <td>6.99</td>\n",
       "      <td>4.39</td>\n",
       "      <td>7.50</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.009127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-07</th>\n",
       "      <td>6.93333</td>\n",
       "      <td>4.43</td>\n",
       "      <td>7.47</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-08</th>\n",
       "      <td>7.22</td>\n",
       "      <td>4.27</td>\n",
       "      <td>7.45</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.003941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-09</th>\n",
       "      <td>6.99</td>\n",
       "      <td>4.09</td>\n",
       "      <td>7.45</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.002575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-10</th>\n",
       "      <td>6.3075</td>\n",
       "      <td>4.13</td>\n",
       "      <td>7.40</td>\n",
       "      <td>6.29</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.002575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-11</th>\n",
       "      <td>6.245</td>\n",
       "      <td>4.59</td>\n",
       "      <td>7.40</td>\n",
       "      <td>6.57</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.008520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.08</td>\n",
       "      <td>7.40</td>\n",
       "      <td>6.49</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.008520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.49</td>\n",
       "      <td>7.40</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.004878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-14</th>\n",
       "      <td>4.24</td>\n",
       "      <td>4.66</td>\n",
       "      <td>7.43</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.003689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-15</th>\n",
       "      <td>7.745</td>\n",
       "      <td>4.72</td>\n",
       "      <td>7.43</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.008790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-16</th>\n",
       "      <td>6.37</td>\n",
       "      <td>4.84</td>\n",
       "      <td>7.29</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.005594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.69</td>\n",
       "      <td>7.17</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.005755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.08</td>\n",
       "      <td>7.18</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.012603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-19</th>\n",
       "      <td>1.75</td>\n",
       "      <td>3.78</td>\n",
       "      <td>7.17</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.012603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-20</th>\n",
       "      <td>2.99</td>\n",
       "      <td>3.82</td>\n",
       "      <td>7.15</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.005242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-21</th>\n",
       "      <td>4.99</td>\n",
       "      <td>3.15</td>\n",
       "      <td>7.08</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.004918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.19</td>\n",
       "      <td>7.07</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.010958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-23</th>\n",
       "      <td>6.74</td>\n",
       "      <td>3.37</td>\n",
       "      <td>7.07</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.010958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.40</td>\n",
       "      <td>7.07</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.003896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-25</th>\n",
       "      <td>4.69</td>\n",
       "      <td>3.59</td>\n",
       "      <td>7.05</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.004317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-26</th>\n",
       "      <td>5.5</td>\n",
       "      <td>3.62</td>\n",
       "      <td>7.05</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.004317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-27</th>\n",
       "      <td>5.99</td>\n",
       "      <td>3.72</td>\n",
       "      <td>7.07</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.40</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.002846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-28</th>\n",
       "      <td>3.75</td>\n",
       "      <td>3.67</td>\n",
       "      <td>7.07</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.002846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-29</th>\n",
       "      <td>4.995</td>\n",
       "      <td>3.58</td>\n",
       "      <td>7.06</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.008284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.71</td>\n",
       "      <td>7.06</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.008284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-01</th>\n",
       "      <td>5</td>\n",
       "      <td>3.35</td>\n",
       "      <td>6.92</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.009467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.23</td>\n",
       "      <td>6.64</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.004380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-03</th>\n",
       "      <td>6.24</td>\n",
       "      <td>3.21</td>\n",
       "      <td>6.50</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.004380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-04</th>\n",
       "      <td>5.97</td>\n",
       "      <td>3.20</td>\n",
       "      <td>6.50</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.004380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.45</td>\n",
       "      <td>6.50</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.004380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-06</th>\n",
       "      <td>5.1225</td>\n",
       "      <td>3.25</td>\n",
       "      <td>6.50</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.005461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.92</td>\n",
       "      <td>6.50</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.005461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-08</th>\n",
       "      <td>2</td>\n",
       "      <td>2.87</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.005461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.81</td>\n",
       "      <td>6.40</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.008186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-10</th>\n",
       "      <td>3.7875</td>\n",
       "      <td>2.66</td>\n",
       "      <td>6.13</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.008186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11</th>\n",
       "      <td>6.385</td>\n",
       "      <td>2.52</td>\n",
       "      <td>5.82</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.004065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.35</td>\n",
       "      <td>6.09</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.001639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.01</td>\n",
       "      <td>6.11</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.002717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-14</th>\n",
       "      <td>5.12</td>\n",
       "      <td>3.48</td>\n",
       "      <td>6.11</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.002717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-15</th>\n",
       "      <td>5.49</td>\n",
       "      <td>3.37</td>\n",
       "      <td>6.11</td>\n",
       "      <td>6.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.002717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-16</th>\n",
       "      <td>8.465</td>\n",
       "      <td>3.61</td>\n",
       "      <td>6.11</td>\n",
       "      <td>6.12</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.007833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-17</th>\n",
       "      <td>6.28</td>\n",
       "      <td>3.46</td>\n",
       "      <td>5.93</td>\n",
       "      <td>5.59</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.007833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.31</td>\n",
       "      <td>5.82</td>\n",
       "      <td>5.20</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-19</th>\n",
       "      <td>7.87</td>\n",
       "      <td>3.13</td>\n",
       "      <td>5.82</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-20</th>\n",
       "      <td>5.87</td>\n",
       "      <td>3.08</td>\n",
       "      <td>5.76</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.004165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.84</td>\n",
       "      <td>5.47</td>\n",
       "      <td>4.49</td>\n",
       "      <td>5.17</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.004165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.78</td>\n",
       "      <td>5.14</td>\n",
       "      <td>4.28</td>\n",
       "      <td>4.82</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.002956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-23</th>\n",
       "      <td>5.95</td>\n",
       "      <td>2.68</td>\n",
       "      <td>5.34</td>\n",
       "      <td>4.28</td>\n",
       "      <td>4.47</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.011145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-24</th>\n",
       "      <td>7.25</td>\n",
       "      <td>2.84</td>\n",
       "      <td>5.28</td>\n",
       "      <td>4.24</td>\n",
       "      <td>3.99</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.000952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-25</th>\n",
       "      <td>6.775</td>\n",
       "      <td>2.97</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.17</td>\n",
       "      <td>3.99</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.000417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-26</th>\n",
       "      <td>5.5</td>\n",
       "      <td>3.27</td>\n",
       "      <td>5.42</td>\n",
       "      <td>4.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.002016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-27</th>\n",
       "      <td>5.485</td>\n",
       "      <td>3.37</td>\n",
       "      <td>5.79</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.99</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.003354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.21</td>\n",
       "      <td>5.60</td>\n",
       "      <td>4.08</td>\n",
       "      <td>3.99</td>\n",
       "      <td>5.77</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.003354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.34</td>\n",
       "      <td>5.54</td>\n",
       "      <td>4.10</td>\n",
       "      <td>3.99</td>\n",
       "      <td>5.33</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.003354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-30</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.29</td>\n",
       "      <td>5.54</td>\n",
       "      <td>4.10</td>\n",
       "      <td>3.99</td>\n",
       "      <td>5.33</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.003754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31</th>\n",
       "      <td>6.96</td>\n",
       "      <td>3.26</td>\n",
       "      <td>5.54</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.43</td>\n",
       "      <td>5.33</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.003754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.98</td>\n",
       "      <td>5.40</td>\n",
       "      <td>4.02</td>\n",
       "      <td>4.45</td>\n",
       "      <td>5.06</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.002049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-02</th>\n",
       "      <td>6.39</td>\n",
       "      <td>2.78</td>\n",
       "      <td>5.30</td>\n",
       "      <td>3.97</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.003226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.32</td>\n",
       "      <td>3.89</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-04</th>\n",
       "      <td>5.73667</td>\n",
       "      <td>4.53</td>\n",
       "      <td>5.78</td>\n",
       "      <td>4.76</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.75</td>\n",
       "      <td>6.18</td>\n",
       "      <td>5.09</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.012587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-06</th>\n",
       "      <td>5.48</td>\n",
       "      <td>4.82</td>\n",
       "      <td>6.18</td>\n",
       "      <td>5.13</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.007494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-07</th>\n",
       "      <td>5.49</td>\n",
       "      <td>4.98</td>\n",
       "      <td>6.22</td>\n",
       "      <td>5.37</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.007494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-08</th>\n",
       "      <td>5.33167</td>\n",
       "      <td>4.86</td>\n",
       "      <td>6.78</td>\n",
       "      <td>5.66</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.005741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.40</td>\n",
       "      <td>6.72</td>\n",
       "      <td>5.76</td>\n",
       "      <td>4.64</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.004002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.56</td>\n",
       "      <td>6.68</td>\n",
       "      <td>5.83</td>\n",
       "      <td>4.91</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.007080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.48</td>\n",
       "      <td>6.25</td>\n",
       "      <td>5.83</td>\n",
       "      <td>4.91</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.008333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-12</th>\n",
       "      <td>5.99</td>\n",
       "      <td>3.77</td>\n",
       "      <td>6.02</td>\n",
       "      <td>5.83</td>\n",
       "      <td>4.91</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.008333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-13</th>\n",
       "      <td>4.75</td>\n",
       "      <td>3.70</td>\n",
       "      <td>6.02</td>\n",
       "      <td>5.92</td>\n",
       "      <td>4.91</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.005949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-14</th>\n",
       "      <td>9.74</td>\n",
       "      <td>3.39</td>\n",
       "      <td>6.02</td>\n",
       "      <td>5.92</td>\n",
       "      <td>4.91</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.005949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.67</td>\n",
       "      <td>6.02</td>\n",
       "      <td>5.92</td>\n",
       "      <td>4.91</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.010937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-16</th>\n",
       "      <td>2.785</td>\n",
       "      <td>4.17</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.06</td>\n",
       "      <td>4.91</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.010085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.42</td>\n",
       "      <td>6.49</td>\n",
       "      <td>6.12</td>\n",
       "      <td>4.91</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.009156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-18</th>\n",
       "      <td>5.965</td>\n",
       "      <td>3.96</td>\n",
       "      <td>6.49</td>\n",
       "      <td>6.12</td>\n",
       "      <td>4.91</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.009889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.77</td>\n",
       "      <td>6.48</td>\n",
       "      <td>6.12</td>\n",
       "      <td>4.91</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.016343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.06</td>\n",
       "      <td>6.46</td>\n",
       "      <td>6.05</td>\n",
       "      <td>4.91</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.007507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.27</td>\n",
       "      <td>6.46</td>\n",
       "      <td>6.04</td>\n",
       "      <td>4.91</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.003089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-22</th>\n",
       "      <td>6.04</td>\n",
       "      <td>4.27</td>\n",
       "      <td>6.46</td>\n",
       "      <td>6.04</td>\n",
       "      <td>4.91</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.003089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-23</th>\n",
       "      <td>7.325</td>\n",
       "      <td>4.30</td>\n",
       "      <td>6.57</td>\n",
       "      <td>6.04</td>\n",
       "      <td>4.91</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.003089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.07</td>\n",
       "      <td>6.91</td>\n",
       "      <td>6.04</td>\n",
       "      <td>4.91</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-25</th>\n",
       "      <td>7.5625</td>\n",
       "      <td>4.25</td>\n",
       "      <td>6.36</td>\n",
       "      <td>6.04</td>\n",
       "      <td>4.87</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.005755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-26</th>\n",
       "      <td>6</td>\n",
       "      <td>3.90</td>\n",
       "      <td>6.36</td>\n",
       "      <td>6.04</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.005755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-27</th>\n",
       "      <td>5.1975</td>\n",
       "      <td>3.91</td>\n",
       "      <td>6.40</td>\n",
       "      <td>5.74</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.012325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.08</td>\n",
       "      <td>6.43</td>\n",
       "      <td>5.79</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.012560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-29</th>\n",
       "      <td>6</td>\n",
       "      <td>4.06</td>\n",
       "      <td>6.46</td>\n",
       "      <td>5.79</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.009274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-30</th>\n",
       "      <td>6.58</td>\n",
       "      <td>3.97</td>\n",
       "      <td>6.42</td>\n",
       "      <td>5.89</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.010317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.79</td>\n",
       "      <td>6.48</td>\n",
       "      <td>5.90</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.009076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-01</th>\n",
       "      <td>6.99</td>\n",
       "      <td>3.24</td>\n",
       "      <td>6.36</td>\n",
       "      <td>5.90</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.011102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-02</th>\n",
       "      <td>5.99</td>\n",
       "      <td>3.14</td>\n",
       "      <td>6.37</td>\n",
       "      <td>5.90</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.010292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-03</th>\n",
       "      <td>6.54</td>\n",
       "      <td>2.81</td>\n",
       "      <td>6.38</td>\n",
       "      <td>5.90</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-04</th>\n",
       "      <td>6.25</td>\n",
       "      <td>2.88</td>\n",
       "      <td>6.38</td>\n",
       "      <td>5.90</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-05</th>\n",
       "      <td>6.025</td>\n",
       "      <td>3.08</td>\n",
       "      <td>6.38</td>\n",
       "      <td>5.90</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.011019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-06</th>\n",
       "      <td>4.275</td>\n",
       "      <td>3.32</td>\n",
       "      <td>6.38</td>\n",
       "      <td>5.90</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.004920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-07</th>\n",
       "      <td>8</td>\n",
       "      <td>3.10</td>\n",
       "      <td>6.20</td>\n",
       "      <td>5.90</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.007251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.95</td>\n",
       "      <td>6.20</td>\n",
       "      <td>5.90</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.005755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-09</th>\n",
       "      <td>8</td>\n",
       "      <td>2.75</td>\n",
       "      <td>6.17</td>\n",
       "      <td>5.59</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.005755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-10</th>\n",
       "      <td>6.875</td>\n",
       "      <td>2.86</td>\n",
       "      <td>6.13</td>\n",
       "      <td>5.44</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.005355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-11</th>\n",
       "      <td>6.245</td>\n",
       "      <td>3.09</td>\n",
       "      <td>6.13</td>\n",
       "      <td>5.44</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.005355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.31</td>\n",
       "      <td>6.10</td>\n",
       "      <td>5.39</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.010813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.18</td>\n",
       "      <td>5.95</td>\n",
       "      <td>5.36</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.011024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-14</th>\n",
       "      <td>10</td>\n",
       "      <td>3.16</td>\n",
       "      <td>5.86</td>\n",
       "      <td>5.36</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.016552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.22</td>\n",
       "      <td>5.35</td>\n",
       "      <td>5.36</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.019868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-16</th>\n",
       "      <td>5.975</td>\n",
       "      <td>3.19</td>\n",
       "      <td>5.37</td>\n",
       "      <td>5.37</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.019868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-17</th>\n",
       "      <td>5.75</td>\n",
       "      <td>2.84</td>\n",
       "      <td>5.45</td>\n",
       "      <td>5.29</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-18</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.97</td>\n",
       "      <td>5.52</td>\n",
       "      <td>5.34</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.005495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.93</td>\n",
       "      <td>5.44</td>\n",
       "      <td>5.35</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.004689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-20</th>\n",
       "      <td>5.915</td>\n",
       "      <td>2.68</td>\n",
       "      <td>5.40</td>\n",
       "      <td>5.35</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-21</th>\n",
       "      <td>7.91</td>\n",
       "      <td>2.70</td>\n",
       "      <td>5.39</td>\n",
       "      <td>5.35</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-22</th>\n",
       "      <td>6.14</td>\n",
       "      <td>2.68</td>\n",
       "      <td>5.39</td>\n",
       "      <td>5.35</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.60</td>\n",
       "      <td>5.43</td>\n",
       "      <td>5.36</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.56</td>\n",
       "      <td>5.43</td>\n",
       "      <td>5.36</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.84</td>\n",
       "      <td>5.43</td>\n",
       "      <td>5.36</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-26</th>\n",
       "      <td>5.99</td>\n",
       "      <td>2.98</td>\n",
       "      <td>5.46</td>\n",
       "      <td>5.43</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.11</td>\n",
       "      <td>5.49</td>\n",
       "      <td>4.95</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-28</th>\n",
       "      <td>2.925</td>\n",
       "      <td>2.87</td>\n",
       "      <td>5.50</td>\n",
       "      <td>4.93</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.87</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.80</td>\n",
       "      <td>5.49</td>\n",
       "      <td>4.93</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30</th>\n",
       "      <td>4.99</td>\n",
       "      <td>2.80</td>\n",
       "      <td>5.49</td>\n",
       "      <td>4.93</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01</th>\n",
       "      <td>5.75</td>\n",
       "      <td>2.80</td>\n",
       "      <td>5.49</td>\n",
       "      <td>4.93</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.99</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.006433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           MINAVGMED  price0  price1  price2  price3  price4  price5     ratio\n",
       "carddate                                                                      \n",
       "2017-06-29  NaN       2.14    1.49    3.05    2.53    1.31    10.99   0.003750\n",
       "2017-06-30  NaN       2.50    1.49    3.10    2.53    1.31    10.99   0.003750\n",
       "2017-07-01  NaN       2.85    1.49    3.26    2.28    1.31    10.99   0.003750\n",
       "2017-07-02  NaN       2.86    1.49    3.26    1.83    1.31    10.99   0.003750\n",
       "2017-07-03  NaN       3.06    1.49    3.28    2.05    1.31    10.99   0.003750\n",
       "2017-07-04  NaN       3.57    1.49    3.41    2.41    1.31    10.99   0.003750\n",
       "2017-07-05  NaN       3.54    1.49    3.70    2.57    1.31    10.99   0.003750\n",
       "2017-07-06  NaN       3.79    1.49    3.77    2.57    1.31    10.99   0.003750\n",
       "2017-07-07  NaN       3.84    1.49    3.61    2.57    1.31    10.99   0.003750\n",
       "2017-07-08  NaN       3.92    1.49    3.56    2.61    1.31    10.99   0.003750\n",
       "2017-07-09  NaN       4.16    1.49    3.49    2.86    1.31    10.99   0.003750\n",
       "2017-07-10  NaN       3.91    1.49    3.55    2.86    1.31    10.99   0.003750\n",
       "2017-07-11  NaN       3.42    1.49    3.52    2.86    1.31    10.99   0.003750\n",
       "2017-07-12  NaN       3.32    1.49    3.51    2.86    1.31    10.99   0.003750\n",
       "2017-07-13  NaN       3.11    1.49    3.51    2.86    1.31    10.99   0.003750\n",
       "2017-07-14  NaN       3.10    1.49    3.52    2.86    1.31    10.99   0.003750\n",
       "2017-07-15  NaN       3.47    1.49    3.53    2.86    1.31    10.99   0.003750\n",
       "2017-07-16  NaN       3.63    1.49    3.73    2.86    1.31    10.99   0.003750\n",
       "2017-07-17  NaN       3.59    1.49    3.71    2.86    1.31    10.99   0.003750\n",
       "2017-07-18  NaN       3.69    1.49    3.74    2.86    1.31    10.99   0.003750\n",
       "2017-07-19  NaN       3.38    1.49    3.70    2.86    1.31    10.99   0.003750\n",
       "2017-07-20  NaN       3.45    1.49    3.72    2.86    1.31    10.99   0.003750\n",
       "2017-07-21  NaN       3.60    1.49    3.62    2.98    1.31    10.99   0.003750\n",
       "2017-07-22  NaN       3.74    1.49    3.83    3.57    1.31    10.99   0.003750\n",
       "2017-07-23  NaN       3.88    1.49    4.08    3.57    1.31    10.99   0.003750\n",
       "2017-07-24  NaN       4.16    1.49    4.18    3.63    1.31    10.99   0.003750\n",
       "2017-07-25  NaN       3.97    1.49    4.24    3.96    1.31    10.99   0.003750\n",
       "2017-07-26  NaN       3.89    1.49    4.06    3.96    1.31    10.99   0.003750\n",
       "2017-07-27  NaN       3.92    1.49    4.04    4.02    1.31    10.99   0.003750\n",
       "2017-07-28  NaN       3.79    1.49    4.10    4.35    1.31    10.99   0.003750\n",
       "2017-07-29  NaN       3.66    1.49    4.22    4.35    1.31    10.99   0.003750\n",
       "2017-07-30  NaN       3.34    1.49    4.24    4.35    1.31    10.99   0.003750\n",
       "2017-07-31  NaN       3.41    1.49    4.25    4.35    1.31    10.99   0.003750\n",
       "2017-08-01  NaN       3.35    1.49    4.37    4.35    1.31    10.99   0.003750\n",
       "2017-08-02  NaN       3.09    1.49    4.37    4.35    1.31    10.99   0.003750\n",
       "2017-08-03  NaN       3.44    1.49    4.41    4.35    1.31    10.99   0.003750\n",
       "2017-08-04  NaN       3.76    1.49    4.57    4.35    1.31    10.99   0.003750\n",
       "2017-08-05  NaN       3.65    1.49    4.43    4.35    1.31    10.99   0.003750\n",
       "2017-08-06  NaN       3.40    1.49    4.44    4.35    1.31    10.99   0.003750\n",
       "2017-08-07  NaN       3.49    1.49    4.45    4.35    1.31    10.99   0.003750\n",
       "2017-08-08  NaN       3.65    1.49    4.45    4.35    1.31    10.99   0.003750\n",
       "2017-08-09  NaN       3.59    1.49    4.46    4.35    1.31    10.99   0.003750\n",
       "2017-08-10  NaN       3.66    1.49    4.51    4.24    1.31    10.99   0.003750\n",
       "2017-08-11  NaN       3.94    1.49    4.53    3.96    1.31    10.99   0.003750\n",
       "2017-08-12  NaN       3.97    1.49    4.53    3.96    1.31    10.99   0.003750\n",
       "2017-08-13  NaN       4.18    1.49    4.53    3.96    1.31    10.99   0.003750\n",
       "2017-08-14  NaN       3.83    1.49    4.37    3.96    1.31    10.99   0.003750\n",
       "2017-08-15  NaN       3.84    1.49    4.39    3.96    1.31    10.99   0.003750\n",
       "2017-08-16  NaN       3.64    1.49    4.39    3.96    1.31    10.99   0.003750\n",
       "2017-08-17  NaN       3.47    1.49    4.39    3.96    1.31    10.99   0.003750\n",
       "2017-08-18  NaN       3.23    1.49    4.38    3.96    1.31    10.99   0.003750\n",
       "2017-08-19  NaN       3.18    1.49    4.38    3.96    1.31    10.99   0.003750\n",
       "2017-08-20  NaN       3.12    1.49    4.38    3.96    1.31    10.99   0.003750\n",
       "2017-08-21  NaN       2.93    1.49    4.23    3.96    1.31    10.99   0.003750\n",
       "2017-08-22  NaN       2.40    1.49    4.24    3.96    1.31    10.99   0.003750\n",
       "2017-08-23  NaN       2.27    1.49    4.24    3.96    1.31    10.99   0.003750\n",
       "2017-08-24  NaN       2.27    1.49    4.24    3.96    1.31    10.99   0.003750\n",
       "2017-08-25  NaN       2.63    1.49    4.24    3.96    1.31    10.99   0.003750\n",
       "2017-08-26  NaN       2.49    1.49    4.25    3.96    1.31    10.99   0.003750\n",
       "2017-08-27  NaN       2.65    1.49    4.25    3.96    1.31    10.99   0.003750\n",
       "2017-08-28  NaN       2.79    1.49    4.25    3.96    1.31    10.99   0.003750\n",
       "2017-08-29  NaN       2.56    1.49    4.26    3.96    1.31    10.99   0.003750\n",
       "2017-08-30  NaN       2.68    1.49    4.25    3.96    1.31    10.99   0.003750\n",
       "2017-08-31  NaN       2.81    1.49    4.25    3.96    1.31    10.99   0.003750\n",
       "2017-09-01  NaN       3.20    1.49    4.25    3.96    1.31    10.99   0.003750\n",
       "2017-09-02  NaN       3.35    1.49    4.25    3.96    1.31    10.99   0.003750\n",
       "2017-09-03  NaN       3.72    1.49    4.26    3.96    1.31    10.99   0.003750\n",
       "2017-09-04  NaN       4.15    1.49    4.26    3.96    1.31    10.99   0.003750\n",
       "2017-09-05  NaN       4.05    1.49    4.26    3.96    1.31    10.99   0.003750\n",
       "2017-09-06  NaN       3.59    1.49    4.26    3.96    1.31    10.99   0.003750\n",
       "2017-09-07  NaN       3.57    1.49    4.27    3.96    1.31    10.99   0.003750\n",
       "2017-09-08  NaN       3.05    1.49    4.23    3.88    1.31    10.74   0.003750\n",
       "2017-09-09  NaN       2.29    1.49    4.19    2.99    1.31    7.99    0.003750\n",
       "2017-09-10  NaN       2.44    1.49    4.34    3.38    1.31    7.99    0.003750\n",
       "2017-09-11  NaN       2.54    1.49    4.34    3.38    1.31    7.99    0.003750\n",
       "2017-09-12  NaN       2.34    1.49    4.35    3.38    1.31    7.99    0.003750\n",
       "2017-09-13  NaN       2.19    1.49    4.35    3.38    1.31    7.99    0.003750\n",
       "2017-09-14  NaN       2.17    1.49    4.36    3.38    1.31    7.99    0.003750\n",
       "2017-09-15  NaN       2.23    1.49    4.36    3.38    1.31    7.99    0.003750\n",
       "2017-09-16  NaN       2.14    1.49    4.36    3.38    1.31    7.99    0.003750\n",
       "2017-09-17  NaN       2.07    1.49    4.35    3.38    1.31    7.99    0.003750\n",
       "2017-09-18  NaN       2.05    1.49    4.35    3.38    1.31    7.99    0.003750\n",
       "2017-09-19  NaN       1.89    1.49    4.32    3.38    1.31    7.99    0.003750\n",
       "2017-09-20  NaN       1.96    1.49    4.35    3.38    1.31    7.99    0.003750\n",
       "2017-09-21  NaN       2.18    1.49    4.33    3.38    1.31    7.99    0.003750\n",
       "2017-09-22  NaN       2.39    1.49    4.34    3.07    1.31    7.99    0.003750\n",
       "2017-09-23  NaN       2.57    1.49    4.54    3.02    1.31    7.99    0.003750\n",
       "2017-09-24  NaN       2.32    1.49    4.60    3.02    1.31    7.99    0.003750\n",
       "2017-09-25  NaN       2.12    1.49    4.72    3.02    1.31    7.99    0.003750\n",
       "2017-09-26  NaN       1.84    1.49    4.72    3.02    1.31    7.99    0.003750\n",
       "2017-09-27  NaN       1.80    1.49    4.78    3.02    1.31    7.99    0.003750\n",
       "2017-09-28  NaN       1.51    1.49    4.75    3.02    1.31    7.99    0.003750\n",
       "2017-09-29  NaN       1.45    1.49    4.70    2.74    1.31    7.99    0.003750\n",
       "2017-09-30  NaN       1.34    1.49    4.69    2.57    1.31    7.99    0.003750\n",
       "2017-10-01  NaN       1.68    1.49    4.69    2.29    1.31    7.99    0.003750\n",
       "2017-10-02  NaN       2.13    1.49    4.66    2.29    1.31    7.99    0.003750\n",
       "2017-10-03  NaN       2.01    1.49    4.35    2.29    1.31    7.99    0.003750\n",
       "2017-10-04  NaN       1.89    1.49    4.30    2.29    1.31    7.99    0.003750\n",
       "2017-10-05  NaN       1.92    1.49    4.30    2.29    1.31    8.57    0.003750\n",
       "2017-10-06  NaN       1.74    1.49    4.30    2.23    1.31    8.72    0.003750\n",
       "2017-10-07  NaN       1.78    1.49    4.28    2.10    1.31    8.72    0.003750\n",
       "2017-10-08  NaN       1.88    1.49    4.30    2.58    1.31    8.72    0.003750\n",
       "2017-10-09  NaN       1.77    1.49    4.32    2.47    1.31    8.72    0.003750\n",
       "2017-10-10  NaN       1.71    1.49    4.28    2.19    1.31    8.72    0.003750\n",
       "2017-10-11  NaN       1.58    1.49    4.28    2.19    1.31    8.72    0.003750\n",
       "2017-10-12  NaN       1.39    1.49    4.28    2.19    1.31    8.72    0.003750\n",
       "2017-10-13  NaN       1.54    1.49    4.28    2.19    1.31    8.72    0.003750\n",
       "2017-10-14  NaN       1.54    1.49    4.28    2.19    1.31    8.72    0.003750\n",
       "2017-10-15  NaN       1.59    1.49    4.28    2.19    1.31    8.72    0.003750\n",
       "2017-10-16  NaN       1.67    1.49    4.28    2.19    1.31    8.72    0.003750\n",
       "2017-10-17  NaN       1.69    1.49    4.28    1.91    1.31    8.72    0.003750\n",
       "2017-10-18  NaN       1.57    1.49    4.28    1.90    1.31    8.72    0.003750\n",
       "2017-10-19  NaN       1.44    1.49    4.28    1.90    1.31    8.72    0.003750\n",
       "2017-10-20  NaN       1.55    1.49    4.28    1.90    1.31    8.72    0.003750\n",
       "2017-10-21  NaN       1.60    1.49    4.28    1.90    1.31    8.72    0.003750\n",
       "2017-10-22  NaN       1.45    1.49    4.32    1.90    1.31    8.72    0.003750\n",
       "2017-10-23  NaN       1.64    1.49    4.32    1.90    1.31    8.72    0.003750\n",
       "2017-10-24  NaN       1.71    1.49    4.32    1.90    1.31    8.72    0.003750\n",
       "2017-10-25  NaN       2.01    1.49    4.32    2.15    1.31    8.72    0.003750\n",
       "2017-10-26  NaN       2.01    1.49    4.32    2.19    1.31    8.72    0.003750\n",
       "2017-10-27  NaN       1.81    1.49    4.35    2.19    1.31    8.72    0.003750\n",
       "2017-10-28  NaN       1.92    1.49    4.53    2.19    1.31    8.72    0.003750\n",
       "2017-10-29  NaN       2.17    1.49    4.42    2.19    1.31    8.72    0.003750\n",
       "2017-10-30  NaN       2.36    1.49    4.43    2.19    1.31    8.72    0.003750\n",
       "2017-10-31  NaN       2.33    1.49    4.43    2.19    1.31    8.72    0.003750\n",
       "2017-11-01  NaN       2.47    1.49    4.43    2.29    1.31    8.72    0.003750\n",
       "2017-11-02  NaN       2.46    1.49    4.46    2.48    1.31    8.72    0.003750\n",
       "2017-11-03  NaN       2.68    1.49    4.47    2.48    1.31    8.72    0.003750\n",
       "2017-11-04  NaN       2.57    1.49    4.47    2.48    1.31    8.72    0.003750\n",
       "2017-11-05  NaN       2.72    1.49    4.46    2.48    1.31    8.72    0.003750\n",
       "2017-11-06  NaN       2.80    1.49    4.44    2.48    1.31    8.72    0.003750\n",
       "2017-11-07  NaN       3.00    1.49    4.47    2.48    1.31    8.72    0.003750\n",
       "2017-11-08  NaN       3.31    1.49    4.48    2.48    1.31    8.72    0.003750\n",
       "2017-11-09  NaN       3.18    1.49    4.48    2.48    1.31    8.72    0.003750\n",
       "2017-11-10  NaN       3.15    1.49    4.48    2.94    1.31    8.72    0.003750\n",
       "2017-11-11  NaN       3.12    1.49    4.48    3.06    1.31    8.72    0.003750\n",
       "2017-11-12  NaN       2.57    1.49    4.49    3.06    1.31    8.72    0.003750\n",
       "2017-11-13  NaN       2.48    1.49    4.49    3.06    1.31    8.72    0.003750\n",
       "2017-11-14  NaN       2.27    1.49    4.48    2.82    1.31    8.72    0.003750\n",
       "2017-11-15  NaN       2.09    1.49    4.47    2.77    1.31    8.72    0.003750\n",
       "2017-11-16  NaN       1.78    1.49    4.47    2.63    1.31    8.72    0.003750\n",
       "2017-11-17  NaN       1.45    1.49    4.47    2.48    1.31    8.72    0.003750\n",
       "2017-11-18  NaN       1.33    1.63    4.50    2.48    1.31    8.72    0.003750\n",
       "2017-11-19  NaN       1.32    1.29    4.50    2.48    1.28    8.72    0.003750\n",
       "2017-11-20  NaN       1.34    1.27    4.50    2.43    1.31    8.72    0.003750\n",
       "2017-11-21  NaN       1.31    1.21    4.50    2.19    1.16    8.72    0.003750\n",
       "2017-11-22  NaN       1.35    1.22    4.50    2.19    1.02    8.72    0.003750\n",
       "2017-11-23  NaN       1.35    1.30    4.50    2.19    1.08    8.72    0.003750\n",
       "2017-11-24  NaN       1.57    1.35    4.48    2.19    0.99    8.72    0.003750\n",
       "2017-11-25  NaN       1.67    1.33    4.49    2.19    0.96    8.72    0.003750\n",
       "2017-11-26  NaN       1.54    1.48    4.51    2.15    1.00    8.72    0.003750\n",
       "2017-11-27  NaN       1.66    1.58    4.50    1.90    1.10    8.72    0.003750\n",
       "2017-11-28  NaN       1.86    1.56    4.50    1.90    1.12    8.72    0.003750\n",
       "2017-11-29  NaN       1.95    1.53    4.46    1.90    1.21    8.72    0.003750\n",
       "2017-11-30  NaN       2.04    1.54    4.46    1.85    1.11    8.72    0.003750\n",
       "2017-12-01  NaN       2.13    1.60    4.46    1.61    1.06    8.72    0.003750\n",
       "2017-12-02  NaN       1.91    1.57    4.45    1.54    1.25    8.72    0.003750\n",
       "2017-12-03  NaN       1.64    1.41    4.45    1.54    1.25    8.72    0.003750\n",
       "2017-12-04  NaN       1.52    1.49    4.45    1.54    1.27    8.72    0.003750\n",
       "2017-12-05  NaN       1.49    1.61    4.44    1.54    1.66    8.72    0.003750\n",
       "2017-12-06  NaN       1.67    1.74    4.44    1.54    1.63    8.72    0.003750\n",
       "2017-12-07  NaN       2.06    1.92    4.66    1.54    1.47    8.72    0.003750\n",
       "2017-12-08  NaN       1.90    1.97    4.66    1.54    1.47    8.72    0.003750\n",
       "2017-12-09  NaN       2.03    1.90    4.66    1.54    1.47    8.72    0.003750\n",
       "2017-12-10  NaN       2.06    1.83    4.66    1.54    1.47    8.72    0.003750\n",
       "2017-12-11  NaN       2.08    1.87    4.70    1.54    1.47    8.72    0.003750\n",
       "2017-12-12  NaN       2.08    1.92    4.90    1.54    1.51    8.72    0.003750\n",
       "2017-12-13  NaN       2.09    1.97    4.90    1.54    1.69    8.72    0.003750\n",
       "2017-12-14  NaN       1.86    1.93    4.90    1.58    1.51    8.72    0.003750\n",
       "2017-12-15  NaN       1.92    2.08    4.87    1.87    1.56    8.72    0.003750\n",
       "2017-12-16  NaN       2.00    2.08    4.85    1.87    1.74    8.72    0.003750\n",
       "2017-12-17  NaN       2.07    2.08    4.83    1.87    1.69    8.72    0.003750\n",
       "2017-12-18  NaN       1.97    2.08    4.83    1.82    1.69    8.72    0.003750\n",
       "2017-12-19  NaN       2.18    2.08    4.83    1.65    1.69    8.72    0.003750\n",
       "2017-12-20  NaN       2.00    2.12    4.83    1.66    1.69    8.72    0.003750\n",
       "2017-12-21  NaN       1.85    2.07    4.67    1.91    1.67    8.72    0.003750\n",
       "2017-12-22  NaN       1.90    1.99    4.61    1.91    1.47    8.72    0.003750\n",
       "2017-12-23  NaN       1.88    1.99    4.62    1.91    1.47    8.72    0.003750\n",
       "2017-12-24  NaN       1.67    1.99    4.65    1.91    1.47    8.72    0.003750\n",
       "2017-12-25  NaN       1.81    1.91    4.65    1.91    1.47    8.72    0.003750\n",
       "2017-12-26  NaN       1.79    1.90    4.65    1.72    1.47    8.72    0.003750\n",
       "2017-12-27  NaN       1.67    1.90    4.65    1.34    1.56    8.72    0.003750\n",
       "2017-12-28  NaN       1.65    1.90    4.65    1.25    1.69    8.72    0.003750\n",
       "2017-12-29  NaN       1.63    1.90    4.66    1.12    1.69    8.72    0.003750\n",
       "2017-12-30  NaN       1.48    1.84    4.66    1.04    1.69    8.72    0.003750\n",
       "2017-12-31  NaN       1.26    1.63    4.66    0.97    1.69    8.72    0.003750\n",
       "2018-01-01  NaN       1.36    1.63    4.66    1.11    1.69    8.72    0.003750\n",
       "2018-01-02  NaN       1.58    1.73    4.66    1.08    1.69    8.72    0.003750\n",
       "2018-01-03  NaN       1.54    1.82    4.66    1.04    1.69    8.72    0.003750\n",
       "2018-01-04  NaN       1.58    1.76    4.66    1.24    1.69    8.72    0.003750\n",
       "2018-01-05  NaN       1.61    1.77    4.66    1.45    1.69    8.72    0.003750\n",
       "2018-01-06  NaN       1.83    2.00    4.66    1.45    1.69    8.72    0.003750\n",
       "2018-01-07  NaN       1.83    1.91    4.67    1.45    1.71    8.72    0.003750\n",
       "2018-01-08  NaN       2.13    2.01    4.69    1.51    1.90    8.72    0.003750\n",
       "2018-01-09  NaN       2.10    2.22    4.69    1.93    1.89    8.72    0.003750\n",
       "2018-01-10  NaN       2.11    2.32    4.68    1.93    1.89    8.72    0.003750\n",
       "2018-01-11  NaN       2.20    2.53    4.53    1.93    2.06    8.72    0.003750\n",
       "2018-01-12  NaN       2.27    2.52    4.45    1.97    2.18    8.72    0.003750\n",
       "2018-01-13  NaN       2.34    2.54    4.32    2.24    2.18    8.72    0.003750\n",
       "2018-01-14  NaN       2.78    2.61    4.32    2.01    2.18    8.72    0.003750\n",
       "2018-01-15  NaN       2.80    2.78    4.33    2.11    2.18    8.72    0.003750\n",
       "2018-01-16  NaN       2.69    2.60    4.29    2.24    2.18    8.72    0.003750\n",
       "2018-01-17  NaN       2.50    2.45    4.14    2.17    2.18    8.72    0.003750\n",
       "2018-01-18  NaN       2.35    2.65    4.14    2.23    2.18    8.72    0.003750\n",
       "2018-01-19  NaN       2.16    2.86    4.07    2.24    2.18    8.72    0.003750\n",
       "2018-01-20  NaN       2.37    2.89    3.91    2.24    2.64    8.72    0.003750\n",
       "2018-01-21  NaN       2.43    2.76    3.90    2.24    2.76    8.72    0.003750\n",
       "2018-01-22  NaN       2.30    2.48    3.83    2.24    2.76    8.72    0.003750\n",
       "2018-01-23  NaN       2.26    2.53    3.50    2.24    2.76    8.72    0.003750\n",
       "2018-01-24  NaN       1.93    2.63    3.37    2.24    2.76    8.72    0.003750\n",
       "2018-01-25  NaN       2.04    2.65    3.18    2.24    2.76    8.72    0.003750\n",
       "2018-01-26  NaN       2.17    2.65    3.21    2.24    2.76    8.72    0.003750\n",
       "2018-01-27  NaN       2.17    2.62    3.21    2.24    2.76    8.72    0.003750\n",
       "2018-01-28  NaN       2.20    2.51    3.17    2.24    2.76    8.72    0.003750\n",
       "2018-01-29  NaN       2.07    2.48    3.03    2.24    2.76    8.72    0.003750\n",
       "2018-01-30  NaN       2.07    2.48    3.03    2.24    2.76    8.72    0.003750\n",
       "2018-01-31  NaN       2.02    2.45    3.02    2.24    2.76    8.72    0.003750\n",
       "2018-02-01  NaN       2.23    2.47    2.96    2.22    2.76    8.72    0.003750\n",
       "2018-02-02  NaN       2.14    2.38    2.96    1.95    2.76    8.72    0.003750\n",
       "2018-02-03  NaN       2.17    2.30    2.96    1.95    2.76    8.72    0.003750\n",
       "2018-02-04  NaN       2.34    2.24    2.94    1.95    2.76    8.72    0.003750\n",
       "2018-02-05  NaN       2.29    2.69    2.92    1.99    2.93    8.72    0.003750\n",
       "2018-02-06  NaN       2.22    2.51    2.72    2.24    3.05    8.72    0.003750\n",
       "2018-02-07  NaN       1.97    2.30    2.72    2.10    3.05    8.72    0.003750\n",
       "2018-02-08  NaN       1.97    2.27    2.72    1.95    3.05    8.72    0.003750\n",
       "2018-02-09  NaN       2.02    2.26    2.72    1.99    3.05    8.72    0.003750\n",
       "2018-02-10  NaN       2.01    2.26    2.72    2.63    3.05    8.72    0.003750\n",
       "2018-02-11  NaN       2.24    2.34    2.72    2.82    3.05    8.72    0.003750\n",
       "2018-02-12  NaN       2.28    2.73    2.73    2.76    3.05    8.72    0.003750\n",
       "2018-02-13  NaN       2.54    2.85    2.76    2.53    2.87    8.72    0.003750\n",
       "2018-02-14  NaN       2.63    2.69    2.76    2.53    3.36    9.38    0.003750\n",
       "2018-02-15  NaN       2.68    2.91    2.76    2.40    3.41    9.44    0.003750\n",
       "2018-02-16  NaN       2.71    2.95    2.97    2.24    3.41    9.44    0.003750\n",
       "2018-02-17  NaN       2.78    2.82    3.30    2.51    3.41    9.44    0.003750\n",
       "2018-02-18  NaN       2.82    2.86    3.37    2.72    3.41    9.44    0.003750\n",
       "2018-02-19  NaN       2.87    2.73    3.41    3.11    3.41    9.44    0.003750\n",
       "2018-02-20  NaN       2.48    2.55    3.40    3.11    3.05    9.44    0.003750\n",
       "2018-02-21  NaN       2.44    2.51    3.39    3.11    3.02    9.44    0.003750\n",
       "2018-02-22  NaN       2.27    2.49    3.39    2.89    3.02    9.44    0.003750\n",
       "2018-02-23  NaN       2.09    2.48    3.39    2.97    3.02    9.44    0.003750\n",
       "2018-02-24  NaN       2.06    2.41    3.39    3.53    3.02    9.44    0.003750\n",
       "2018-02-25  NaN       2.01    2.38    3.39    3.53    3.02    9.44    0.003750\n",
       "2018-02-26  NaN       2.11    2.33    3.38    3.53    3.02    9.44    0.003750\n",
       "2018-02-27  NaN       2.00    2.23    3.38    3.16    3.02    10.11   0.003750\n",
       "2018-02-28  NaN       1.92    2.25    3.38    2.86    3.02    10.44   0.003750\n",
       "2018-03-01  NaN       1.66    2.23    3.20    2.85    3.02    10.44   0.003750\n",
       "2018-03-02  NaN       1.82    2.21    3.12    2.85    3.02    10.44   0.003750\n",
       "2018-03-03  NaN       1.96    2.22    3.01    2.81    3.02    10.44   0.003750\n",
       "2018-03-04  NaN       1.88    2.26    2.99    2.56    3.02    10.44   0.003750\n",
       "2018-03-05  NaN       1.79    2.36    2.97    2.56    3.02    10.44   0.003750\n",
       "2018-03-06  NaN       1.97    2.34    2.97    2.56    3.02    10.44   0.003750\n",
       "2018-03-07  NaN       2.05    2.33    2.97    2.56    3.02    10.44   0.003750\n",
       "2018-03-08  NaN       2.04    2.32    2.97    2.56    3.02    10.44   0.003750\n",
       "2018-03-09  NaN       2.11    2.24    2.97    2.48    3.02    10.44   0.003750\n",
       "2018-03-10  NaN       1.92    2.23    2.99    2.27    3.02    10.44   0.003750\n",
       "2018-03-11  NaN       1.75    2.24    3.00    2.27    3.02    10.44   0.003750\n",
       "2018-03-12  NaN       1.83    2.26    2.99    1.87    3.02    10.44   0.003750\n",
       "2018-03-13  NaN       1.69    2.31    2.97    1.76    3.02    10.44   0.003750\n",
       "2018-03-14  NaN       1.78    2.26    2.86    1.76    3.02    10.44   0.003750\n",
       "2018-03-15  NaN       1.79    2.27    2.86    1.76    3.02    10.44   0.003750\n",
       "2018-03-16  NaN       1.83    2.31    2.86    1.76    3.02    10.19   0.003750\n",
       "2018-03-17  NaN       1.95    2.22    2.88    1.86    3.02    9.94    0.003750\n",
       "2018-03-18  NaN       2.15    2.33    2.89    2.05    3.02    9.94    0.003750\n",
       "2018-03-19  NaN       2.16    2.42    2.92    2.05    3.02    9.94    0.003750\n",
       "2018-03-20  NaN       2.24    2.40    2.90    2.26    3.02    9.94    0.003750\n",
       "2018-03-21  NaN       2.34    2.56    2.91    2.63    3.02    9.94    0.003750\n",
       "2018-03-22  NaN       2.43    2.66    2.89    2.63    2.80    9.94    0.003750\n",
       "2018-03-23  NaN       2.53    2.67    2.92    2.63    2.73    9.94    0.003750\n",
       "2018-03-24  NaN       2.46    2.66    2.94    2.63    2.73    9.94    0.003750\n",
       "2018-03-25  NaN       2.69    2.71    2.95    2.63    2.73    9.94    0.003750\n",
       "2018-03-26  NaN       2.67    2.72    2.95    2.63    2.73    9.94    0.003750\n",
       "2018-03-27  NaN       2.66    2.92    2.92    2.63    2.73    9.94    0.003750\n",
       "2018-03-28  NaN       2.78    2.89    2.91    2.63    2.73    9.94    0.003750\n",
       "2018-03-29  NaN       2.83    2.90    2.97    2.71    2.73    9.94    0.003750\n",
       "2018-03-30  NaN       2.71    2.83    3.11    2.95    2.73    9.94    0.003750\n",
       "2018-03-31  NaN       2.75    2.89    3.16    3.37    2.73    9.94    0.003750\n",
       "2018-04-01  NaN       2.73    3.08    3.27    3.13    2.73    9.94    0.003750\n",
       "2018-04-02  NaN       2.63    3.23    3.29    2.95    2.73    9.94    0.003750\n",
       "2018-04-03  NaN       2.47    3.49    3.33    2.95    2.73    9.94    0.003750\n",
       "2018-04-04  NaN       2.38    3.48    3.37    2.95    2.73    9.94    0.003750\n",
       "2018-04-05  NaN       2.16    3.71    3.67    2.95    2.73    9.94    0.003750\n",
       "2018-04-06  NaN       2.06    3.60    3.73    2.78    2.73    9.94    0.003750\n",
       "2018-04-07  NaN       2.31    3.47    3.84    2.66    2.73    9.94    0.003750\n",
       "2018-04-08  NaN       2.22    3.54    3.84    2.66    2.73    9.94    0.003750\n",
       "2018-04-09  NaN       2.67    3.68    3.87    2.66    2.80    9.94    0.003750\n",
       "2018-04-10  NaN       2.69    3.78    3.98    2.95    3.09    9.94    0.003750\n",
       "2018-04-11  NaN       2.68    3.95    3.99    2.98    3.09    9.94    0.003750\n",
       "2018-04-12  NaN       2.54    3.88    4.02    2.98    3.09    9.94    0.003750\n",
       "2018-04-13  NaN       2.37    3.85    4.05    2.98    3.09    9.94    0.003750\n",
       "2018-04-14  NaN       2.21    3.72    4.04    3.39    3.09    9.94    0.003750\n",
       "2018-04-15  NaN       2.23    3.48    4.07    3.73    3.09    9.94    0.003750\n",
       "2018-04-16  NaN       2.38    3.46    4.19    3.73    3.09    9.94    0.003750\n",
       "2018-04-17  NaN       2.38    3.44    4.19    3.73    3.09    9.94    0.003750\n",
       "2018-04-18  NaN       2.38    3.32    4.21    3.73    3.09    9.94    0.003750\n",
       "2018-04-19  NaN       2.30    3.26    4.25    3.73    3.09    9.94    0.003750\n",
       "2018-04-20  NaN       2.24    3.33    4.22    3.73    3.09    9.94    0.003750\n",
       "2018-04-21  NaN       2.11    3.34    4.22    3.73    3.09    9.94    0.003750\n",
       "2018-04-22  NaN       2.05    3.37    4.22    3.94    3.09    9.94    0.003750\n",
       "2018-04-23  NaN       1.81    3.41    4.25    4.12    3.09    9.94    0.003750\n",
       "2018-04-24  NaN       1.60    3.42    4.26    4.12    3.09    9.94    0.003750\n",
       "2018-04-25  NaN       1.94    3.49    4.27    4.12    3.09    9.94    0.003750\n",
       "2018-04-26  NaN       2.10    3.53    4.27    3.83    3.09    9.94    0.003750\n",
       "2018-04-27  NaN       1.95    3.53    4.27    3.73    3.09    9.94    0.003750\n",
       "2018-04-28  NaN       1.53    3.53    4.25    3.73    3.09    9.94    0.003750\n",
       "2018-04-29  NaN       1.46    3.50    4.24    3.73    3.09    9.94    0.003750\n",
       "2018-04-30  NaN       1.53    3.28    4.24    3.73    3.09    9.94    0.003750\n",
       "2018-05-01  NaN       1.93    3.24    4.24    3.73    3.09    9.94    0.003750\n",
       "2018-05-02  NaN       1.53    3.26    4.24    3.42    3.09    9.94    0.003750\n",
       "2018-05-03  NaN       1.29    3.44    4.24    3.09    3.09    9.94    0.003750\n",
       "2018-05-04  NaN       1.49    3.47    4.24    2.81    3.09    9.94    0.003750\n",
       "2018-05-05  NaN       1.44    3.40    4.24    2.69    3.09    9.94    0.003750\n",
       "2018-05-06  NaN       1.27    3.20    4.23    2.69    3.09    9.94    0.003750\n",
       "2018-05-07  NaN       1.53    3.01    4.24    2.69    3.09    9.94    0.003750\n",
       "2018-05-08  NaN       1.86    2.74    4.25    2.69    3.09    9.94    0.003750\n",
       "2018-05-09  NaN       1.80    2.74    4.25    2.69    3.09    9.94    0.003750\n",
       "2018-05-10  NaN       1.94    2.57    4.24    2.69    3.09    9.94    0.003750\n",
       "2018-05-11  NaN       2.02    2.37    3.99    2.69    3.09    9.94    0.003750\n",
       "2018-05-12  NaN       1.79    2.32    3.97    2.69    3.09    9.94    0.003750\n",
       "2018-05-13  NaN       1.84    2.29    3.97    2.69    3.09    9.94    0.003750\n",
       "2018-05-14  NaN       1.80    2.31    3.97    2.69    3.09    9.94    0.003750\n",
       "2018-05-15  NaN       1.74    2.34    3.99    2.69    3.09    9.94    0.003750\n",
       "2018-05-16  NaN       1.98    2.50    4.03    2.40    2.96    9.94    0.003750\n",
       "2018-05-17  NaN       2.07    2.47    4.03    2.40    2.80    9.94    0.003750\n",
       "2018-05-18  NaN       2.08    2.26    4.03    2.40    2.80    9.94    0.003750\n",
       "2018-05-19  NaN       1.98    2.28    4.03    2.08    2.80    9.94    0.003750\n",
       "2018-05-20  NaN       2.05    2.29    4.03    1.85    2.80    9.94    0.003750\n",
       "2018-05-21  NaN       1.97    2.31    4.03    1.85    2.80    9.94    0.003750\n",
       "2018-05-22  NaN       1.95    2.36    4.03    1.72    2.80    9.94    0.003750\n",
       "2018-05-23  NaN       1.90    2.56    4.04    1.69    2.80    9.94    0.003750\n",
       "2018-05-24  NaN       1.96    2.78    4.04    2.18    2.80    9.94    0.003750\n",
       "2018-05-25  NaN       1.84    2.81    4.04    2.18    2.80    9.94    0.003750\n",
       "2018-05-26  NaN       1.58    2.75    4.04    2.18    2.80    9.94    0.003750\n",
       "2018-05-27  NaN       1.57    2.61    4.01    2.18    2.80    9.94    0.003750\n",
       "2018-05-28  NaN       1.78    2.51    3.93    2.18    2.80    9.94    0.003750\n",
       "2018-05-29  NaN       1.93    2.53    3.92    2.18    2.80    9.94    0.003750\n",
       "2018-05-30  NaN       2.08    2.53    3.90    2.18    2.80    9.94    0.003750\n",
       "2018-05-31  NaN       2.13    2.57    3.87    2.18    2.80    9.94    0.003750\n",
       "2018-06-01  NaN       2.21    2.64    3.87    2.25    2.80    9.94    0.003750\n",
       "2018-06-02  NaN       2.22    2.64    3.87    2.47    2.80    9.94    0.003750\n",
       "2018-06-03  NaN       2.10    2.66    3.89    2.47    2.80    9.94    0.003750\n",
       "2018-06-04  NaN       1.88    2.71    3.87    2.47    2.80    9.94    0.003750\n",
       "2018-06-05  NaN       1.65    2.74    3.86    2.47    2.80    9.94    0.003750\n",
       "2018-06-06  NaN       1.41    2.64    3.86    2.45    2.80    9.94    0.003750\n",
       "2018-06-07  NaN       1.57    2.62    3.86    1.99    2.80    9.94    0.003750\n",
       "2018-06-08  NaN       1.60    2.63    3.86    1.99    2.80    9.94    0.003750\n",
       "2018-06-09  NaN       1.75    2.66    3.86    1.99    2.80    9.94    0.003750\n",
       "2018-06-10  NaN       1.78    2.70    3.86    2.05    2.80    9.94    0.003750\n",
       "2018-06-11  NaN       1.84    2.69    3.80    2.28    2.80    9.94    0.003750\n",
       "2018-06-12  NaN       1.67    2.69    3.67    2.14    2.80    9.94    0.003750\n",
       "2018-06-13  NaN       1.82    2.59    3.65    1.99    2.75    9.94    0.003750\n",
       "2018-06-14  NaN       1.98    2.50    3.65    1.99    2.51    9.94    0.003750\n",
       "2018-06-15  NaN       1.78    2.52    3.65    1.99    2.51    9.94    0.003750\n",
       "2018-06-16  NaN       1.76    2.55    3.65    1.99    2.51    9.94    0.003750\n",
       "2018-06-17  NaN       1.79    2.68    3.65    1.99    2.51    8.45    0.003750\n",
       "2018-06-18  NaN       1.83    2.69    3.65    1.99    2.51    3.99    0.003750\n",
       "2018-06-19  NaN       1.71    2.68    3.65    1.99    2.51    3.99    0.003750\n",
       "2018-06-20  NaN       1.73    2.68    3.65    2.06    2.51    3.99    0.003750\n",
       "2018-06-21  NaN       1.63    2.69    3.64    2.28    2.51    3.99    0.003750\n",
       "2018-06-22  NaN       1.65    2.71    3.64    2.28    2.51    3.99    0.003750\n",
       "2018-06-23  NaN       1.57    2.71    3.64    2.28    2.51    3.99    0.003750\n",
       "2018-06-24  NaN       1.37    2.71    3.64    2.28    2.51    3.99    0.003750\n",
       "2018-06-25  NaN       1.46    2.69    3.64    2.24    2.51    3.99    0.003750\n",
       "2018-06-26  NaN       1.18    2.69    3.64    1.39    2.51    3.99    0.003750\n",
       "2018-06-27  NaN       1.30    2.68    3.64    1.39    2.51    3.99    0.003750\n",
       "2018-06-28  NaN       1.33    2.68    3.64    1.39    2.51    3.99    0.003750\n",
       "2018-06-29  NaN       1.35    2.68    3.64    1.39    2.51    3.99    0.003750\n",
       "2018-06-30  NaN       1.38    2.78    3.64    1.39    2.51    3.99    0.003750\n",
       "2018-07-01  NaN       1.42    2.91    3.64    1.39    2.51    3.99    0.003750\n",
       "2018-07-02  NaN       1.43    2.94    3.64    1.33    2.51    3.99    0.003750\n",
       "2018-07-03  NaN       1.59    2.95    3.46    1.28    2.51    3.99    0.003750\n",
       "2018-07-04  NaN       1.70    2.96    3.45    1.42    2.51    3.99    0.003750\n",
       "2018-07-05  NaN       1.63    3.00    3.38    1.42    2.51    3.99    0.003750\n",
       "2018-07-06  NaN       1.76    2.99    3.30    1.38    2.51    3.99    0.003750\n",
       "2018-07-07  NaN       1.79    2.96    3.47    1.45    2.51    3.99    0.003750\n",
       "2018-07-08  NaN       1.80    2.79    3.49    1.59    2.51    3.99    0.003750\n",
       "2018-07-09  NaN       1.89    2.81    3.51    1.67    2.51    3.99    0.003750\n",
       "2018-07-10  NaN       1.52    2.77    3.51    2.09    2.51    3.99    0.003750\n",
       "2018-07-11  NaN       1.44    2.70    3.51    2.15    2.51    3.99    0.003750\n",
       "2018-07-12  NaN       1.48    2.67    3.51    2.15    2.51    3.99    0.003750\n",
       "2018-07-13  NaN       1.19    2.67    3.51    1.87    2.51    3.99    0.003750\n",
       "2018-07-14  NaN       0.97    2.67    3.51    1.67    2.51    3.99    0.003750\n",
       "2018-07-15  NaN       1.20    2.67    3.51    1.67    2.51    3.99    0.003750\n",
       "2018-07-16  NaN       1.23    2.67    3.51    1.45    2.51    3.99    0.003750\n",
       "2018-07-17  NaN       1.22    2.71    3.51    1.45    2.51    3.99    0.003750\n",
       "2018-07-18  NaN       1.16    2.83    3.51    1.45    2.51    3.99    0.003750\n",
       "2018-07-19  NaN       1.06    2.82    3.51    1.45    2.51    3.99    0.003750\n",
       "2018-07-20  NaN       1.05    2.84    3.56    1.45    2.51    3.99    0.003750\n",
       "2018-07-21  NaN       1.19    2.84    3.69    1.50    2.51    3.99    0.003750\n",
       "2018-07-22  NaN       1.53    2.87    3.66    1.89    2.51    3.99    0.003750\n",
       "2018-07-23  NaN       1.68    2.90    3.56    1.89    2.51    3.99    0.003750\n",
       "2018-07-24  NaN       1.87    2.91    3.68    1.89    2.51    3.99    0.003750\n",
       "2018-07-25  NaN       1.71    2.86    3.68    1.89    2.51    3.99    0.003750\n",
       "2018-07-26  NaN       1.84    2.90    3.69    1.69    2.51    3.99    0.003750\n",
       "2018-07-27  NaN       1.75    2.89    3.69    1.67    2.51    3.63    0.003750\n",
       "2018-07-28  NaN       1.75    2.89    3.65    1.83    2.51    3.60    0.003750\n",
       "2018-07-29  NaN       1.65    2.80    3.65    1.93    2.51    3.60    0.003750\n",
       "2018-07-30  NaN       1.66    2.81    3.69    1.71    2.51    3.60    0.003750\n",
       "2018-07-31  NaN       1.57    2.78    3.75    1.71    2.51    3.60    0.003750\n",
       "2018-08-01  NaN       1.40    2.69    3.64    1.71    2.51    3.60    0.003750\n",
       "2018-08-02  NaN       1.26    2.72    3.73    1.71    2.51    3.60    0.003750\n",
       "2018-08-03  NaN       1.14    2.66    3.67    1.71    2.51    3.60    0.003750\n",
       "2018-08-04  NaN       1.30    2.67    3.67    1.71    2.51    3.60    0.003750\n",
       "2018-08-05  NaN       1.39    2.68    3.67    1.71    2.51    3.60    0.003750\n",
       "2018-08-06  NaN       1.30    2.59    3.67    1.71    2.51    3.60    0.003750\n",
       "2018-08-07  NaN       1.41    2.55    3.67    1.71    2.51    3.60    0.003750\n",
       "2018-08-08  NaN       1.44    2.56    3.65    1.71    2.51    3.60    0.003750\n",
       "2018-08-09  NaN       1.38    2.83    3.65    1.51    2.51    3.60    0.003750\n",
       "2018-08-10  NaN       1.41    2.75    3.66    1.53    2.51    3.60    0.003750\n",
       "2018-08-11  NaN       1.57    2.80    3.68    1.69    2.51    3.60    0.003750\n",
       "2018-08-12  NaN       1.48    2.90    3.66    1.69    2.51    3.60    0.003750\n",
       "2018-08-13  NaN       1.50    2.91    3.66    1.69    2.51    3.60    0.003750\n",
       "2018-08-14  NaN       1.61    3.05    3.66    1.69    2.51    3.60    0.003750\n",
       "2018-08-15  NaN       1.51    3.06    3.82    1.69    2.51    3.60    0.003750\n",
       "2018-08-16  NaN       1.55    2.90    3.85    1.69    2.51    3.60    0.003750\n",
       "2018-08-17  NaN       1.42    2.77    3.85    1.69    2.51    3.60    0.003750\n",
       "2018-08-18  NaN       1.41    2.75    3.84    1.69    2.51    3.60    0.003750\n",
       "2018-08-19  NaN       1.51    2.75    3.67    1.69    2.51    3.60    0.003750\n",
       "2018-08-20  NaN       1.58    2.73    3.67    1.69    2.51    3.60    0.003750\n",
       "2018-08-21  NaN       1.36    2.73    3.62    1.69    2.51    3.60    0.003750\n",
       "2018-08-22  NaN       1.43    2.75    3.63    1.69    2.51    3.60    0.003750\n",
       "2018-08-23  NaN       1.56    2.75    3.63    1.69    2.51    3.73    0.003750\n",
       "2018-08-24  NaN       1.55    2.75    3.60    1.69    2.51    4.38    0.003750\n",
       "2018-08-25  NaN       1.68    2.78    3.44    1.69    2.51    4.38    0.003750\n",
       "2018-08-26  NaN       1.62    2.79    3.41    1.69    2.51    4.38    0.003750\n",
       "2018-08-27  NaN       1.50    2.80    3.41    1.69    2.51    4.38    0.003750\n",
       "2018-08-28  NaN       1.37    2.80    3.39    1.69    2.51    4.38    0.003750\n",
       "2018-08-29  NaN       1.24    2.84    3.38    1.69    2.51    4.38    0.003750\n",
       "2018-08-30  NaN       1.24    2.85    3.38    1.69    2.51    4.38    0.003750\n",
       "2018-08-31  NaN       1.36    2.85    3.37    1.69    2.51    4.38    0.003750\n",
       "2018-09-01  NaN       1.47    2.88    3.37    1.69    2.51    4.38    0.003750\n",
       "2018-09-02  NaN       1.53    2.90    3.38    1.69    2.51    4.38    0.003750\n",
       "2018-09-03  NaN       1.75    2.93    3.40    1.69    2.51    4.38    0.003750\n",
       "2018-09-04  NaN       1.91    3.09    3.40    1.69    2.69    4.38    0.003750\n",
       "2018-09-05  NaN       2.01    3.11    3.40    1.69    2.80    4.38    0.003750\n",
       "2018-09-06  NaN       2.04    3.13    3.40    1.69    2.80    4.38    0.003750\n",
       "2018-09-07  NaN       1.95    3.12    3.40    1.88    2.80    4.38    0.003750\n",
       "2018-09-08  NaN       1.97    3.12    3.43    1.95    2.80    4.38    0.003750\n",
       "2018-09-09  NaN       2.23    3.09    3.37    1.95    2.80    4.38    0.003750\n",
       "2018-09-10  NaN       2.51    3.09    3.29    2.18    2.80    4.38    0.003750\n",
       "2018-09-11  NaN       2.36    3.11    3.30    2.24    2.80    4.38    0.003750\n",
       "2018-09-12  NaN       2.19    3.29    3.33    2.24    2.80    4.38    0.003750\n",
       "2018-09-13  NaN       1.81    3.29    3.33    2.12    2.80    4.38    0.003750\n",
       "2018-09-14  NaN       1.72    3.32    3.34    1.95    2.80    4.38    0.003750\n",
       "2018-09-15  NaN       1.80    3.33    3.34    1.95    2.80    4.38    0.003750\n",
       "2018-09-16  NaN       1.83    3.33    3.34    1.95    2.80    4.38    0.003750\n",
       "2018-09-17  NaN       1.87    3.35    3.34    1.95    2.80    4.38    0.003750\n",
       "2018-09-18  NaN       1.88    3.35    3.34    1.95    2.80    4.38    0.003750\n",
       "2018-09-19  NaN       1.80    3.32    3.34    1.95    2.80    4.38    0.003750\n",
       "2018-09-20  NaN       1.70    3.32    3.34    1.95    2.80    4.38    0.003750\n",
       "2018-09-21  NaN       1.62    3.32    3.34    1.95    2.80    4.38    0.003750\n",
       "2018-09-22  NaN       1.65    3.43    3.36    1.82    2.80    4.38    0.003750\n",
       "2018-09-23  NaN       1.52    3.51    3.55    1.73    2.80    4.38    0.003750\n",
       "2018-09-24  NaN       1.41    3.62    3.55    1.73    2.80    4.38    0.003750\n",
       "2018-09-25  NaN       1.41    3.58    3.53    1.73    2.93    4.38    0.003750\n",
       "2018-09-26  NaN       1.39    3.32    3.52    1.73    3.16    4.38    0.003750\n",
       "2018-09-27  NaN       1.15    3.31    3.49    1.73    3.16    4.38    0.003750\n",
       "2018-09-28  NaN       1.04    3.31    3.48    1.73    3.16    4.38    0.003750\n",
       "2018-09-29  NaN       0.82    3.30    3.48    1.73    3.16    4.38    0.003750\n",
       "2018-09-30  NaN       1.09    3.14    3.48    1.73    3.16    4.38    0.003750\n",
       "2018-10-01  NaN       1.19    3.14    3.48    1.64    3.16    4.38    0.003750\n",
       "2018-10-02  NaN       1.08    3.14    3.50    1.48    3.16    4.38    0.003750\n",
       "2018-10-03  NaN       1.15    3.14    3.50    1.39    3.16    4.38    0.003750\n",
       "2018-10-04  NaN       1.14    3.01    3.50    1.39    3.16    4.38    0.003750\n",
       "2018-10-05  NaN       1.06    3.01    3.38    1.39    3.16    4.38    0.003750\n",
       "2018-10-06  NaN       0.97    2.89    3.35    1.39    3.16    4.38    0.003750\n",
       "2018-10-07  NaN       0.89    2.85    3.35    1.39    3.16    4.38    0.003750\n",
       "2018-10-08  NaN       0.95    2.85    3.34    1.39    3.16    4.38    0.003750\n",
       "2018-10-09  NaN       0.78    2.78    3.34    1.39    3.16    4.38    0.003750\n",
       "2018-10-10  NaN       0.77    2.76    3.34    1.39    3.16    4.38    0.003750\n",
       "2018-10-11  NaN       0.77    2.69    3.34    1.39    3.16    4.38    0.003750\n",
       "2018-10-12  NaN       0.75    2.66    3.32    1.39    3.16    4.38    0.003750\n",
       "2018-10-13  NaN       0.75    2.71    3.31    1.39    3.16    4.38    0.003750\n",
       "2018-10-14  NaN       0.83    2.71    3.31    1.39    3.16    4.38    0.003750\n",
       "2018-10-15  NaN       0.81    2.77    3.31    1.39    3.16    4.38    0.003750\n",
       "2018-10-16  NaN       0.83    2.75    3.31    1.39    3.16    4.38    0.003750\n",
       "2018-10-17  NaN       0.80    2.53    3.31    1.39    3.16    4.38    0.003750\n",
       "2018-10-18  NaN       0.58    2.33    3.32    1.39    3.16    4.38    0.003750\n",
       "2018-10-19  NaN       0.63    2.39    3.35    1.39    3.16    4.38    0.003750\n",
       "2018-10-20  NaN       0.72    2.26    3.35    1.39    3.16    4.38    0.003750\n",
       "2018-10-21  NaN       0.72    1.96    3.35    1.39    3.16    4.38    0.003750\n",
       "2018-10-22  NaN       0.83    1.94    3.31    1.39    3.16    4.38    0.003750\n",
       "2018-10-23  NaN       0.86    1.85    3.31    1.39    2.75    4.38    0.003750\n",
       "2018-10-24  NaN       0.85    1.54    3.30    1.39    2.58    4.38    0.003750\n",
       "2018-10-25  NaN       0.78    1.54    3.11    1.39    2.58    4.38    0.003750\n",
       "2018-10-26  NaN       0.76    1.51    3.11    1.39    2.58    4.38    0.003750\n",
       "2018-10-27  NaN       0.72    1.47    3.10    1.39    2.58    4.38    0.003750\n",
       "2018-10-28  NaN       0.67    1.40    2.93    1.39    2.58    4.38    0.003750\n",
       "2018-10-29  NaN       0.77    1.27    2.89    1.39    2.58    4.38    0.003750\n",
       "2018-10-30  NaN       0.80    1.22    2.80    1.39    2.58    4.38    0.003750\n",
       "2018-10-31  NaN       0.61    1.03    2.80    1.39    2.58    4.38    0.003750\n",
       "2018-11-01  NaN       0.70    0.85    2.77    1.39    2.58    4.38    0.003750\n",
       "2018-11-02  NaN       0.76    0.96    2.72    1.39    2.58    4.38    0.003750\n",
       "2018-11-03  NaN       0.68    0.93    2.72    1.39    2.58    4.38    0.003750\n",
       "2018-11-04  NaN       0.69    0.95    2.72    1.39    2.58    4.38    0.003750\n",
       "2018-11-05  NaN       0.70    0.98    2.72    1.39    2.58    4.38    0.003750\n",
       "2018-11-06  NaN       0.67    0.93    2.72    1.39    2.58    4.38    0.003750\n",
       "2018-11-07  NaN       0.67    0.93    2.89    1.39    2.58    4.38    0.003750\n",
       "2018-11-08  NaN       0.61    1.00    2.86    1.31    2.58    4.38    0.003750\n",
       "2018-11-09  NaN       0.61    0.94    2.77    1.20    2.58    4.38    0.003750\n",
       "2018-11-10  NaN       0.60    0.86    2.80    1.20    2.58    4.38    0.003750\n",
       "2018-11-11  NaN       0.55    0.84    2.80    1.20    2.58    4.38    0.003750\n",
       "2018-11-12  NaN       0.35    0.88    2.80    1.20    2.58    4.38    0.003750\n",
       "2018-11-13  NaN       0.44    0.93    2.78    1.20    2.58    4.38    0.003750\n",
       "2018-11-14  NaN       0.44    0.95    2.76    1.20    2.58    4.38    0.003750\n",
       "2018-11-15  NaN       0.51    0.97    2.69    1.19    2.58    4.38    0.003750\n",
       "2018-11-16  NaN       0.42    0.97    2.62    1.01    2.58    4.38    0.003750\n",
       "2018-11-17  NaN       0.44    0.99    2.62    1.01    2.58    4.38    0.003750\n",
       "2018-11-18  NaN       0.43    1.03    2.62    1.01    2.58    4.38    0.003750\n",
       "2018-11-19  NaN       0.38    1.10    2.62    1.01    2.58    4.38    0.003750\n",
       "2018-11-20  NaN       0.48    1.10    2.62    1.01    2.58    4.38    0.003750\n",
       "2018-11-21  NaN       0.60    1.10    2.62    1.01    2.58    4.38    0.003750\n",
       "2018-11-22  NaN       0.53    1.10    2.64    1.01    2.58    4.38    0.003750\n",
       "2018-11-23  NaN       0.51    1.12    2.66    1.01    2.58    4.38    0.003750\n",
       "2018-11-24  NaN       0.38    1.14    2.64    1.01    2.58    4.38    0.003750\n",
       "2018-11-25  NaN       0.47    1.15    2.52    1.01    2.58    4.38    0.003750\n",
       "2018-11-26  NaN       0.54    1.15    2.52    1.01    2.58    4.38    0.003750\n",
       "2018-11-27  NaN       0.54    1.15    2.52    1.01    2.58    4.38    0.003750\n",
       "2018-11-28  NaN       0.56    1.17    2.52    1.01    2.58    4.38    0.003750\n",
       "2018-11-29  NaN       0.58    1.17    2.52    1.01    2.58    4.38    0.003750\n",
       "2018-11-30  NaN       0.66    1.17    2.52    1.01    2.58    4.38    0.003750\n",
       "2018-12-01  NaN       0.76    1.18    2.51    1.01    2.58    4.38    0.003750\n",
       "2018-12-02  NaN       0.65    1.20    2.51    1.01    2.58    4.38    0.003750\n",
       "2018-12-03  NaN       0.52    1.20    2.61    1.01    2.58    4.38    0.003750\n",
       "2018-12-04  NaN       0.56    1.19    2.65    1.01    2.58    4.38    0.003750\n",
       "2018-12-05  NaN       0.15    1.19    2.65    0.89    2.58    4.38    0.003750\n",
       "2018-12-06  NaN       0.23    1.22    2.65    0.82    2.58    4.38    0.003750\n",
       "2018-12-07  NaN       0.15    1.20    2.64    0.78    2.58    4.38    0.003750\n",
       "2018-12-08  NaN       0.14    1.15    2.62    0.70    2.58    4.38    0.003750\n",
       "2018-12-09  NaN       0.22    1.15    2.62    0.70    2.58    4.38    0.003750\n",
       "2018-12-10  NaN       0.27    1.13    2.62    0.70    2.58    4.38    0.003750\n",
       "2018-12-11  NaN       0.23    1.13    2.62    0.70    2.58    4.38    0.003750\n",
       "2018-12-12  NaN       0.24    1.13    2.53    0.70    2.58    4.38    0.003750\n",
       "2018-12-13  NaN       0.23    1.01    2.52    0.68    2.58    4.38    0.003750\n",
       "2018-12-14  NaN       0.22    0.99    2.52    0.58    2.58    4.38    0.003750\n",
       "2018-12-15  NaN       0.30    0.99    2.41    0.57    2.58    4.38    0.003750\n",
       "2018-12-16  NaN       0.40    1.00    2.38    0.49    2.58    4.38    0.003750\n",
       "2018-12-17  NaN       0.51    1.00    2.38    0.49    2.58    4.38    0.003750\n",
       "2018-12-18  NaN       0.51    1.00    2.28    0.49    2.58    4.38    0.003750\n",
       "2018-12-19  NaN       0.54    1.01    2.29    0.49    2.58    4.38    0.003750\n",
       "2018-12-20  NaN       0.43    1.03    2.29    0.49    2.58    4.38    0.003750\n",
       "2018-12-21  NaN       0.41    1.03    2.03    0.49    2.32    4.14    0.003750\n",
       "2018-12-22  NaN       0.43    1.02    1.50    0.49    1.79    3.99    0.003750\n",
       "2018-12-23  NaN       0.40    1.01    1.50    0.49    1.79    3.99    0.003750\n",
       "2018-12-24  NaN       0.48    1.01    1.49    0.44    1.79    3.99    0.003750\n",
       "2018-12-25  NaN       0.56    1.01    1.49    0.49    1.79    3.99    0.003750\n",
       "2018-12-26  NaN       0.56    1.02    1.49    0.49    1.79    3.99    0.003750\n",
       "2018-12-27  NaN       0.53    1.01    1.48    0.49    1.79    3.99    0.003750\n",
       "2018-12-28  NaN       0.62    1.00    1.48    0.49    1.79    3.99    0.003750\n",
       "2018-12-29  NaN       0.70    1.00    1.48    0.49    1.59    3.99    0.003750\n",
       "2018-12-30  NaN       0.77    1.00    1.40    0.51    1.57    3.99    0.003750\n",
       "2018-12-31  NaN       0.84    1.02    1.38    0.60    1.57    3.99    0.003750\n",
       "2019-01-01  NaN       0.74    1.06    1.38    0.60    1.57    3.99    0.003750\n",
       "2019-01-02  NaN       0.69    1.25    1.31    0.60    1.57    3.99    0.003750\n",
       "2019-01-03  NaN       0.68    1.25    1.26    0.60    1.57    3.99    0.003750\n",
       "2019-01-04  NaN       0.58    1.26    1.26    0.60    1.57    3.99    0.003750\n",
       "2019-01-05  NaN       0.50    1.26    1.26    0.60    1.57    3.99    0.003750\n",
       "2019-01-06  NaN       0.48    1.25    1.26    0.60    1.57    3.99    0.003750\n",
       "2019-01-07  NaN       0.51    1.25    1.20    0.60    1.57    3.99    0.003750\n",
       "2019-01-08  NaN       0.50    1.25    1.29    0.60    1.57    3.99    0.003750\n",
       "2019-01-09  NaN       0.61    1.25    1.35    0.60    1.57    3.99    0.003750\n",
       "2019-01-10  NaN       0.60    1.24    1.29    0.60    1.60    3.99    0.003750\n",
       "2019-01-11  NaN       0.58    1.24    1.29    0.51    1.79    3.99    0.003750\n",
       "2019-01-12  NaN       0.62    1.20    1.39    0.51    1.79    3.99    0.003750\n",
       "2019-01-13  NaN       0.64    1.15    1.41    0.51    1.79    3.99    0.003750\n",
       "2019-01-14  NaN       0.65    1.18    1.54    0.51    1.79    3.99    0.003750\n",
       "2019-01-15  NaN       0.62    1.18    1.56    0.51    1.79    3.99    0.003750\n",
       "2019-01-16  NaN       0.60    1.17    1.55    0.51    1.79    3.99    0.003750\n",
       "2019-01-17  NaN       0.46    0.99    1.55    0.51    1.79    3.99    0.003750\n",
       "2019-01-18  NaN       0.42    0.91    1.52    0.51    1.79    3.99    0.003750\n",
       "2019-01-19  NaN       0.36    0.91    1.51    0.51    1.79    3.99    0.003750\n",
       "2019-01-20  NaN       0.40    0.92    1.51    0.51    1.79    3.99    0.003750\n",
       "2019-01-21  NaN       0.47    0.93    1.46    0.57    1.79    3.99    0.003750\n",
       "2019-01-22  NaN       0.38    0.93    1.44    0.63    1.79    3.99    0.003750\n",
       "2019-01-23  NaN       0.26    0.89    1.30    0.63    1.79    3.99    0.003750\n",
       "2019-01-24  NaN       0.23    0.83    0.99    0.63    1.79    3.99    0.003750\n",
       "2019-01-25  NaN       0.27    0.85    1.18    0.63    1.79    3.99    0.003750\n",
       "2019-01-26  NaN       0.22    1.01    1.11    0.63    1.79    3.99    0.003750\n",
       "2019-01-27  NaN       0.26    1.03    0.91    0.63    1.79    3.99    0.003750\n",
       "2019-01-28  NaN       0.26    1.03    0.91    0.63    1.79    3.99    0.003750\n",
       "2019-01-29  NaN       0.25    0.96    0.91    0.63    1.79    3.99    0.003750\n",
       "2019-01-30  NaN       0.22    0.92    0.91    0.63    1.79    3.99    0.003750\n",
       "2019-01-31  NaN       0.23    0.87    0.91    0.62    1.79    3.99    0.003750\n",
       "2019-02-01  NaN       0.21    0.81    0.90    0.54    1.79    3.99    0.003750\n",
       "2019-02-02  NaN       0.16    0.69    0.83    0.54    1.79    3.99    0.003750\n",
       "2019-02-03  NaN       0.18    0.70    0.80    0.54    1.79    3.99    0.003750\n",
       "2019-02-04  NaN       0.18    0.68    0.80    0.54    1.79    3.99    0.003750\n",
       "2019-02-05  NaN       0.24    0.67    0.80    0.54    1.57    3.99    0.003750\n",
       "2019-02-06  NaN       0.28    0.67    0.80    0.54    1.57    3.99    0.003750\n",
       "2019-02-07  NaN       0.22    0.67    0.80    0.54    1.57    3.99    0.003750\n",
       "2019-02-08  NaN       0.20    0.66    0.82    0.54    1.57    3.99    0.003750\n",
       "2019-02-09  NaN       0.20    0.68    0.86    0.54    1.57    3.99    0.003750\n",
       "2019-02-10  NaN       0.22    0.68    0.85    0.45    1.57    3.99    0.003750\n",
       "2019-02-11  NaN       0.29    0.68    0.86    0.45    1.57    3.99    0.003750\n",
       "2019-02-12  NaN       0.22    0.69    0.88    0.45    1.57    3.99    0.003750\n",
       "2019-02-13  NaN       0.23    0.77    0.93    0.36    1.57    3.99    0.003750\n",
       "2019-02-14  NaN       0.19    0.78    0.85    0.36    1.57    3.99    0.003750\n",
       "2019-02-15  NaN       0.19    0.74    0.67    0.36    1.43    3.99    0.003750\n",
       "2019-02-16  NaN       0.25    0.69    0.43    0.36    1.19    3.99    0.003750\n",
       "2019-02-17  NaN       0.24    0.72    0.43    0.36    1.19    3.99    0.003750\n",
       "2019-02-18  NaN       0.33    0.74    0.43    0.36    1.19    3.99    0.003750\n",
       "2019-02-19  NaN       0.30    0.74    0.43    0.36    1.19    3.99    0.003750\n",
       "2019-02-20  NaN       0.29    0.72    0.43    0.36    1.19    3.99    0.003750\n",
       "2019-02-21  NaN       0.26    0.70    0.41    0.36    1.19    3.99    0.003750\n",
       "2019-02-22  NaN       0.26    0.78    0.55    0.36    1.19    3.99    0.003750\n",
       "2019-02-23  NaN       0.23    0.80    0.62    0.36    1.16    3.99    0.003750\n",
       "2019-02-24  NaN       0.26    0.79    0.68    0.36    1.16    3.99    0.003750\n",
       "2019-02-25  NaN       0.34    0.83    0.71    0.25    1.31    3.99    0.003750\n",
       "2019-02-26  NaN       0.35    0.84    0.72    0.23    1.31    3.99    0.003750\n",
       "2019-02-27  NaN       0.45    0.87    0.73    0.23    1.31    3.99    0.003750\n",
       "2019-02-28  NaN       0.60    0.89    0.81    0.21    1.31    3.99    0.003750\n",
       "2019-03-01  NaN       0.53    0.91    0.74    0.31    1.31    3.99    0.003750\n",
       "2019-03-02  NaN       0.39    0.96    0.72    0.33    1.31    3.99    0.003750\n",
       "2019-03-03  NaN       0.45    0.93    0.81    0.40    1.31    3.99    0.003750\n",
       "2019-03-04  NaN       0.42    0.92    0.83    0.40    1.31    3.99    0.003750\n",
       "2019-03-05  NaN       0.39    0.98    0.83    0.40    1.31    3.99    0.003750\n",
       "2019-03-06  NaN       0.46    1.07    0.92    0.40    1.31    3.99    0.003750\n",
       "2019-03-07  NaN       0.43    1.17    0.96    0.40    1.31    3.99    0.003750\n",
       "2019-03-08  NaN       0.38    1.28    0.85    0.40    1.31    3.99    0.003750\n",
       "2019-03-09  NaN       0.55    1.31    0.90    0.46    1.31    3.99    0.003750\n",
       "2019-03-10  NaN       0.67    1.12    0.91    0.60    1.31    3.99    0.003750\n",
       "2019-03-11  NaN       0.68    1.01    0.93    0.71    1.31    3.99    0.003750\n",
       "2019-03-12  NaN       0.71    1.11    0.92    0.72    1.31    3.99    0.003750\n",
       "2019-03-13  NaN       0.64    1.10    0.93    0.72    1.31    3.99    0.003750\n",
       "2019-03-14  NaN       0.67    1.08    1.00    0.72    1.31    3.99    0.003750\n",
       "2019-03-15  NaN       0.64    1.08    1.04    0.72    1.31    3.99    0.003750\n",
       "2019-03-16  NaN       0.65    1.09    1.05    0.72    1.31    3.99    0.003750\n",
       "2019-03-17  NaN       0.70    1.10    1.11    0.62    1.31    3.99    0.003750\n",
       "2019-03-18  NaN       0.70    1.11    1.12    0.60    1.31    3.99    0.003750\n",
       "2019-03-19  NaN       0.66    1.11    1.21    0.63    1.31    3.99    0.003750\n",
       "2019-03-20  NaN       0.65    1.33    1.25    0.72    1.31    3.99    0.003750\n",
       "2019-03-21  NaN       0.58    1.39    1.25    0.72    1.31    3.99    0.003750\n",
       "2019-03-22  NaN       0.56    1.32    1.27    0.72    1.31    3.99    0.003750\n",
       "2019-03-23  NaN       0.55    1.39    1.28    0.72    1.31    3.99    0.003750\n",
       "2019-03-24  NaN       0.79    1.41    1.29    0.84    1.31    3.99    0.003750\n",
       "2019-03-25  NaN       0.75    1.27    1.32    0.84    1.31    3.99    0.003750\n",
       "2019-03-26  NaN       0.75    1.35    1.34    0.84    1.31    3.99    0.003750\n",
       "2019-03-27  NaN       0.77    1.52    1.40    0.84    1.31    3.99    0.003750\n",
       "2019-03-28  NaN       0.79    1.59    1.39    0.84    1.31    3.99    0.003750\n",
       "2019-03-29  NaN       0.79    1.52    1.43    0.84    1.31    3.99    0.003750\n",
       "2019-03-30  NaN       0.74    1.34    1.42    0.98    1.31    3.99    0.003750\n",
       "2019-03-31  NaN       0.74    1.28    1.43    0.98    1.31    3.99    0.003750\n",
       "2019-04-01  NaN       0.72    1.29    1.46    0.98    1.31    3.99    0.003750\n",
       "2019-04-02  NaN       0.71    1.30    1.47    0.98    1.31    3.99    0.003750\n",
       "2019-04-03  NaN       0.67    1.32    1.48    0.98    1.31    3.99    0.003750\n",
       "2019-04-04  NaN       0.71    1.35    1.49    0.98    1.31    3.99    0.003750\n",
       "2019-04-05  NaN       0.77    1.35    1.50    1.14    1.31    3.99    0.003750\n",
       "2019-04-06  NaN       0.77    1.37    1.51    1.14    1.31    3.99    0.003750\n",
       "2019-04-07  NaN       0.78    1.37    1.56    1.14    1.31    3.99    0.003750\n",
       "2019-04-08  NaN       1.06    1.40    1.71    1.33    1.31    3.99    0.003750\n",
       "2019-04-09  NaN       1.19    1.58    1.76    1.33    1.31    3.99    0.003750\n",
       "2019-04-10  NaN       1.08    1.62    1.59    1.33    1.31    3.99    0.003750\n",
       "2019-04-11  NaN       1.19    1.44    1.55    1.33    1.31    3.99    0.003750\n",
       "2019-04-12  NaN       1.02    1.45    1.55    1.33    1.31    3.99    0.003750\n",
       "2019-04-13  NaN       1.13    1.53    1.64    1.33    1.31    3.99    0.003750\n",
       "2019-04-14  NaN       1.12    1.53    1.72    1.52    1.31    3.99    0.003750\n",
       "2019-04-15  NaN       1.24    1.59    1.74    1.52    1.31    3.99    0.003750\n",
       "2019-04-16  NaN       1.21    1.71    1.74    1.52    1.36    3.99    0.003750\n",
       "2019-04-17  NaN       1.23    1.88    1.74    1.52    1.53    3.99    0.003750\n",
       "2019-04-18  NaN       1.17    1.90    1.74    1.52    1.53    3.99    0.003750\n",
       "2019-04-19  NaN       1.13    1.91    1.74    1.52    1.53    3.99    0.003750\n",
       "2019-04-20  NaN       1.04    1.96    1.71    1.74    1.53    3.99    0.003750\n",
       "2019-04-21  NaN       1.12    2.07    1.74    1.74    1.53    3.99    0.003750\n",
       "2019-04-22  NaN       0.96    2.02    1.77    1.74    1.53    3.99    0.003750\n",
       "2019-04-23  NaN       0.73    2.01    1.77    1.57    1.53    3.99    0.003750\n",
       "2019-04-24  NaN       0.60    2.02    1.76    1.52    1.53    3.99    0.003750\n",
       "2019-04-25  NaN       0.51    2.02    1.76    1.52    1.53    3.99    0.003750\n",
       "2019-04-26  NaN       0.40    2.04    1.80    1.52    1.53    3.99    0.003750\n",
       "2019-04-27  NaN       0.58    2.07    1.83    1.52    1.53    3.99    0.003750\n",
       "2019-04-28  NaN       0.59    2.04    2.05    1.48    1.53    3.99    0.003750\n",
       "2019-04-29  NaN       0.66    1.99    2.40    1.30    1.53    3.99    0.003750\n",
       "2019-04-30  NaN       0.74    1.99    2.44    1.30    1.82    3.99    0.003750\n",
       "2019-05-01  NaN       0.93    1.98    2.42    1.30    1.97    3.99    0.003750\n",
       "2019-05-02  NaN       1.08    2.01    2.43    1.30    1.97    3.99    0.003750\n",
       "2019-05-03  NaN       1.16    2.01    2.19    1.30    2.12    3.99    0.003750\n",
       "2019-05-04  NaN       1.19    2.02    2.17    1.30    2.19    3.99    0.003750\n",
       "2019-05-05  NaN       1.35    2.05    2.19    1.30    2.19    3.99    0.003750\n",
       "2019-05-06  NaN       1.46    2.23    2.19    1.30    2.37    3.99    0.003750\n",
       "2019-05-07  NaN       1.28    2.32    2.20    1.30    2.48    3.99    0.003750\n",
       "2019-05-08  NaN       1.28    2.40    2.20    1.30    2.48    3.99    0.003750\n",
       "2019-05-09  NaN       1.21    2.43    2.20    1.30    2.48    3.99    0.003750\n",
       "2019-05-10  NaN       1.13    2.49    2.20    1.30    2.70    3.99    0.003750\n",
       "2019-05-11  NaN       1.15    2.58    2.28    1.30    2.77    3.99    0.003750\n",
       "2019-05-12  NaN       1.20    2.65    2.41    1.30    2.77    3.99    0.003750\n",
       "2019-05-13  NaN       1.48    2.81    2.51    1.30    2.99    3.99    0.003750\n",
       "2019-05-14  NaN       1.63    2.79    2.54    1.30    3.06    3.99    0.003750\n",
       "2019-05-15  NaN       1.31    2.86    2.55    1.30    3.06    3.99    0.003750\n",
       "2019-05-16  NaN       1.31    3.00    2.56    1.30    3.06    3.99    0.003750\n",
       "2019-05-17  NaN       1.14    3.00    2.56    1.30    3.06    3.99    0.003750\n",
       "2019-05-18  NaN       1.03    2.86    2.56    1.30    3.06    3.99    0.003750\n",
       "2019-05-19  NaN       1.12    2.83    2.55    1.30    3.06    3.99    0.003750\n",
       "2019-05-20  NaN       1.04    2.81    2.55    1.30    3.06    3.99    0.003750\n",
       "2019-05-21  NaN       1.08    2.80    2.55    1.30    3.06    3.99    0.003750\n",
       "2019-05-22  NaN       1.18    2.76    2.55    1.30    3.06    3.99    0.003750\n",
       "2019-05-23  NaN       1.21    2.72    2.55    1.30    3.06    3.99    0.003750\n",
       "2019-05-24  NaN       1.17    2.70    2.59    1.30    3.06    3.99    0.003750\n",
       "2019-05-25  NaN       1.20    2.70    2.61    1.23    3.06    3.99    0.003750\n",
       "2019-05-26  NaN       1.31    2.68    2.62    1.11    3.06    3.99    0.003750\n",
       "2019-05-27  NaN       1.50    2.77    2.67    1.11    3.06    3.99    0.003750\n",
       "2019-05-28  NaN       1.42    2.67    2.72    1.11    3.06    3.99    0.003750\n",
       "2019-05-29  NaN       1.45    2.58    2.72    1.11    3.06    3.99    0.003750\n",
       "2019-05-30  NaN       1.46    2.76    2.71    1.15    3.06    3.99    0.003750\n",
       "2019-05-31  NaN       1.56    2.78    2.71    1.33    3.06    3.99    0.003750\n",
       "2019-06-01  NaN       1.63    2.66    2.71    1.33    3.06    3.99    0.003750\n",
       "2019-06-02  NaN       1.38    2.71    2.69    1.33    3.06    3.99    0.003750\n",
       "2019-06-03  NaN       1.63    2.69    2.70    1.33    3.06    3.99    0.003750\n",
       "2019-06-04  NaN       1.79    2.74    2.69    1.33    3.06    3.99    0.003750\n",
       "2019-06-05  NaN       1.62    2.77    2.68    1.33    3.06    3.99    0.003750\n",
       "2019-06-06  NaN       1.64    2.77    2.71    1.65    3.06    3.99    0.003750\n",
       "2019-06-07  NaN       1.65    2.76    2.72    1.77    3.06    3.99    0.003750\n",
       "2019-06-08  NaN       1.63    2.77    2.73    1.64    3.06    3.99    0.003750\n",
       "2019-06-09  NaN       1.55    2.60    2.78    1.55    3.06    3.99    0.003750\n",
       "2019-06-10  NaN       1.31    2.63    2.79    1.55    3.06    3.99    0.003750\n",
       "2019-06-11  NaN       1.39    2.67    2.75    1.55    3.06    3.99    0.003750\n",
       "2019-06-12  NaN       1.52    2.74    2.62    1.55    3.06    3.99    0.003750\n",
       "2019-06-13  NaN       1.51    3.01    2.67    1.55    3.06    3.99    0.003750\n",
       "2019-06-14  NaN       1.45    3.06    2.73    1.55    3.06    3.99    0.003750\n",
       "2019-06-15  NaN       1.26    3.12    2.82    1.55    3.06    3.99    0.003750\n",
       "2019-06-16  NaN       1.34    3.31    2.82    1.55    3.06    3.99    0.003750\n",
       "2019-06-17  NaN       1.41    3.34    3.03    1.55    3.06    3.99    0.003750\n",
       "2019-06-18  NaN       1.44    3.47    3.07    1.55    3.06    3.99    0.003750\n",
       "2019-06-19  NaN       1.46    3.57    3.07    1.55    3.06    3.99    0.003750\n",
       "2019-06-20  NaN       1.55    3.51    3.23    1.55    3.06    3.99    0.003750\n",
       "2019-06-21  NaN       1.47    3.36    3.29    1.55    3.06    3.99    0.003750\n",
       "2019-06-22  NaN       1.64    3.69    3.33    1.62    3.06    3.99    0.003750\n",
       "2019-06-23  NaN       1.65    3.76    3.49    1.99    3.06    3.99    0.003750\n",
       "2019-06-24  NaN       1.38    3.50    3.54    1.99    3.06    3.99    0.003750\n",
       "2019-06-25  NaN       1.29    3.48    3.55    1.99    3.06    3.99    0.003750\n",
       "2019-06-26  NaN       1.15    3.50    3.52    1.82    3.06    3.99    0.003750\n",
       "2019-06-27  NaN       1.17    3.56    3.37    1.77    3.06    3.99    0.003750\n",
       "2019-06-28  NaN       1.19    3.70    3.30    1.77    3.06    3.99    0.003750\n",
       "2019-06-29  NaN       1.12    3.69    3.30    1.77    3.06    3.99    0.003750\n",
       "2019-06-30  NaN       1.32    3.95    3.56    1.85    2.96    3.99    0.003750\n",
       "2019-07-01  NaN       1.46    4.05    3.99    1.99    2.79    3.99    0.003750\n",
       "2019-07-02  NaN       1.55    3.68    3.99    1.99    2.79    4.02    0.003750\n",
       "2019-07-03  NaN       1.70    3.70    4.03    1.99    2.79    4.38    0.003750\n",
       "2019-07-04  NaN       1.61    3.70    4.03    1.99    2.79    4.38    0.003750\n",
       "2019-07-05  NaN       1.65    3.71    4.08    1.99    2.79    4.38    0.003750\n",
       "2019-07-06  NaN       1.58    3.68    4.07    1.99    2.79    4.38    0.003750\n",
       "2019-07-07  NaN       1.49    3.67    4.14    1.99    2.79    4.38    0.003750\n",
       "2019-07-08  NaN       1.40    3.64    4.17    1.99    2.79    4.38    0.003750\n",
       "2019-07-09  NaN       1.26    3.63    4.18    1.99    2.79    4.38    0.003750\n",
       "2019-07-10  NaN       1.11    3.62    4.19    1.99    2.79    4.38    0.003750\n",
       "2019-07-11  NaN       1.23    3.63    4.21    1.99    2.79    4.38    0.003750\n",
       "2019-07-12  NaN       1.19    3.63    4.19    1.99    2.79    4.38    0.003750\n",
       "2019-07-13  NaN       1.16    3.63    4.19    1.99    2.79    4.38    0.003750\n",
       "2019-07-14  NaN       1.07    3.61    4.19    1.99    2.79    4.38    0.003750\n",
       "2019-07-15  NaN       1.33    3.62    4.22    1.99    2.79    4.38    0.003750\n",
       "2019-07-16  NaN       1.30    3.31    3.97    1.99    2.79    4.38    0.003750\n",
       "2019-07-17  NaN       1.39    3.24    3.50    1.99    2.79    4.38    0.003750\n",
       "2019-07-18  NaN       1.30    3.26    3.50    1.99    2.79    4.38    0.003750\n",
       "2019-07-19  NaN       1.16    3.24    3.50    1.99    2.79    4.38    0.003750\n",
       "2019-07-20  NaN       1.02    3.24    3.50    2.20    2.79    4.38    0.003750\n",
       "2019-07-21  NaN       1.05    3.25    3.49    2.28    2.79    4.38    0.003750\n",
       "2019-07-22  NaN       0.98    3.51    3.49    2.28    2.79    4.38    0.003750\n",
       "2019-07-23  NaN       0.91    3.56    3.49    2.05    2.79    4.38    0.003750\n",
       "2019-07-24  NaN       0.92    3.54    3.49    1.99    2.79    4.38    0.003750\n",
       "2019-07-25  NaN       0.91    3.55    3.49    1.99    2.79    4.38    0.003750\n",
       "2019-07-26  NaN       0.98    3.54    3.54    1.99    2.79    4.38    0.003750\n",
       "2019-07-27  NaN       0.94    3.54    3.46    1.99    2.79    4.38    0.003750\n",
       "2019-07-28  NaN       0.94    3.52    3.21    1.99    2.79    4.38    0.003750\n",
       "2019-07-29  NaN       1.19    3.52    3.21    1.99    2.79    4.38    0.003750\n",
       "2019-07-30  NaN       1.25    3.48    3.20    1.99    2.79    4.38    0.003750\n",
       "2019-07-31  NaN       1.01    3.20    3.20    1.99    2.79    4.38    0.003750\n",
       "2019-08-01  NaN       1.01    2.80    3.19    1.99    2.79    4.38    0.003750\n",
       "2019-08-02  NaN       0.96    2.80    3.19    1.83    2.79    4.38    0.003750\n",
       "2019-08-03  NaN       0.94    2.80    3.19    1.77    2.79    4.38    0.003750\n",
       "2019-08-04  NaN       1.06    2.80    3.09    1.77    2.79    4.38    0.003750\n",
       "2019-08-05  NaN       0.94    2.79    3.08    1.77    2.79    4.38    0.003750\n",
       "2019-08-06  NaN       0.94    2.73    2.98    1.77    2.79    4.38    0.003750\n",
       "2019-08-07  NaN       0.84    2.71    2.88    1.77    2.79    4.38    0.003750\n",
       "2019-08-08  NaN       0.78    2.67    2.87    1.77    2.79    4.38    0.003750\n",
       "2019-08-09  NaN       0.93    2.70    2.87    1.77    2.79    4.38    0.003750\n",
       "2019-08-10  NaN       1.10    2.72    2.89    1.77    2.79    4.38    0.003750\n",
       "2019-08-11  NaN       1.14    2.71    2.93    1.77    2.79    4.38    0.003750\n",
       "2019-08-12  NaN       1.34    2.71    2.93    1.77    2.79    4.38    0.003750\n",
       "2019-08-13  NaN       1.34    2.69    2.93    1.77    2.79    4.38    0.003750\n",
       "2019-08-14  NaN       1.32    2.69    2.93    1.77    2.79    4.38    0.003750\n",
       "2019-08-15  NaN       1.46    2.65    2.93    1.77    2.79    4.38    0.003750\n",
       "2019-08-16  NaN       1.71    2.57    2.90    1.77    2.79    4.38    0.003750\n",
       "2019-08-17  NaN       1.71    2.70    2.90    1.77    2.79    4.38    0.003750\n",
       "2019-08-18  NaN       1.51    2.78    2.90    1.77    2.79    4.38    0.003750\n",
       "2019-08-19  NaN       1.48    2.79    2.88    1.77    2.79    4.38    0.003750\n",
       "2019-08-20  NaN       1.50    2.79    2.88    1.77    2.79    4.38    0.003750\n",
       "2019-08-21  NaN       1.44    2.79    2.88    1.77    2.79    4.38    0.003750\n",
       "2019-08-22  NaN       1.39    2.77    2.89    1.77    2.79    4.38    0.003750\n",
       "2019-08-23  NaN       1.48    2.64    2.91    1.77    2.79    4.38    0.003750\n",
       "2019-08-24  NaN       1.31    2.64    2.91    1.77    2.79    4.38    0.003750\n",
       "2019-08-25  NaN       1.19    2.64    2.91    1.77    2.79    4.38    0.003750\n",
       "2019-08-26  NaN       1.16    2.63    2.92    1.77    2.79    4.38    0.003750\n",
       "2019-08-27  NaN       1.22    2.61    2.92    1.77    2.58    4.38    0.003750\n",
       "2019-08-28  NaN       1.45    2.61    2.96    1.77    2.50    4.38    0.003750\n",
       "2019-08-29  NaN       1.59    2.62    2.97    1.77    2.73    4.38    0.003750\n",
       "2019-08-30  NaN       1.73    2.84    3.01    1.77    2.79    4.38    0.003750\n",
       "2019-08-31  NaN       1.72    2.93    3.12    1.77    2.79    4.38    0.003750\n",
       "2019-09-01  NaN       1.66    2.95    3.12    1.77    2.79    4.38    0.003750\n",
       "2019-09-02  NaN       1.57    2.98    3.04    1.77    2.79    4.38    0.003750\n",
       "2019-09-03  NaN       1.32    2.98    2.95    1.77    2.79    4.38    0.003750\n",
       "2019-09-04  NaN       1.27    2.96    2.95    1.77    2.79    4.38    0.003750\n",
       "2019-09-05  NaN       1.43    2.95    2.94    1.77    2.79    4.38    0.003750\n",
       "2019-09-06  NaN       1.65    2.95    2.99    1.77    2.79    4.38    0.003750\n",
       "2019-09-07  NaN       1.66    3.07    2.99    1.77    2.79    4.38    0.003750\n",
       "2019-09-08  NaN       1.48    3.07    2.99    1.55    2.79    4.38    0.003750\n",
       "2019-09-09  NaN       1.44    2.78    3.00    1.55    2.79    4.38    0.003750\n",
       "2019-09-10  NaN       1.43    2.77    2.97    1.55    2.79    4.38    0.003750\n",
       "2019-09-11  NaN       1.42    2.77    2.96    1.55    2.79    4.38    0.003750\n",
       "2019-09-12  NaN       1.24    2.79    2.96    1.55    2.79    4.38    0.003750\n",
       "2019-09-13  NaN       1.22    2.79    2.97    1.55    2.79    4.38    0.003750\n",
       "2019-09-14  NaN       1.36    2.80    2.96    1.55    2.79    4.38    0.003750\n",
       "2019-09-15  NaN       1.46    2.81    2.96    1.55    2.79    4.38    0.003750\n",
       "2019-09-16  NaN       1.54    2.82    2.96    1.55    2.79    4.38    0.003750\n",
       "2019-09-17  NaN       1.56    2.81    2.96    1.55    2.79    4.38    0.003750\n",
       "2019-09-18  NaN       1.38    2.80    2.96    1.62    2.79    4.38    0.003750\n",
       "2019-09-19  NaN       1.37    2.82    2.96    1.77    2.79    4.38    0.003750\n",
       "2019-09-20  NaN       1.46    2.81    2.96    1.77    2.79    4.38    0.003750\n",
       "2019-09-21  NaN       1.21    2.84    2.96    1.77    2.79    4.38    0.003750\n",
       "2019-09-22  NaN       1.29    2.82    3.02    1.77    2.79    4.38    0.003750\n",
       "2019-09-23  NaN       1.32    2.82    3.09    1.77    2.79    4.38    0.003750\n",
       "2019-09-24  NaN       1.35    2.82    3.09    1.77    2.79    4.38    0.003750\n",
       "2019-09-25  NaN       1.29    2.74    2.99    1.77    2.79    4.38    0.003750\n",
       "2019-09-26  NaN       1.23    2.62    2.79    1.77    2.79    4.38    0.003750\n",
       "2019-09-27  NaN       1.38    2.64    2.79    1.77    2.79    4.38    0.003750\n",
       "2019-09-28  NaN       1.34    2.64    2.79    1.77    2.79    4.38    0.003750\n",
       "2019-09-29  NaN       1.16    2.63    2.79    1.77    2.79    4.38    0.003750\n",
       "2019-09-30  NaN       1.00    2.62    2.79    1.77    2.79    4.38    0.003750\n",
       "2019-10-01  NaN       1.16    2.62    2.79    1.77    2.79    4.38    0.003750\n",
       "2019-10-02  NaN       1.16    2.64    2.60    1.77    2.79    4.38    0.003750\n",
       "2019-10-03  NaN       0.99    2.63    2.44    1.77    2.79    4.38    0.003750\n",
       "2019-10-04  NaN       0.93    2.65    2.44    1.77    2.79    4.38    0.003750\n",
       "2019-10-05  NaN       0.99    2.65    2.44    1.77    2.79    4.38    0.003750\n",
       "2019-10-06  NaN       0.92    2.66    2.44    1.77    2.79    4.38    0.003750\n",
       "2019-10-07  NaN       0.91    2.65    2.43    1.77    2.79    4.38    0.003750\n",
       "2019-10-08  NaN       0.82    2.67    2.41    1.77    2.79    4.38    0.003750\n",
       "2019-10-09  NaN       0.94    2.66    2.41    1.77    2.79    4.38    0.003750\n",
       "2019-10-10  NaN       0.78    2.65    2.41    1.77    2.79    4.38    0.003750\n",
       "2019-10-11  NaN       0.80    2.66    2.42    1.77    2.79    4.38    0.003750\n",
       "2019-10-12  NaN       0.90    2.65    2.45    1.77    2.79    4.38    0.003750\n",
       "2019-10-13  NaN       0.94    2.62    2.45    1.77    2.79    4.38    0.003750\n",
       "2019-10-14  NaN       1.43    2.66    2.44    1.77    2.79    4.38    0.003750\n",
       "2019-10-15  NaN       1.38    2.67    2.40    1.77    2.79    4.38    0.003750\n",
       "2019-10-16  NaN       1.27    2.69    2.37    1.77    2.79    4.38    0.003750\n",
       "2019-10-17  NaN       1.27    2.80    2.37    1.77    2.79    4.38    0.003750\n",
       "2019-10-18  NaN       1.30    2.83    2.37    1.77    2.79    4.38    0.003750\n",
       "2019-10-19  NaN       1.39    2.87    2.37    1.77    2.79    4.38    0.003750\n",
       "2019-10-20  NaN       1.33    3.12    2.37    1.77    2.79    4.38    0.003750\n",
       "2019-10-21  5.245     1.31    3.13    2.42    1.83    2.79    4.38    0.003750\n",
       "2019-10-22  6         2.84    3.23    2.77    2.25    2.93    4.38    0.003750\n",
       "2019-10-23  5.29      3.53    3.95    3.37    2.48    3.08    4.38    0.003750\n",
       "2019-10-24  6.295     4.46    4.95    4.78    3.44    3.34    4.27    0.003750\n",
       "2019-10-25  6.118     3.90    6.93    5.49    3.99    4.02    4.19    0.003750\n",
       "2019-10-26  8.94      4.47    7.97    5.75    3.86    4.39    4.19    0.003750\n",
       "2019-10-27  7.24333   5.57    8.17    5.79    3.80    4.39    4.41    0.003750\n",
       "2019-10-28  6.83333   5.98    8.22    6.12    4.12    4.64    4.58    0.006046\n",
       "2019-10-29  NaN       6.01    7.61    6.17    4.35    4.85    4.58    0.006046\n",
       "2019-10-30  6.88      6.08    7.97    6.35    4.10    4.85    4.85    0.006046\n",
       "2019-10-31  5.485     7.56    9.51    7.50    4.79    5.12    5.23    0.008246\n",
       "2019-11-01  7.175     7.55    9.90    7.82    4.57    5.49    5.49    0.001667\n",
       "2019-11-02  7.62      7.34    8.92    7.93    4.57    5.49    5.49    0.001667\n",
       "2019-11-03  7.82      6.64    8.84    7.83    5.03    5.49    5.68    0.005000\n",
       "2019-11-04  7.475     5.67    8.91    7.96    6.00    5.49    5.95    0.005972\n",
       "2019-11-05  8.495     5.90    9.01    8.03    6.14    5.59    5.95    0.005972\n",
       "2019-11-06  9.52      7.10    9.60    8.73    6.14    5.95    6.12    0.005972\n",
       "2019-11-07  7.92333   6.97    10.02   9.18    6.72    5.95    6.54    0.005557\n",
       "2019-11-08  7.755     8.42    10.07   9.14    6.74    6.07    6.54    0.005557\n",
       "2019-11-09  NaN       9.47    10.34   9.65    6.74    6.54    6.75    0.005838\n",
       "2019-11-10  NaN       9.34    11.57   9.77    7.44    6.54    7.27    0.009163\n",
       "2019-11-11  10.68     9.68    13.09   10.29   7.53    6.69    7.27    0.005086\n",
       "2019-11-12  9.18625   11.17   12.92   10.60   7.53    7.27    7.48    0.005086\n",
       "2019-11-13  10.34     11.58   12.86   11.81   7.53    7.27    8.00    0.005086\n",
       "2019-11-14  10.79     12.08   13.50   12.35   7.53    7.42    8.00    0.006987\n",
       "2019-11-15  9.81167   11.14   13.36   12.29   7.53    8.00    8.21    0.011250\n",
       "2019-11-16  10.06     9.46    12.04   11.88   7.53    8.00    8.73    0.011250\n",
       "2019-11-17  10.495    8.29    11.93   11.49   7.32    8.15    8.73    0.003769\n",
       "2019-11-18  11.3778   7.83    11.94   11.50   6.80    8.73    8.94    0.005506\n",
       "2019-11-19  11.6775   6.39    12.06   11.48   6.80    8.73    9.44    0.005506\n",
       "2019-11-20  NaN       6.90    12.01   11.41   6.22    8.85    9.44    0.005506\n",
       "2019-11-21  11.79     6.28    12.16   11.48   6.47    9.44    9.73    0.005520\n",
       "2019-11-22  10.3      7.10    12.80   11.72   7.96    8.86    10.44   0.005520\n",
       "2019-11-23  10        7.27    12.66   11.49   8.44    8.45    10.44   0.005520\n",
       "2019-11-24  4.33      7.11    12.52   11.29   8.52    8.18    10.73   0.007222\n",
       "2019-11-25  11.7633   7.22    13.12   11.38   7.79    7.72    11.44   0.007543\n",
       "2019-11-26  9.99      7.24    12.94   11.45   7.79    7.72    11.44   0.007923\n",
       "2019-11-27  NaN       7.63    12.87   11.38   7.79    7.72    11.44   0.007923\n",
       "2019-11-28  11.975    8.31    12.76   11.38   7.79    7.72    11.44   0.005228\n",
       "2019-11-29  11        7.64    12.50   11.38   7.79    8.09    11.44   0.007503\n",
       "2019-11-30  11.506    7.50    12.17   11.38   9.21    8.71    11.44   0.003333\n",
       "2019-12-01  14.48     6.63    12.17   11.38   9.49    8.71    11.44   0.002271\n",
       "2019-12-02  12.74     6.48    12.20   11.38   9.49    8.71    11.44   0.003489\n",
       "2019-12-03  11        7.46    12.19   11.40   8.54    8.71    11.44   0.003489\n",
       "2019-12-04  11.79     8.45    12.12   11.45   8.50    8.71    11.44   0.003489\n",
       "2019-12-05  11.49     8.24    12.35   11.44   8.50    8.71    11.44   0.006306\n",
       "2019-12-06  NaN       8.55    11.93   11.44   8.50    8.71    11.44   0.006306\n",
       "2019-12-07  11.59     8.25    11.93   11.41   8.32    8.71    11.44   0.005000\n",
       "2019-12-08  NaN       8.62    11.89   11.40   7.77    8.71    11.44   0.010014\n",
       "2019-12-09  11.5767   8.06    11.68   11.40   7.77    8.71    11.44   0.010014\n",
       "2019-12-10  13.5      7.47    11.25   11.39   7.77    8.71    11.44   0.004680\n",
       "2019-12-11  10.8825   8.26    10.85   11.38   7.77    8.71    11.44   0.004680\n",
       "2019-12-12  11.99     8.24    10.86   11.38   7.77    10.44   11.44   0.005186\n",
       "2019-12-13  12.485    8.48    10.96   10.86   7.77    10.69   11.44   0.005186\n",
       "2019-12-14  NaN       8.06    10.88   10.68   7.25    10.61   11.44   0.002667\n",
       "2019-12-15  11.42     7.93    10.80   10.58   7.04    9.69    11.44   0.011314\n",
       "2019-12-16  NaN       7.96    11.27   10.58   7.04    9.69    11.44   0.007181\n",
       "2019-12-17  12.236    9.43    11.49   10.60   7.56    9.69    11.44   0.019210\n",
       "2019-12-18  11.99     9.17    11.54   10.63   7.77    9.69    11.44   0.011429\n",
       "2019-12-19  NaN       8.40    11.52   10.63   7.77    9.69    11.44   0.004089\n",
       "2019-12-20  12.75     9.00    11.47   10.69   7.77    9.69    11.44   0.001905\n",
       "2019-12-21  11.355    10.14   11.48   10.63   8.55    9.69    11.44   0.011439\n",
       "2019-12-22  12.89     11.18   11.67   10.38   9.68    9.69    11.44   0.010797\n",
       "2019-12-23  13.745    10.08   12.44   10.80   11.04   9.69    11.44   0.004660\n",
       "2019-12-24  NaN       8.72    12.59   11.08   10.46   9.81    11.44   0.003955\n",
       "2019-12-25  NaN       7.79    13.09   10.90   10.04   9.99    11.44   0.005333\n",
       "2019-12-26  12.56     7.78    13.25   10.95   10.04   9.99    11.44   0.002945\n",
       "2019-12-27  12.75     7.83    13.60   10.96   10.04   9.99    11.44   0.001250\n",
       "2019-12-28  14.9967   7.53    13.29   10.96   9.92    9.99    11.44   0.005330\n",
       "2019-12-29  NaN       6.36    13.22   10.96   9.04    9.99    11.44   0.004167\n",
       "2019-12-30  13.82     5.94    13.14   10.96   9.04    9.99    11.44   0.002291\n",
       "2019-12-31  14.3375   5.74    13.13   10.94   7.75    9.99    11.44   0.000702\n",
       "2020-01-01  NaN       5.77    13.12   10.94   7.32    9.99    11.44   0.001876\n",
       "2020-01-02  14.99     5.02    13.10   10.97   7.32    9.99    11.44   0.002001\n",
       "2020-01-03  14.44     5.20    13.10   10.93   7.32    9.99    11.44   0.005614\n",
       "2020-01-04  13        5.62    13.11   10.93   7.81    9.99    11.44   0.006219\n",
       "2020-01-05  12.375    5.58    13.23   10.91   8.05    9.99    11.44   0.009184\n",
       "2020-01-06  14.5      5.69    12.72   10.91   8.05    9.99    11.44   0.003828\n",
       "2020-01-07  12.5      6.02    12.29   10.90   8.05    9.99    11.44   0.005492\n",
       "2020-01-08  12.75     7.43    11.96   10.89   8.05    9.99    11.44   0.007304\n",
       "2020-01-09  12        9.28    12.07   10.40   8.05    9.99    11.44   0.003891\n",
       "2020-01-10  14.385    10.45   11.94   10.34   8.05    9.99    11.44   0.006377\n",
       "2020-01-11  11.74     9.61    11.92   10.38   8.05    9.99    11.44   0.002857\n",
       "2020-01-12  12.646    9.64    11.87   11.15   9.87    9.99    11.44   0.006155\n",
       "2020-01-13  15        9.20    11.97   11.26   10.04   9.57    11.44   0.004002\n",
       "2020-01-14  12.3333   8.53    12.09   11.25   10.04   8.99    11.44   0.007622\n",
       "2020-01-15  11.485    8.48    11.98   11.22   10.04   8.99    11.44   0.005060\n",
       "2020-01-16  NaN       7.79    11.74   11.14   10.04   8.99    11.44   0.006832\n",
       "2020-01-17  13.5      7.97    12.67   11.22   9.50    9.86    11.44   0.005591\n",
       "2020-01-18  12.5      7.50    12.66   11.22   9.04    9.99    11.44   0.009412\n",
       "2020-01-19  11.495    6.74    12.66   11.52   9.04    9.99    11.44   0.009170\n",
       "2020-01-20  13.25     7.03    12.29   11.47   9.04    9.99    11.44   0.005265\n",
       "2020-01-21  12.5      8.20    11.98   11.45   8.13    9.99    11.44   0.005641\n",
       "2020-01-22  11.738    8.20    12.22   11.27   8.05    9.99    11.44   0.002353\n",
       "2020-01-23  11.99     8.64    11.86   11.23   8.05    9.99    11.44   0.003791\n",
       "2020-01-24  10.5      9.95    12.06   11.23   8.05    9.99    11.44   0.007083\n",
       "2020-01-25  13.25     10.42   12.56   11.20   8.05    9.12    11.44   0.005869\n",
       "2020-01-26  12        10.63   12.53   10.93   8.05    8.99    11.44   0.012358\n",
       "2020-01-27  10.5      9.39    12.54   10.88   8.83    9.78    11.44   0.004243\n",
       "2020-01-28  12        8.30    11.95   10.96   9.04    9.99    11.44   0.009905\n",
       "2020-01-29  12.94     7.88    11.82   10.97   9.04    9.99    11.44   0.005336\n",
       "2020-01-30  13        7.34    11.78   10.91   9.04    9.99    11.44   0.006325\n",
       "2020-01-31  NaN       6.77    11.88   10.91   9.04    9.99    11.44   0.006325\n",
       "2020-02-01  9.5       6.15    11.84   10.91   9.04    9.99    11.40   0.005725\n",
       "2020-02-02  11.6575   6.84    11.74   10.78   9.04    9.99    10.44   0.006091\n",
       "2020-02-03  13.5      6.36    11.71   10.19   9.04    9.99    10.44   0.005079\n",
       "2020-02-04  12.96     6.45    10.04   10.19   9.04    9.99    10.44   0.004713\n",
       "2020-02-05  12.16     7.25    9.90    10.19   9.04    9.99    10.44   0.004713\n",
       "2020-02-06  9.655     7.63    9.93    10.19   9.04    9.99    10.44   0.006667\n",
       "2020-02-07  12.6      7.08    9.94    10.19   9.04    9.99    10.44   0.003337\n",
       "2020-02-08  NaN       7.33    9.93    10.21   9.04    9.99    10.44   0.003498\n",
       "2020-02-09  11        6.90    10.06   10.21   8.88    9.99    10.44   0.004469\n",
       "2020-02-10  NaN       6.44    10.72   10.19   8.13    9.99    10.44   0.004199\n",
       "2020-02-11  12.99     6.29    10.72   10.19   8.05    9.99    10.44   0.004199\n",
       "2020-02-12  11        6.44    10.73   10.19   8.05    9.99    10.44   0.003333\n",
       "2020-02-13  NaN       6.41    10.71   10.19   8.05    9.99    10.44   0.003811\n",
       "2020-02-14  13.95     6.07    10.73   10.19   8.05    9.99    10.44   0.003639\n",
       "2020-02-15  13.35     5.26    10.72   10.19   8.05    9.99    10.44   0.002473\n",
       "2020-02-16  12.0333   5.20    10.74   10.19   8.05    9.99    10.44   0.002666\n",
       "2020-02-17  14.11     5.18    11.01   10.19   8.05    9.99    10.44   0.004693\n",
       "2020-02-18  NaN       5.16    11.05   10.19   8.05    9.99    10.44   0.008000\n",
       "2020-02-19  14.99     5.07    11.13   10.19   8.05    9.99    10.44   0.003556\n",
       "2020-02-20  NaN       4.94    11.42   10.19   8.05    9.99    10.44   0.003334\n",
       "2020-02-21  NaN       4.37    11.42   10.19   8.05    9.99    10.40   0.003334\n",
       "2020-02-22  12.1167   4.23    11.42   10.15   8.05    9.99    9.44    0.003810\n",
       "2020-02-23  NaN       4.27    11.35   10.11   8.05    9.99    9.44    0.001994\n",
       "2020-02-24  12        4.35    11.04   10.11   8.05    9.99    9.44    0.002899\n",
       "2020-02-25  10.5      4.31    11.05   10.11   8.05    9.99    9.44    0.002668\n",
       "2020-02-26  NaN       4.75    11.05   10.11   8.05    9.99    9.44    0.002668\n",
       "2020-02-27  NaN       4.87    11.05   10.11   8.05    9.99    9.44    0.004241\n",
       "2020-02-28  NaN       4.99    11.02   10.04   8.05    9.99    9.44    0.001975\n",
       "2020-02-29  11.5      5.39    10.92   10.03   8.05    9.99    9.44    0.002667\n",
       "2020-03-01  11.99     5.20    10.71   10.03   8.05    9.99    9.44    0.002435\n",
       "2020-03-02  12        4.30    10.68   10.03   8.05    9.99    9.44    0.002994\n",
       "2020-03-03  NaN       4.88    9.96    9.94    8.05    9.99    9.44    0.002994\n",
       "2020-03-04  10.5475   4.51    9.84    9.72    8.05    9.99    9.44    0.000494\n",
       "2020-03-05  17.19     4.34    9.79    9.72    8.05    9.99    9.44    0.004127\n",
       "2020-03-06  10.0125   4.17    9.79    9.70    8.05    9.99    9.44    0.002319\n",
       "2020-03-07  11.9      4.50    9.79    9.64    8.05    9.99    9.44    0.001212\n",
       "2020-03-08  11.49     4.92    9.79    9.64    8.05    9.99    9.44    0.002919\n",
       "2020-03-09  9.615     4.61    9.68    9.60    6.67    9.99    8.44    0.005786\n",
       "2020-03-10  13        4.76    9.66    9.49    5.39    9.99    7.72    0.002133\n",
       "2020-03-11  10.615    5.71    9.66    9.49    5.39    9.99    7.72    0.007312\n",
       "2020-03-12  NaN       6.37    9.54    9.47    5.39    9.99    7.29    0.005458\n",
       "2020-03-13  8.285     6.47    9.09    8.96    5.69    9.49    6.99    0.006126\n",
       "2020-03-14  8.33      6.67    8.64    8.45    5.99    8.95    6.99    0.008757\n",
       "2020-03-15  9.19      6.46    8.29    8.35    5.99    8.99    6.99    0.003810\n",
       "2020-03-16  6.5       5.75    7.91    8.29    5.99    8.99    6.99    0.002581\n",
       "2020-03-17  8.4       5.43    7.89    8.56    5.99    8.99    6.99    0.003200\n",
       "2020-03-18  NaN       5.06    7.89    8.56    5.99    8.99    6.99    0.002020\n",
       "2020-03-19  NaN       5.12    7.89    8.47    5.99    8.99    6.99    0.003165\n",
       "2020-03-20  6.7       4.50    7.89    8.09    5.99    8.99    6.53    0.001778\n",
       "2020-03-21  9.72      4.53    7.89    8.01    5.99    8.99    6.08    0.002083\n",
       "2020-03-22  7.95      4.28    7.79    6.31    5.99    8.99    5.53    0.001667\n",
       "2020-03-23  7.28      4.22    7.87    4.99    5.99    8.99    5.53    0.002754\n",
       "2020-03-24  5.15      3.96    7.97    4.77    5.99    8.99    5.68    0.002754\n",
       "2020-03-25  NaN       3.93    7.67    4.64    6.69    8.99    6.26    0.002754\n",
       "2020-03-26  NaN       3.54    7.63    4.30    6.72    8.99    5.56    0.002910\n",
       "2020-03-27  NaN       3.73    7.63    4.29    6.72    8.99    5.53    0.002910\n",
       "2020-03-28  NaN       4.51    7.60    4.82    6.72    8.99    5.53    0.002597\n",
       "2020-03-29  6.98      5.07    7.19    5.98    6.97    8.08    5.02    0.002597\n",
       "2020-03-30  7.3725    5.32    7.06    5.99    6.99    7.99    5.41    0.002549\n",
       "2020-03-31  5.5       4.70    7.05    5.99    6.99    7.99    5.58    0.001441\n",
       "2020-04-01  7         4.63    7.05    5.99    6.99    8.45    6.67    0.001441\n",
       "2020-04-02  6.75      4.42    7.05    5.99    6.99    8.98    7.04    0.003972\n",
       "2020-04-03  6.86      4.52    7.05    5.99    6.99    8.98    7.04    0.003972\n",
       "2020-04-04  NaN       4.53    7.05    5.99    6.99    8.98    7.04    0.003158\n",
       "2020-04-05  6.7025    4.29    6.87    5.99    6.99    8.98    7.04    0.003158\n",
       "2020-04-06  7.74333   4.79    6.60    5.99    6.99    8.98    7.04    0.004267\n",
       "2020-04-07  NaN       5.49    6.61    5.99    6.99    8.98    7.04    0.003619\n",
       "2020-04-08  6         4.85    6.34    5.99    6.99    8.98    7.04    0.001481\n",
       "2020-04-09  9.226     4.77    6.32    5.99    6.99    8.98    7.04    0.003200\n",
       "2020-04-10  6.75      4.91    6.33    5.99    6.99    8.83    7.04    0.000556\n",
       "2020-04-11  6.25      4.53    6.16    5.99    6.78    8.25    7.04    0.002667\n",
       "2020-04-12  5.99      3.85    5.94    5.99    6.26    8.25    7.04    0.002667\n",
       "2020-04-13  NaN       3.68    5.75    5.99    6.26    8.25    7.04    0.004444\n",
       "2020-04-14  NaN       3.66    5.77    5.99    6.26    8.25    7.04    0.004444\n",
       "2020-04-15  6.745     3.60    5.81    5.99    6.26    8.25    7.04    0.004444\n",
       "2020-04-16  6.8       3.75    5.81    5.99    6.26    8.25    7.04    0.002370\n",
       "2020-04-17  5.67      3.80    5.82    5.99    6.26    8.25    7.04    0.001767\n",
       "2020-04-18  7.75      3.92    5.85    5.99    6.26    8.25    7.04    0.001767\n",
       "2020-04-19  8.225     3.83    5.64    5.99    6.26    8.25    7.04    0.001767\n",
       "2020-04-20  7.005     3.83    5.53    5.99    6.26    8.25    7.04    0.004063\n",
       "2020-04-21  7         3.67    5.56    5.99    6.26    7.89    7.04    0.004678\n",
       "2020-04-22  6.99      3.88    5.58    6.22    6.26    6.79    7.04    0.002388\n",
       "2020-04-23  8.75      3.90    5.71    6.28    6.26    6.79    7.04    0.010000\n",
       "2020-04-24  6.695     4.02    5.86    6.28    6.26    6.19    6.85    0.004425\n",
       "2020-04-25  6.225     4.36    5.99    6.51    6.26    5.99    6.79    0.006202\n",
       "2020-04-26  6.99      4.61    6.33    6.57    6.26    5.99    6.79    0.003917\n",
       "2020-04-27  6.262     4.70    5.80    6.57    6.26    6.39    6.79    0.007182\n",
       "2020-04-28  8.1       4.64    5.73    6.57    6.26    6.72    6.79    0.009877\n",
       "2020-04-29  5.99      4.73    5.83    6.57    6.26    6.72    6.79    0.002853\n",
       "2020-04-30  6.39      4.87    5.89    6.57    6.26    6.72    6.79    0.009864\n",
       "2020-05-01  6.83083   4.82    5.84    6.57    6.26    6.72    6.79    0.005427\n",
       "2020-05-02  9.135     4.84    6.13    6.57    6.26    6.72    6.79    0.006250\n",
       "2020-05-03  8.515     5.18    6.28    6.57    6.26    6.72    6.79    0.004753\n",
       "2020-05-04  5.605     5.40    6.27    6.57    6.26    6.54    6.79    0.006190\n",
       "2020-05-05  7.74      5.71    6.21    6.57    6.26    5.99    6.79    0.002332\n",
       "2020-05-06  7.615     5.37    5.97    6.35    6.26    5.99    6.79    0.007238\n",
       "2020-05-07  7         5.08    6.08    6.03    6.26    5.99    6.79    0.008254\n",
       "2020-05-08  7.83      5.94    6.36    5.99    6.26    5.99    6.79    0.004853\n",
       "2020-05-09  NaN       6.28    6.69    6.11    6.35    5.99    6.79    0.006299\n",
       "2020-05-10  7.095     5.92    6.92    6.32    6.99    5.99    6.79    0.008847\n",
       "2020-05-11  NaN       4.30    6.96    6.28    6.99    5.99    6.79    0.010591\n",
       "2020-05-12  8.29      4.41    7.13    6.28    6.84    5.99    6.79    0.011673\n",
       "2020-05-13  8.37      4.87    7.25    6.28    6.26    5.99    6.79    0.009206\n",
       "2020-05-14  11.99     5.35    7.23    6.28    6.17    5.99    6.79    0.015054\n",
       "2020-05-15  9.975     5.69    7.14    6.28    5.53    5.99    6.79    0.009061\n",
       "2020-05-16  6.48333   5.95    7.15    6.28    5.53    5.99    6.79    0.010448\n",
       "2020-05-17  7.33      5.55    7.20    6.28    5.53    5.99    6.79    0.013600\n",
       "2020-05-18  5.87      5.40    7.31    6.28    5.15    5.99    6.79    0.011435\n",
       "2020-05-19  6.8       5.52    7.31    6.28    5.37    5.99    6.79    0.006780\n",
       "2020-05-20  9.97      5.60    7.08    6.28    5.73    5.99    6.79    0.006604\n",
       "2020-05-21  5.75      5.39    6.77    6.28    5.73    5.99    6.79    0.011828\n",
       "2020-05-22  9.655     4.73    6.77    6.28    5.36    5.99    6.79    0.011765\n",
       "2020-05-23  5.97      5.56    7.67    6.28    5.21    5.99    6.79    0.009612\n",
       "2020-05-24  6.495     4.99    7.68    6.28    5.21    5.99    6.79    0.011019\n",
       "2020-05-25  7.5       5.17    7.67    6.28    5.21    5.99    6.79    0.004350\n",
       "2020-05-26  NaN       5.32    7.70    6.28    5.21    5.99    6.79    0.009394\n",
       "2020-05-27  5.99      4.86    7.68    6.28    5.21    5.99    6.79    0.009394\n",
       "2020-05-28  7         4.47    7.67    6.28    5.21    5.99    6.79    0.008832\n",
       "2020-05-29  5.915     4.47    7.67    6.28    5.21    5.99    6.79    0.008594\n",
       "2020-05-30  NaN       4.25    7.67    6.28    5.21    5.99    6.79    0.013740\n",
       "2020-05-31  NaN       3.87    7.67    6.28    5.21    5.99    6.79    0.013740\n",
       "2020-06-01  7.215     3.99    7.66    6.28    5.21    5.99    6.79    0.011273\n",
       "2020-06-02  6.89      3.84    7.44    6.28    5.51    5.99    6.79    0.011273\n",
       "2020-06-03  8.21      3.82    7.44    6.28    5.87    5.99    6.79    0.009249\n",
       "2020-06-04  7.005     4.24    7.50    6.28    5.87    5.99    6.79    0.004619\n",
       "2020-06-05  6.25      4.55    7.50    6.28    5.87    5.99    6.79    0.006978\n",
       "2020-06-06  6.99      4.39    7.50    6.28    5.87    5.99    6.79    0.009127\n",
       "2020-06-07  6.93333   4.43    7.47    6.28    5.87    5.99    6.79    0.006048\n",
       "2020-06-08  7.22      4.27    7.45    6.28    5.87    5.99    6.79    0.003941\n",
       "2020-06-09  6.99      4.09    7.45    6.28    5.87    5.99    6.79    0.002575\n",
       "2020-06-10  6.3075    4.13    7.40    6.29    5.87    5.99    6.79    0.002575\n",
       "2020-06-11  6.245     4.59    7.40    6.57    5.87    5.99    6.79    0.008520\n",
       "2020-06-12  NaN       5.08    7.40    6.49    5.87    5.99    6.79    0.008520\n",
       "2020-06-13  NaN       4.49    7.40    6.28    5.87    5.99    6.79    0.004878\n",
       "2020-06-14  4.24      4.66    7.43    6.28    5.87    5.99    6.79    0.003689\n",
       "2020-06-15  7.745     4.72    7.43    6.28    5.87    5.99    6.79    0.008790\n",
       "2020-06-16  6.37      4.84    7.29    6.28    5.87    5.99    6.79    0.005594\n",
       "2020-06-17  NaN       4.69    7.17    6.28    5.87    5.99    6.79    0.005755\n",
       "2020-06-18  NaN       4.08    7.18    6.28    5.87    5.99    6.79    0.012603\n",
       "2020-06-19  1.75      3.78    7.17    6.28    5.87    5.99    6.79    0.012603\n",
       "2020-06-20  2.99      3.82    7.15    6.28    5.87    5.99    6.79    0.005242\n",
       "2020-06-21  4.99      3.15    7.08    6.28    5.87    5.99    6.79    0.004918\n",
       "2020-06-22  NaN       3.19    7.07    6.28    5.87    5.99    6.79    0.010958\n",
       "2020-06-23  6.74      3.37    7.07    6.28    5.87    5.99    6.79    0.010958\n",
       "2020-06-24  NaN       3.40    7.07    6.28    5.87    5.99    6.79    0.003896\n",
       "2020-06-25  4.69      3.59    7.05    6.28    5.87    5.99    6.79    0.004317\n",
       "2020-06-26  5.5       3.62    7.05    6.28    5.87    5.99    6.79    0.004317\n",
       "2020-06-27  5.99      3.72    7.07    6.28    5.40    5.99    6.79    0.002846\n",
       "2020-06-28  3.75      3.67    7.07    6.28    5.28    5.99    6.79    0.002846\n",
       "2020-06-29  4.995     3.58    7.06    6.28    5.28    5.99    6.79    0.008284\n",
       "2020-06-30  NaN       3.71    7.06    6.28    5.28    5.99    6.79    0.008284\n",
       "2020-07-01  5         3.35    6.92    6.28    5.28    5.99    6.79    0.009467\n",
       "2020-07-02  NaN       3.23    6.64    6.28    5.28    5.99    6.79    0.004380\n",
       "2020-07-03  6.24      3.21    6.50    6.28    5.28    5.99    6.79    0.004380\n",
       "2020-07-04  5.97      3.20    6.50    6.28    5.28    5.99    6.79    0.004380\n",
       "2020-07-05  NaN       3.45    6.50    6.28    5.28    5.99    6.79    0.004380\n",
       "2020-07-06  5.1225    3.25    6.50    6.28    5.28    5.99    6.79    0.005461\n",
       "2020-07-07  NaN       2.92    6.50    6.28    5.28    5.99    6.79    0.005461\n",
       "2020-07-08  2         2.87    6.41    6.28    5.28    5.99    6.79    0.005461\n",
       "2020-07-09  NaN       2.81    6.40    6.28    5.28    5.99    6.79    0.008186\n",
       "2020-07-10  3.7875    2.66    6.13    6.28    5.28    5.99    6.79    0.008186\n",
       "2020-07-11  6.385     2.52    5.82    6.28    5.28    5.99    6.79    0.004065\n",
       "2020-07-12  NaN       2.35    6.09    6.28    5.28    5.99    6.79    0.001639\n",
       "2020-07-13  NaN       3.01    6.11    6.28    5.28    5.99    6.79    0.002717\n",
       "2020-07-14  5.12      3.48    6.11    6.28    5.28    5.99    6.79    0.002717\n",
       "2020-07-15  5.49      3.37    6.11    6.28    5.28    5.99    6.79    0.002717\n",
       "2020-07-16  8.465     3.61    6.11    6.12    5.28    5.99    6.79    0.007833\n",
       "2020-07-17  6.28      3.46    5.93    5.59    5.28    5.99    6.79    0.007833\n",
       "2020-07-18  NaN       3.31    5.82    5.20    5.28    5.99    6.79    0.002500\n",
       "2020-07-19  7.87      3.13    5.82    5.18    5.28    5.99    6.79    0.002500\n",
       "2020-07-20  5.87      3.08    5.76    5.00    5.28    5.99    6.79    0.004165\n",
       "2020-07-21  NaN       2.84    5.47    4.49    5.17    5.99    6.79    0.004165\n",
       "2020-07-22  NaN       2.78    5.14    4.28    4.82    5.99    6.79    0.002956\n",
       "2020-07-23  5.95      2.68    5.34    4.28    4.47    5.99    6.79    0.011145\n",
       "2020-07-24  7.25      2.84    5.28    4.24    3.99    5.99    6.79    0.000952\n",
       "2020-07-25  6.775     2.97    5.06    4.17    3.99    5.99    6.79    0.000417\n",
       "2020-07-26  5.5       3.27    5.42    4.06    3.99    5.99    6.79    0.002016\n",
       "2020-07-27  5.485     3.37    5.79    4.07    3.99    5.99    6.79    0.003354\n",
       "2020-07-28  NaN       3.21    5.60    4.08    3.99    5.77    6.79    0.003354\n",
       "2020-07-29  NaN       3.34    5.54    4.10    3.99    5.33    6.79    0.003354\n",
       "2020-07-30  6.5       3.29    5.54    4.10    3.99    5.33    6.79    0.003754\n",
       "2020-07-31  6.96      3.26    5.54    4.10    4.43    5.33    6.79    0.003754\n",
       "2020-08-01  NaN       2.98    5.40    4.02    4.45    5.06    6.79    0.002049\n",
       "2020-08-02  6.39      2.78    5.30    3.97    4.45    4.87    6.79    0.003226\n",
       "2020-08-03  NaN       2.81    5.32    3.89    4.45    4.87    6.79    0.000692\n",
       "2020-08-04  5.73667   4.53    5.78    4.76    4.45    4.87    6.79    0.000692\n",
       "2020-08-05  NaN       4.75    6.18    5.09    4.45    4.87    6.79    0.012587\n",
       "2020-08-06  5.48      4.82    6.18    5.13    4.45    4.87    6.79    0.007494\n",
       "2020-08-07  5.49      4.98    6.22    5.37    4.45    4.87    6.79    0.007494\n",
       "2020-08-08  5.33167   4.86    6.78    5.66    4.45    4.87    6.79    0.005741\n",
       "2020-08-09  NaN       3.40    6.72    5.76    4.64    4.87    6.79    0.004002\n",
       "2020-08-10  NaN       3.56    6.68    5.83    4.91    4.87    6.79    0.007080\n",
       "2020-08-11  NaN       3.48    6.25    5.83    4.91    4.87    6.79    0.008333\n",
       "2020-08-12  5.99      3.77    6.02    5.83    4.91    4.87    6.79    0.008333\n",
       "2020-08-13  4.75      3.70    6.02    5.92    4.91    4.87    6.79    0.005949\n",
       "2020-08-14  9.74      3.39    6.02    5.92    4.91    4.87    6.79    0.005949\n",
       "2020-08-15  NaN       3.67    6.02    5.92    4.91    4.87    6.79    0.010937\n",
       "2020-08-16  2.785     4.17    6.41    6.06    4.91    4.87    6.79    0.010085\n",
       "2020-08-17  NaN       4.42    6.49    6.12    4.91    4.87    6.79    0.009156\n",
       "2020-08-18  5.965     3.96    6.49    6.12    4.91    4.87    6.79    0.009889\n",
       "2020-08-19  NaN       3.77    6.48    6.12    4.91    4.87    6.79    0.016343\n",
       "2020-08-20  NaN       4.06    6.46    6.05    4.91    4.87    6.79    0.007507\n",
       "2020-08-21  NaN       4.27    6.46    6.04    4.91    4.87    6.79    0.003089\n",
       "2020-08-22  6.04      4.27    6.46    6.04    4.91    4.87    6.79    0.003089\n",
       "2020-08-23  7.325     4.30    6.57    6.04    4.91    4.87    6.79    0.003089\n",
       "2020-08-24  NaN       4.07    6.91    6.04    4.91    4.87    6.79    0.006494\n",
       "2020-08-25  7.5625    4.25    6.36    6.04    4.87    4.87    6.79    0.005755\n",
       "2020-08-26  6         3.90    6.36    6.04    4.45    4.87    6.79    0.005755\n",
       "2020-08-27  5.1975    3.91    6.40    5.74    4.45    4.87    6.79    0.012325\n",
       "2020-08-28  NaN       4.08    6.43    5.79    4.45    4.87    6.79    0.012560\n",
       "2020-08-29  6         4.06    6.46    5.79    4.45    4.87    6.79    0.009274\n",
       "2020-08-30  6.58      3.97    6.42    5.89    4.45    4.87    6.79    0.010317\n",
       "2020-08-31  NaN       3.79    6.48    5.90    4.45    4.87    6.79    0.009076\n",
       "2020-09-01  6.99      3.24    6.36    5.90    4.45    4.87    6.79    0.011102\n",
       "2020-09-02  5.99      3.14    6.37    5.90    4.20    4.87    6.79    0.010292\n",
       "2020-09-03  6.54      2.81    6.38    5.90    3.99    4.87    6.79    0.008475\n",
       "2020-09-04  6.25      2.88    6.38    5.90    3.99    4.87    6.79    0.008475\n",
       "2020-09-05  6.025     3.08    6.38    5.90    3.99    4.87    6.79    0.011019\n",
       "2020-09-06  4.275     3.32    6.38    5.90    3.99    4.87    6.79    0.004920\n",
       "2020-09-07  8         3.10    6.20    5.90    3.99    4.87    6.79    0.007251\n",
       "2020-09-08  NaN       2.95    6.20    5.90    3.99    4.87    6.79    0.005755\n",
       "2020-09-09  8         2.75    6.17    5.59    3.99    4.87    6.79    0.005755\n",
       "2020-09-10  6.875     2.86    6.13    5.44    3.99    4.87    6.79    0.005355\n",
       "2020-09-11  6.245     3.09    6.13    5.44    3.99    4.87    6.79    0.005355\n",
       "2020-09-12  NaN       3.31    6.10    5.39    3.99    4.87    6.79    0.010813\n",
       "2020-09-13  NaN       3.18    5.95    5.36    3.99    4.87    6.79    0.011024\n",
       "2020-09-14  10        3.16    5.86    5.36    3.99    4.87    6.79    0.016552\n",
       "2020-09-15  NaN       3.22    5.35    5.36    3.99    4.87    6.79    0.019868\n",
       "2020-09-16  5.975     3.19    5.37    5.37    3.99    4.87    6.79    0.019868\n",
       "2020-09-17  5.75      2.84    5.45    5.29    3.99    4.87    6.79    0.015625\n",
       "2020-09-18  5.5       2.97    5.52    5.34    3.99    4.87    6.79    0.005495\n",
       "2020-09-19  NaN       2.93    5.44    5.35    3.99    4.87    6.79    0.004689\n",
       "2020-09-20  5.915     2.68    5.40    5.35    3.99    4.87    6.79    0.006855\n",
       "2020-09-21  7.91      2.70    5.39    5.35    3.99    4.87    6.79    0.006433\n",
       "2020-09-22  6.14      2.68    5.39    5.35    3.99    4.87    6.79    0.006433\n",
       "2020-09-23  NaN       2.60    5.43    5.36    3.99    4.87    6.79    0.006433\n",
       "2020-09-24  NaN       2.56    5.43    5.36    3.99    4.87    6.79    0.006433\n",
       "2020-09-25  NaN       2.84    5.43    5.36    3.99    4.87    6.79    0.006433\n",
       "2020-09-26  5.99      2.98    5.46    5.43    3.99    4.87    6.79    0.006433\n",
       "2020-09-27  NaN       3.11    5.49    4.95    3.99    4.87    6.79    0.006433\n",
       "2020-09-28  2.925     2.87    5.50    4.93    3.99    4.87    6.79    0.006433\n",
       "2020-09-29  NaN       2.80    5.49    4.93    3.99    4.99    6.79    0.006433\n",
       "2020-09-30  4.99      2.80    5.49    4.93    3.99    4.99    6.79    0.006433\n",
       "2020-10-01  5.75      2.80    5.49    4.93    3.99    4.99    6.79    0.006433"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overalldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.xlarge',\n",
    "    base_job_name='deepar-mtgml',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-03 17:46:25 Starting - Starting the training job...\n",
      "2020-10-03 17:46:27 Starting - Launching requested ML instances......\n",
      "2020-10-03 17:47:28 Starting - Preparing the instances for training...\n",
      "2020-10-03 17:48:06 Downloading - Downloading input data...\n",
      "2020-10-03 17:48:54 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:55 INFO 139787867731776] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:55 INFO 139787867731776] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'5E-4', u'prediction_length': u'1', u'epochs': u'400', u'time_freq': u'D', u'context_length': u'7', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'400', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'1', u'time_freq': u'D', u'context_length': u'7', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=7 from dataset.\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] Training set statistics:\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] Real time series\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] number of time series: 1\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] number of observations: 343\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] mean target length: 343\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] min/mean/max target: 0.0/6.56027497495/17.1899986267\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] mean abs(target): 6.56027497495\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] contains missing values: yes (24.8%)\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] Small number of time series. Doing 640 passes over dataset with prob 1.0 per epoch.\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] No test channel found not running evaluations\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] nvidia-smi took: 0.0251579284668 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 20.35999298095703, \"sum\": 20.35999298095703, \"min\": 20.35999298095703}}, \"EndTime\": 1601747336.168691, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747336.147411}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 58.53700637817383, \"sum\": 58.53700637817383, \"min\": 58.53700637817383}}, \"EndTime\": 1601747336.206074, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747336.168766}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] Epoch[0] Batch[0] avg_epoch_loss=3.023760\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=3.02375984192\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] Epoch[0] Batch[5] avg_epoch_loss=2.855639\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=2.85563949744\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] Epoch[0] Batch [5]#011Speed: 4170.64 samples/sec#011loss=2.855639\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 400, \"sum\": 400.0, \"min\": 400}, \"update.time\": {\"count\": 1, \"max\": 572.3719596862793, \"sum\": 572.3719596862793, \"min\": 572.3719596862793}}, \"EndTime\": 1601747336.778594, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747336.206132}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1067.28981774 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=0, train loss <loss>=2.78517975807\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:56 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_bbd74000-fb24-4881-870c-3e5049a1ccc6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.595134735107422, \"sum\": 6.595134735107422, \"min\": 6.595134735107422}}, \"EndTime\": 1601747336.785766, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747336.778665}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:57 INFO 139787867731776] Epoch[1] Batch[0] avg_epoch_loss=2.488167\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=2.48816728592\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:57 INFO 139787867731776] Epoch[1] Batch[5] avg_epoch_loss=2.399731\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=2.39973111947\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:57 INFO 139787867731776] Epoch[1] Batch [5]#011Speed: 3815.06 samples/sec#011loss=2.399731\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:57 INFO 139787867731776] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 527.2629261016846, \"sum\": 527.2629261016846, \"min\": 527.2629261016846}}, \"EndTime\": 1601747337.313139, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747336.785821}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:57 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1181.36108023 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:57 INFO 139787867731776] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=1, train loss <loss>=2.33609781265\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:57 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:57 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_d8adab19-4628-4e0d-85b5-7fce48ce2c5d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.124019622802734, \"sum\": 6.124019622802734, \"min\": 6.124019622802734}}, \"EndTime\": 1601747337.319809, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747337.313202}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:57 INFO 139787867731776] Epoch[2] Batch[0] avg_epoch_loss=2.225946\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=2.22594571114\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:57 INFO 139787867731776] Epoch[2] Batch[5] avg_epoch_loss=2.117540\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=2.11754000187\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:57 INFO 139787867731776] Epoch[2] Batch [5]#011Speed: 3064.99 samples/sec#011loss=2.117540\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:58 INFO 139787867731776] processed a total of 599 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 705.8508396148682, \"sum\": 705.8508396148682, \"min\": 705.8508396148682}}, \"EndTime\": 1601747338.025773, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747337.319867}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:58 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=848.49941337 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:58 INFO 139787867731776] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=2, train loss <loss>=2.04655691385\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:58 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:58 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_3af9dc31-faf9-40f9-8099-c870f126999c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.519079208374023, \"sum\": 6.519079208374023, \"min\": 6.519079208374023}}, \"EndTime\": 1601747338.032935, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747338.02584}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:58 INFO 139787867731776] Epoch[3] Batch[0] avg_epoch_loss=1.997963\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=1.99796307087\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:58 INFO 139787867731776] Epoch[3] Batch[5] avg_epoch_loss=1.907021\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=1.90702060858\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:58 INFO 139787867731776] Epoch[3] Batch [5]#011Speed: 3640.66 samples/sec#011loss=1.907021\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:58 INFO 139787867731776] Epoch[3] Batch[10] avg_epoch_loss=1.818199\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=1.71161282063\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:58 INFO 139787867731776] Epoch[3] Batch [10]#011Speed: 3812.97 samples/sec#011loss=1.711613\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:58 INFO 139787867731776] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 666.6839122772217, \"sum\": 666.6839122772217, \"min\": 666.6839122772217}}, \"EndTime\": 1601747338.699755, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747338.033011}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:58 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=962.846653565 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:58 INFO 139787867731776] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=3, train loss <loss>=1.81819888678\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:58 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:58 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_79783e43-4463-4bc4-90dc-b3935f242ca5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.17108154296875, \"sum\": 8.17108154296875, \"min\": 8.17108154296875}}, \"EndTime\": 1601747338.708323, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747338.699813}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] Epoch[4] Batch[0] avg_epoch_loss=1.640058\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=1.64005804062\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] Epoch[4] Batch[5] avg_epoch_loss=1.632891\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=1.63289074103\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] Epoch[4] Batch [5]#011Speed: 4239.31 samples/sec#011loss=1.632891\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 566.5168762207031, \"sum\": 566.5168762207031, \"min\": 566.5168762207031}}, \"EndTime\": 1601747339.274953, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747338.708381}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1106.56382723 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=4, train loss <loss>=1.58769125938\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_ebee8469-0fd4-46a9-b506-70d7c5bf3580-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.251096725463867, \"sum\": 6.251096725463867, \"min\": 6.251096725463867}}, \"EndTime\": 1601747339.281756, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747339.275023}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] Epoch[5] Batch[0] avg_epoch_loss=1.487640\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=1.48763990402\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] Epoch[5] Batch[5] avg_epoch_loss=1.448442\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=1.44844249884\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] Epoch[5] Batch [5]#011Speed: 3849.25 samples/sec#011loss=1.448442\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] Epoch[5] Batch[10] avg_epoch_loss=1.431666\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=1.41153347492\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] Epoch[5] Batch [10]#011Speed: 4214.63 samples/sec#011loss=1.411533\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 559.7190856933594, \"sum\": 559.7190856933594, \"min\": 559.7190856933594}}, \"EndTime\": 1601747339.841592, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747339.281817}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1177.17274422 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=5, train loss <loss>=1.43166566979\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:48:59 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_ad107dd5-b7b2-45dc-bc2a-9d9cda9b1e8a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.827043533325195, \"sum\": 7.827043533325195, \"min\": 7.827043533325195}}, \"EndTime\": 1601747339.849823, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747339.841658}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] Epoch[6] Batch[0] avg_epoch_loss=1.384678\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=1.38467824459\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] Epoch[6] Batch[5] avg_epoch_loss=1.374199\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=1.37419895331\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] Epoch[6] Batch [5]#011Speed: 4296.92 samples/sec#011loss=1.374199\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] processed a total of 591 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 543.7240600585938, \"sum\": 543.7240600585938, \"min\": 543.7240600585938}}, \"EndTime\": 1601747340.393875, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747339.8501}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1086.76065022 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=6, train loss <loss>=1.39698623419\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_eb5228b9-a871-4e14-ad2d-64e428af5b5f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.576942443847656, \"sum\": 7.576942443847656, \"min\": 7.576942443847656}}, \"EndTime\": 1601747340.401996, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747340.393941}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] Epoch[7] Batch[0] avg_epoch_loss=1.311082\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=1.31108248234\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] Epoch[7] Batch[5] avg_epoch_loss=1.341942\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=1.3419415156\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] Epoch[7] Batch [5]#011Speed: 4105.62 samples/sec#011loss=1.341942\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 541.496992111206, \"sum\": 541.496992111206, \"min\": 541.496992111206}}, \"EndTime\": 1601747340.943602, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747340.402052}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1166.9365048 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=7, train loss <loss>=1.34119218588\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:00 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_a84f2c94-e705-4b58-b503-9fa0c1d250a0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.597042083740234, \"sum\": 6.597042083740234, \"min\": 6.597042083740234}}, \"EndTime\": 1601747340.950637, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747340.943667}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:01 INFO 139787867731776] Epoch[8] Batch[0] avg_epoch_loss=1.326342\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=1.32634246349\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:01 INFO 139787867731776] Epoch[8] Batch[5] avg_epoch_loss=1.321166\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=1.3211662968\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:01 INFO 139787867731776] Epoch[8] Batch [5]#011Speed: 3846.01 samples/sec#011loss=1.321166\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:01 INFO 139787867731776] Epoch[8] Batch[10] avg_epoch_loss=1.320124\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=1.31887311935\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:01 INFO 139787867731776] Epoch[8] Batch [10]#011Speed: 3784.61 samples/sec#011loss=1.318873\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:01 INFO 139787867731776] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 549.091100692749, \"sum\": 549.091100692749, \"min\": 549.091100692749}}, \"EndTime\": 1601747341.499834, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747340.950689}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:01 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1183.5739946 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:01 INFO 139787867731776] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=8, train loss <loss>=1.32012394342\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:01 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:01 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_a7e639f1-499b-4d35-8668-c05f86c5c290-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.592035293579102, \"sum\": 6.592035293579102, \"min\": 6.592035293579102}}, \"EndTime\": 1601747341.506907, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747341.499898}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:01 INFO 139787867731776] Epoch[9] Batch[0] avg_epoch_loss=1.330183\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=1.33018302917\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:01 INFO 139787867731776] Epoch[9] Batch[5] avg_epoch_loss=1.323369\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=1.32336932421\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:01 INFO 139787867731776] Epoch[9] Batch [5]#011Speed: 3933.47 samples/sec#011loss=1.323369\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] Epoch[9] Batch[10] avg_epoch_loss=1.338335\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=1.35629310608\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] Epoch[9] Batch [10]#011Speed: 3501.94 samples/sec#011loss=1.356293\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 570.1849460601807, \"sum\": 570.1849460601807, \"min\": 570.1849460601807}}, \"EndTime\": 1601747342.077203, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747341.506964}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1201.18778484 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=9, train loss <loss>=1.3383346796\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] Epoch[10] Batch[0] avg_epoch_loss=1.174271\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=1.17427146435\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] Epoch[10] Batch[5] avg_epoch_loss=1.278203\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=1.27820301056\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] Epoch[10] Batch [5]#011Speed: 4070.99 samples/sec#011loss=1.278203\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 522.1350193023682, \"sum\": 522.1350193023682, \"min\": 522.1350193023682}}, \"EndTime\": 1601747342.59977, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747342.077257}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1208.25415921 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=10, train loss <loss>=1.27772285938\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_a92c4d60-d8f8-46fc-869a-1147341c295e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.376909255981445, \"sum\": 7.376909255981445, \"min\": 7.376909255981445}}, \"EndTime\": 1601747342.607657, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747342.599842}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] Epoch[11] Batch[0] avg_epoch_loss=1.215525\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=1.21552455425\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] Epoch[11] Batch[5] avg_epoch_loss=1.281526\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=1.28152618806\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] Epoch[11] Batch [5]#011Speed: 4229.83 samples/sec#011loss=1.281526\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] Epoch[11] Batch[10] avg_epoch_loss=1.289553\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=1.29918427467\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] Epoch[11] Batch [10]#011Speed: 4267.87 samples/sec#011loss=1.299184\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 542.0210361480713, \"sum\": 542.0210361480713, \"min\": 542.0210361480713}}, \"EndTime\": 1601747343.149795, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747342.607718}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1195.31156193 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=11, train loss <loss>=1.28955259106\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] Epoch[12] Batch[0] avg_epoch_loss=1.221768\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=1.22176790237\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] Epoch[12] Batch[5] avg_epoch_loss=1.280408\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=1.28040832281\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] Epoch[12] Batch [5]#011Speed: 3660.87 samples/sec#011loss=1.280408\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 534.4069004058838, \"sum\": 534.4069004058838, \"min\": 534.4069004058838}}, \"EndTime\": 1601747343.684639, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747343.149862}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1165.4814606 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=12, train loss <loss>=1.25097568035\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:03 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_a7a90d19-1973-4a51-8d1b-4a2116dea683-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.9059600830078125, \"sum\": 7.9059600830078125, \"min\": 7.9059600830078125}}, \"EndTime\": 1601747343.693044, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747343.684743}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] Epoch[13] Batch[0] avg_epoch_loss=1.379013\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=1.37901306152\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] Epoch[13] Batch[5] avg_epoch_loss=1.311783\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=1.31178295612\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] Epoch[13] Batch [5]#011Speed: 4120.69 samples/sec#011loss=1.311783\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 533.7061882019043, \"sum\": 533.7061882019043, \"min\": 533.7061882019043}}, \"EndTime\": 1601747344.22687, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747343.693107}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1155.86747849 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=13, train loss <loss>=1.27509270906\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] Epoch[14] Batch[0] avg_epoch_loss=1.211818\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=1.2118178606\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] Epoch[14] Batch[5] avg_epoch_loss=1.270626\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=1.27062553167\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] Epoch[14] Batch [5]#011Speed: 2707.48 samples/sec#011loss=1.270626\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 558.0289363861084, \"sum\": 558.0289363861084, \"min\": 558.0289363861084}}, \"EndTime\": 1601747344.785369, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747344.226931}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1080.40125248 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=14, train loss <loss>=1.24501636028\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:04 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_39918b06-a3d2-41cc-9f26-7fc6d5d9fd9f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.848978042602539, \"sum\": 7.848978042602539, \"min\": 7.848978042602539}}, \"EndTime\": 1601747344.793673, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747344.785434}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] Epoch[15] Batch[0] avg_epoch_loss=1.293790\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=1.29379034042\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] Epoch[15] Batch[5] avg_epoch_loss=1.247377\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=1.24737735589\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] Epoch[15] Batch [5]#011Speed: 3644.77 samples/sec#011loss=1.247377\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 589.2689228057861, \"sum\": 589.2689228057861, \"min\": 589.2689228057861}}, \"EndTime\": 1601747345.383058, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747344.793731}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1068.95053658 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=15, train loss <loss>=1.23637923002\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_b6b1dd68-11a4-410a-8219-50d4423e54cb-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.10804557800293, \"sum\": 6.10804557800293, \"min\": 6.10804557800293}}, \"EndTime\": 1601747345.389689, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747345.38312}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] Epoch[16] Batch[0] avg_epoch_loss=1.220285\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=1.22028529644\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] Epoch[16] Batch[5] avg_epoch_loss=1.230818\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=1.23081775506\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] Epoch[16] Batch [5]#011Speed: 3985.89 samples/sec#011loss=1.230818\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 525.5210399627686, \"sum\": 525.5210399627686, \"min\": 525.5210399627686}}, \"EndTime\": 1601747345.915324, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747345.389745}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1209.99746169 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=16, train loss <loss>=1.22782226801\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:05 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_8e495183-19c0-451f-a14c-ff458f6033ee-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.10098648071289, \"sum\": 8.10098648071289, \"min\": 8.10098648071289}}, \"EndTime\": 1601747345.923875, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747345.915392}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:06 INFO 139787867731776] Epoch[17] Batch[0] avg_epoch_loss=1.222013\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=1.22201311588\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:06 INFO 139787867731776] Epoch[17] Batch[5] avg_epoch_loss=1.256950\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=1.2569501996\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:06 INFO 139787867731776] Epoch[17] Batch [5]#011Speed: 4328.29 samples/sec#011loss=1.256950\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:06 INFO 139787867731776] processed a total of 593 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 542.8788661956787, \"sum\": 542.8788661956787, \"min\": 542.8788661956787}}, \"EndTime\": 1601747346.466867, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747345.923935}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:06 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1092.14340533 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:06 INFO 139787867731776] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=17, train loss <loss>=1.35466698408\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:06 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:06 INFO 139787867731776] Epoch[18] Batch[0] avg_epoch_loss=1.246565\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=1.24656498432\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:06 INFO 139787867731776] Epoch[18] Batch[5] avg_epoch_loss=1.253245\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=1.25324519475\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:06 INFO 139787867731776] Epoch[18] Batch [5]#011Speed: 3960.30 samples/sec#011loss=1.253245\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] Epoch[18] Batch[10] avg_epoch_loss=1.241725\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=1.22790102959\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] Epoch[18] Batch [10]#011Speed: 4091.60 samples/sec#011loss=1.227901\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 544.4891452789307, \"sum\": 544.4891452789307, \"min\": 544.4891452789307}}, \"EndTime\": 1601747347.011771, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747346.466929}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1202.75284778 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=18, train loss <loss>=1.24172511968\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] Epoch[19] Batch[0] avg_epoch_loss=1.202448\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=1.20244824886\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] Epoch[19] Batch[5] avg_epoch_loss=1.213128\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=1.21312799056\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] Epoch[19] Batch [5]#011Speed: 4255.60 samples/sec#011loss=1.213128\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] Epoch[19] Batch[10] avg_epoch_loss=1.202810\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=1.19042732716\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] Epoch[19] Batch [10]#011Speed: 4233.97 samples/sec#011loss=1.190427\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 533.1380367279053, \"sum\": 533.1380367279053, \"min\": 533.1380367279053}}, \"EndTime\": 1601747347.545342, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747347.011833}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1286.49679839 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=19, train loss <loss>=1.2028095072\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_d6bd87fc-98d6-419a-9bfd-6a2102cb9d95-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.261110305786133, \"sum\": 6.261110305786133, \"min\": 6.261110305786133}}, \"EndTime\": 1601747347.552078, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747347.545402}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] Epoch[20] Batch[0] avg_epoch_loss=1.175224\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=1.17522370815\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] Epoch[20] Batch[5] avg_epoch_loss=1.164375\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=1.16437528531\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] Epoch[20] Batch [5]#011Speed: 4375.76 samples/sec#011loss=1.164375\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] Epoch[20] Batch[10] avg_epoch_loss=1.189023\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=1.21859927177\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] Epoch[20] Batch [10]#011Speed: 4176.67 samples/sec#011loss=1.218599\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 535.5508327484131, \"sum\": 535.5508327484131, \"min\": 535.5508327484131}}, \"EndTime\": 1601747348.087734, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747347.552133}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1267.63341557 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=20, train loss <loss>=1.18902255188\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_6c99eba9-fede-4c3e-a74d-1466ad5f2b17-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.557941436767578, \"sum\": 6.557941436767578, \"min\": 6.557941436767578}}, \"EndTime\": 1601747348.09477, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747348.087799}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] Epoch[21] Batch[0] avg_epoch_loss=1.161524\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=1.16152405739\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] Epoch[21] Batch[5] avg_epoch_loss=1.183692\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=1.18369237582\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] Epoch[21] Batch [5]#011Speed: 3613.25 samples/sec#011loss=1.183692\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 540.442943572998, \"sum\": 540.442943572998, \"min\": 540.442943572998}}, \"EndTime\": 1601747348.635322, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747348.094823}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1161.80120493 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=21, train loss <loss>=1.17082902193\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:08 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_2daa3478-c8be-4b65-83a6-8f2ede6cc8bd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.067037582397461, \"sum\": 6.067037582397461, \"min\": 6.067037582397461}}, \"EndTime\": 1601747348.641884, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747348.635387}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] Epoch[22] Batch[0] avg_epoch_loss=1.154099\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=1.15409910679\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] Epoch[22] Batch[5] avg_epoch_loss=1.189648\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=1.18964823087\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] Epoch[22] Batch [5]#011Speed: 4320.77 samples/sec#011loss=1.189648\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] Epoch[22] Batch[10] avg_epoch_loss=1.214840\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=1.24506926537\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] Epoch[22] Batch [10]#011Speed: 4268.89 samples/sec#011loss=1.245069\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 530.5559635162354, \"sum\": 530.5559635162354, \"min\": 530.5559635162354}}, \"EndTime\": 1601747349.172555, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747348.641945}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1209.83939367 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=22, train loss <loss>=1.21483961019\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] Epoch[23] Batch[0] avg_epoch_loss=1.181182\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=1.18118190765\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] Epoch[23] Batch[5] avg_epoch_loss=1.164921\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=1.16492074728\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] Epoch[23] Batch [5]#011Speed: 4106.20 samples/sec#011loss=1.164921\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] Epoch[23] Batch[10] avg_epoch_loss=1.154647\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=1.14231946468\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] Epoch[23] Batch [10]#011Speed: 2315.55 samples/sec#011loss=1.142319\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 619.0659999847412, \"sum\": 619.0659999847412, \"min\": 619.0659999847412}}, \"EndTime\": 1601747349.792019, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747349.172618}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1039.89913971 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=23, train loss <loss>=1.15464743701\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:09 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_cd7b3dd4-c706-4280-aed3-12f986989e80-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.897853851318359, \"sum\": 7.897853851318359, \"min\": 7.897853851318359}}, \"EndTime\": 1601747349.800789, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747349.792192}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:10 INFO 139787867731776] Epoch[24] Batch[0] avg_epoch_loss=1.242684\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=1.24268436432\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:10 INFO 139787867731776] Epoch[24] Batch[5] avg_epoch_loss=1.173660\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=1.17365972201\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:10 INFO 139787867731776] Epoch[24] Batch [5]#011Speed: 4333.52 samples/sec#011loss=1.173660\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:10 INFO 139787867731776] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 533.2369804382324, \"sum\": 533.2369804382324, \"min\": 533.2369804382324}}, \"EndTime\": 1601747350.334134, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747349.800849}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:10 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1188.79949429 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:10 INFO 139787867731776] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=24, train loss <loss>=1.18090349436\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:10 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:10 INFO 139787867731776] Epoch[25] Batch[0] avg_epoch_loss=1.184340\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=1.18433976173\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:10 INFO 139787867731776] Epoch[25] Batch[5] avg_epoch_loss=1.176522\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=1.17652201653\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:10 INFO 139787867731776] Epoch[25] Batch [5]#011Speed: 3582.59 samples/sec#011loss=1.176522\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:10 INFO 139787867731776] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 534.3339443206787, \"sum\": 534.3339443206787, \"min\": 534.3339443206787}}, \"EndTime\": 1601747350.868869, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747350.334183}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:10 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1171.34136999 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:10 INFO 139787867731776] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=25, train loss <loss>=1.20125448704\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:10 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] Epoch[26] Batch[0] avg_epoch_loss=1.113979\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=1.11397945881\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] Epoch[26] Batch[5] avg_epoch_loss=1.155297\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=1.15529696147\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] Epoch[26] Batch [5]#011Speed: 4359.20 samples/sec#011loss=1.155297\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] Epoch[26] Batch[10] avg_epoch_loss=1.176726\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=1.20244164467\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] Epoch[26] Batch [10]#011Speed: 4279.86 samples/sec#011loss=1.202442\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 544.4490909576416, \"sum\": 544.4490909576416, \"min\": 544.4490909576416}}, \"EndTime\": 1601747351.413716, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747350.868934}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1184.4835508 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=26, train loss <loss>=1.17672636292\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] Epoch[27] Batch[0] avg_epoch_loss=1.239109\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=1.23910856247\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] Epoch[27] Batch[5] avg_epoch_loss=1.218306\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=1.21830602487\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] Epoch[27] Batch [5]#011Speed: 4301.48 samples/sec#011loss=1.218306\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 543.9939498901367, \"sum\": 543.9939498901367, \"min\": 543.9939498901367}}, \"EndTime\": 1601747351.958072, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747351.413779}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1163.41039552 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=27, train loss <loss>=1.21600472927\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:11 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:12 INFO 139787867731776] Epoch[28] Batch[0] avg_epoch_loss=1.120029\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=1.12002944946\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:12 INFO 139787867731776] Epoch[28] Batch[5] avg_epoch_loss=1.169116\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=1.16911572218\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:12 INFO 139787867731776] Epoch[28] Batch [5]#011Speed: 4320.48 samples/sec#011loss=1.169116\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:12 INFO 139787867731776] Epoch[28] Batch[10] avg_epoch_loss=1.164557\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=1.15908668041\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:12 INFO 139787867731776] Epoch[28] Batch [10]#011Speed: 3839.19 samples/sec#011loss=1.159087\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:12 INFO 139787867731776] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 567.655086517334, \"sum\": 567.655086517334, \"min\": 567.655086517334}}, \"EndTime\": 1601747352.526198, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747351.958135}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:12 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1160.73243172 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:12 INFO 139787867731776] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=28, train loss <loss>=1.16455706683\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:12 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:12 INFO 139787867731776] Epoch[29] Batch[0] avg_epoch_loss=1.074617\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=1.07461726665\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:12 INFO 139787867731776] Epoch[29] Batch[5] avg_epoch_loss=1.128595\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=1.1285948356\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:12 INFO 139787867731776] Epoch[29] Batch [5]#011Speed: 4009.62 samples/sec#011loss=1.128595\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:13 INFO 139787867731776] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 524.8000621795654, \"sum\": 524.8000621795654, \"min\": 524.8000621795654}}, \"EndTime\": 1601747353.051453, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747352.52626}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:13 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1139.2647096 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:13 INFO 139787867731776] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=29, train loss <loss>=1.1254263401\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:13 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:13 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_4686c5b1-d14a-4679-8b2a-1877d96b64a2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 5.72514533996582, \"sum\": 5.72514533996582, \"min\": 5.72514533996582}}, \"EndTime\": 1601747353.057691, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747353.051518}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:13 INFO 139787867731776] Epoch[30] Batch[0] avg_epoch_loss=1.148735\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=1.14873504639\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:13 INFO 139787867731776] Epoch[30] Batch[5] avg_epoch_loss=1.132760\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=1.13275990884\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:13 INFO 139787867731776] Epoch[30] Batch [5]#011Speed: 3838.41 samples/sec#011loss=1.132760\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:13 INFO 139787867731776] Epoch[30] Batch[10] avg_epoch_loss=1.122838\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=1.11093124151\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:13 INFO 139787867731776] Epoch[30] Batch [10]#011Speed: 3987.29 samples/sec#011loss=1.110931\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:13 INFO 139787867731776] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 562.6490116119385, \"sum\": 562.6490116119385, \"min\": 562.6490116119385}}, \"EndTime\": 1601747353.620456, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747353.057749}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:13 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1153.25474778 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:13 INFO 139787867731776] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=30, train loss <loss>=1.12283778732\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:13 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:13 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_48dab232-6779-469d-983b-1f93ddcee83f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.3190460205078125, \"sum\": 6.3190460205078125, \"min\": 6.3190460205078125}}, \"EndTime\": 1601747353.627391, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747353.620527}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] Epoch[31] Batch[0] avg_epoch_loss=1.208923\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=1.20892298222\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] Epoch[31] Batch[5] avg_epoch_loss=1.197138\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=1.19713832935\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] Epoch[31] Batch [5]#011Speed: 4277.61 samples/sec#011loss=1.197138\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] Epoch[31] Batch[10] avg_epoch_loss=1.188036\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=1.17711241245\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] Epoch[31] Batch [10]#011Speed: 4195.34 samples/sec#011loss=1.177112\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 535.6440544128418, \"sum\": 535.6440544128418, \"min\": 535.6440544128418}}, \"EndTime\": 1601747354.163156, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747353.627456}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1246.88315245 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=31, train loss <loss>=1.18803563985\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] Epoch[32] Batch[0] avg_epoch_loss=1.119873\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=1.1198734045\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] Epoch[32] Batch[5] avg_epoch_loss=1.131700\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=1.13170023759\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] Epoch[32] Batch [5]#011Speed: 4338.11 samples/sec#011loss=1.131700\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] Epoch[32] Batch[10] avg_epoch_loss=1.165015\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=1.20499222279\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] Epoch[32] Batch [10]#011Speed: 3954.17 samples/sec#011loss=1.204992\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 532.4749946594238, \"sum\": 532.4749946594238, \"min\": 532.4749946594238}}, \"EndTime\": 1601747354.696054, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747354.163216}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1224.25247475 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=32, train loss <loss>=1.16501477632\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:14 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] Epoch[33] Batch[0] avg_epoch_loss=1.198948\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=1.19894766808\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] Epoch[33] Batch[5] avg_epoch_loss=1.152898\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=1.15289803346\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] Epoch[33] Batch [5]#011Speed: 4278.77 samples/sec#011loss=1.152898\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] Epoch[33] Batch[10] avg_epoch_loss=1.202640\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=1.26233100891\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] Epoch[33] Batch [10]#011Speed: 4311.39 samples/sec#011loss=1.262331\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 584.5198631286621, \"sum\": 584.5198631286621, \"min\": 584.5198631286621}}, \"EndTime\": 1601747355.280937, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747354.69612}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1110.13413517 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=33, train loss <loss>=1.20264029503\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] Epoch[34] Batch[0] avg_epoch_loss=1.116270\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=1.11627018452\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] Epoch[34] Batch[5] avg_epoch_loss=1.138304\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=1.13830443223\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] Epoch[34] Batch [5]#011Speed: 4179.83 samples/sec#011loss=1.138304\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 580.7058811187744, \"sum\": 580.7058811187744, \"min\": 580.7058811187744}}, \"EndTime\": 1601747355.862031, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747355.281002}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1082.93864168 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=34, train loss <loss>=1.1359411478\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:15 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] Epoch[35] Batch[0] avg_epoch_loss=1.102996\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=1.1029958725\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] Epoch[35] Batch[5] avg_epoch_loss=1.149438\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=1.14943842093\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] Epoch[35] Batch [5]#011Speed: 4122.06 samples/sec#011loss=1.149438\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 532.6919555664062, \"sum\": 532.6919555664062, \"min\": 532.6919555664062}}, \"EndTime\": 1601747356.395171, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747355.862111}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1193.72743797 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=35, train loss <loss>=1.16492476463\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] Epoch[36] Batch[0] avg_epoch_loss=1.136096\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=1.1360963583\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] Epoch[36] Batch[5] avg_epoch_loss=1.125676\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=1.12567615509\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] Epoch[36] Batch [5]#011Speed: 3825.77 samples/sec#011loss=1.125676\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] Epoch[36] Batch[10] avg_epoch_loss=1.090554\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=1.04840843678\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] Epoch[36] Batch [10]#011Speed: 4021.11 samples/sec#011loss=1.048408\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 545.5400943756104, \"sum\": 545.5400943756104, \"min\": 545.5400943756104}}, \"EndTime\": 1601747356.94117, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747356.395233}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1178.44761685 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=36, train loss <loss>=1.09055446495\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:16 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_c7db8bd3-bbcd-47e4-8b6a-68b265e14805-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.206035614013672, \"sum\": 6.206035614013672, \"min\": 6.206035614013672}}, \"EndTime\": 1601747356.947865, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747356.941232}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:17 INFO 139787867731776] Epoch[37] Batch[0] avg_epoch_loss=1.168228\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=1.168227911\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:17 INFO 139787867731776] Epoch[37] Batch[5] avg_epoch_loss=1.154351\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=1.15435111523\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:17 INFO 139787867731776] Epoch[37] Batch [5]#011Speed: 4022.37 samples/sec#011loss=1.154351\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:17 INFO 139787867731776] Epoch[37] Batch[10] avg_epoch_loss=1.177075\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=1.20434377193\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:17 INFO 139787867731776] Epoch[37] Batch [10]#011Speed: 4301.08 samples/sec#011loss=1.204344\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:17 INFO 139787867731776] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 535.8340740203857, \"sum\": 535.8340740203857, \"min\": 535.8340740203857}}, \"EndTime\": 1601747357.483808, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747356.947917}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:17 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1220.30175108 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:17 INFO 139787867731776] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=37, train loss <loss>=1.17707505009\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:17 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:17 INFO 139787867731776] Epoch[38] Batch[0] avg_epoch_loss=1.253994\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=1.25399422646\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:17 INFO 139787867731776] Epoch[38] Batch[5] avg_epoch_loss=1.159303\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=1.15930306911\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:17 INFO 139787867731776] Epoch[38] Batch [5]#011Speed: 3539.67 samples/sec#011loss=1.159303\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:18 INFO 139787867731776] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 527.2328853607178, \"sum\": 527.2328853607178, \"min\": 527.2328853607178}}, \"EndTime\": 1601747358.011506, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747357.483874}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:18 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1179.52296014 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:18 INFO 139787867731776] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=38, train loss <loss>=1.16451251507\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:18 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:18 INFO 139787867731776] Epoch[39] Batch[0] avg_epoch_loss=1.049752\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=1.04975235462\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:18 INFO 139787867731776] Epoch[39] Batch[5] avg_epoch_loss=1.151025\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=1.15102454027\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:18 INFO 139787867731776] Epoch[39] Batch [5]#011Speed: 3864.55 samples/sec#011loss=1.151025\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:18 INFO 139787867731776] Epoch[39] Batch[10] avg_epoch_loss=1.128376\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=1.10119848251\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:18 INFO 139787867731776] Epoch[39] Batch [10]#011Speed: 4333.94 samples/sec#011loss=1.101198\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:18 INFO 139787867731776] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 552.1390438079834, \"sum\": 552.1390438079834, \"min\": 552.1390438079834}}, \"EndTime\": 1601747358.564047, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747358.011573}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:18 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1160.7459828 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:18 INFO 139787867731776] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=39, train loss <loss>=1.1283763322\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:18 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:18 INFO 139787867731776] Epoch[40] Batch[0] avg_epoch_loss=1.236599\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=1.2365988493\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:19 INFO 139787867731776] Epoch[40] Batch[5] avg_epoch_loss=1.178477\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=1.1784774065\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:19 INFO 139787867731776] Epoch[40] Batch [5]#011Speed: 4300.49 samples/sec#011loss=1.178477\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:19 INFO 139787867731776] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 535.1870059967041, \"sum\": 535.1870059967041, \"min\": 535.1870059967041}}, \"EndTime\": 1601747359.099586, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747358.56411}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:19 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1135.85870854 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:19 INFO 139787867731776] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=40, train loss <loss>=1.1558332324\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:19 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:19 INFO 139787867731776] Epoch[41] Batch[0] avg_epoch_loss=1.100840\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=1.10083973408\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:19 INFO 139787867731776] Epoch[41] Batch[5] avg_epoch_loss=1.113080\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=1.11308024327\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:19 INFO 139787867731776] Epoch[41] Batch [5]#011Speed: 4308.66 samples/sec#011loss=1.113080\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:19 INFO 139787867731776] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 508.44502449035645, \"sum\": 508.44502449035645, \"min\": 508.44502449035645}}, \"EndTime\": 1601747359.608507, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747359.099651}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:19 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1181.80760099 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:19 INFO 139787867731776] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=41, train loss <loss>=1.11542963982\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:19 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] Epoch[42] Batch[0] avg_epoch_loss=1.118484\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=1.1184835434\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] Epoch[42] Batch[5] avg_epoch_loss=1.107949\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=1.10794939597\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] Epoch[42] Batch [5]#011Speed: 4258.15 samples/sec#011loss=1.107949\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 563.0559921264648, \"sum\": 563.0559921264648, \"min\": 563.0559921264648}}, \"EndTime\": 1601747360.171989, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747359.608573}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1067.20909795 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=42, train loss <loss>=1.10687031746\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] Epoch[43] Batch[0] avg_epoch_loss=1.072321\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=1.07232069969\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] Epoch[43] Batch[5] avg_epoch_loss=1.135884\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=1.13588442405\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] Epoch[43] Batch [5]#011Speed: 4249.27 samples/sec#011loss=1.135884\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] Epoch[43] Batch[10] avg_epoch_loss=1.101356\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=1.05992149115\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] Epoch[43] Batch [10]#011Speed: 4200.63 samples/sec#011loss=1.059921\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 528.3839702606201, \"sum\": 528.3839702606201, \"min\": 528.3839702606201}}, \"EndTime\": 1601747360.700822, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747360.172054}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1212.90800473 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=43, train loss <loss>=1.10135581818\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:20 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] Epoch[44] Batch[0] avg_epoch_loss=1.154948\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=1.15494775772\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] Epoch[44] Batch[5] avg_epoch_loss=1.150833\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=1.15083253384\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] Epoch[44] Batch [5]#011Speed: 4291.51 samples/sec#011loss=1.150833\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 513.1790637969971, \"sum\": 513.1790637969971, \"min\": 513.1790637969971}}, \"EndTime\": 1601747361.214466, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747360.700888}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1244.95151529 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=44, train loss <loss>=1.12862118483\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] Epoch[45] Batch[0] avg_epoch_loss=1.129005\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=1.12900519371\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] Epoch[45] Batch[5] avg_epoch_loss=1.114053\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=1.11405269305\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] Epoch[45] Batch [5]#011Speed: 4035.71 samples/sec#011loss=1.114053\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] Epoch[45] Batch[10] avg_epoch_loss=1.111788\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=1.10907084942\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] Epoch[45] Batch [10]#011Speed: 3788.69 samples/sec#011loss=1.109071\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 545.1459884643555, \"sum\": 545.1459884643555, \"min\": 545.1459884643555}}, \"EndTime\": 1601747361.760113, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747361.21453}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1223.30771716 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=45, train loss <loss>=1.11178821867\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:21 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:22 INFO 139787867731776] Epoch[46] Batch[0] avg_epoch_loss=1.090061\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=1.09006106853\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:22 INFO 139787867731776] Epoch[46] Batch[5] avg_epoch_loss=1.137032\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=1.13703187307\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:22 INFO 139787867731776] Epoch[46] Batch [5]#011Speed: 4235.15 samples/sec#011loss=1.137032\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:22 INFO 139787867731776] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 518.8570022583008, \"sum\": 518.8570022583008, \"min\": 518.8570022583008}}, \"EndTime\": 1601747362.279454, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747361.760178}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:22 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1217.84151889 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:22 INFO 139787867731776] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=46, train loss <loss>=1.13903287649\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:22 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:22 INFO 139787867731776] Epoch[47] Batch[0] avg_epoch_loss=1.035270\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=1.03526997566\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:22 INFO 139787867731776] Epoch[47] Batch[5] avg_epoch_loss=1.105359\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=1.10535911719\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:22 INFO 139787867731776] Epoch[47] Batch [5]#011Speed: 4068.24 samples/sec#011loss=1.105359\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:22 INFO 139787867731776] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 520.733118057251, \"sum\": 520.733118057251, \"min\": 520.733118057251}}, \"EndTime\": 1601747362.800731, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747362.279515}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:22 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1175.04770982 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:22 INFO 139787867731776] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=47, train loss <loss>=1.09787467718\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:22 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] Epoch[48] Batch[0] avg_epoch_loss=1.145137\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=1.14513742924\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] Epoch[48] Batch[5] avg_epoch_loss=1.093926\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=1.09392602245\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] Epoch[48] Batch [5]#011Speed: 4312.15 samples/sec#011loss=1.093926\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 526.4379978179932, \"sum\": 526.4379978179932, \"min\": 526.4379978179932}}, \"EndTime\": 1601747363.327637, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747362.800795}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1168.00234008 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=48, train loss <loss>=1.1030629158\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] Epoch[49] Batch[0] avg_epoch_loss=1.047805\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=1.04780519009\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] Epoch[49] Batch[5] avg_epoch_loss=1.110418\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=1.11041758458\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] Epoch[49] Batch [5]#011Speed: 3752.39 samples/sec#011loss=1.110418\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] Epoch[49] Batch[10] avg_epoch_loss=1.083503\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=1.05120588541\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] Epoch[49] Batch [10]#011Speed: 4231.96 samples/sec#011loss=1.051206\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 553.2708168029785, \"sum\": 553.2708168029785, \"min\": 553.2708168029785}}, \"EndTime\": 1601747363.881324, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747363.327704}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1185.45544412 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=49, train loss <loss>=1.08350317587\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:23 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_e41ce060-12ac-4aea-946c-347d77808978-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 5.639076232910156, \"sum\": 5.639076232910156, \"min\": 5.639076232910156}}, \"EndTime\": 1601747363.887472, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747363.881394}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] Epoch[50] Batch[0] avg_epoch_loss=1.120062\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=1.12006151676\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] Epoch[50] Batch[5] avg_epoch_loss=1.094686\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=1.09468640884\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] Epoch[50] Batch [5]#011Speed: 4025.97 samples/sec#011loss=1.094686\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] Epoch[50] Batch[10] avg_epoch_loss=1.049870\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=0.996091127396\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] Epoch[50] Batch [10]#011Speed: 4166.61 samples/sec#011loss=0.996091\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 548.2559204101562, \"sum\": 548.2559204101562, \"min\": 548.2559204101562}}, \"EndTime\": 1601747364.43583, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747363.887522}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1183.5550767 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=50, train loss <loss>=1.04987037182\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_2cb3ea22-b1bc-4f61-b22b-ac6c61e58e57-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.838964462280273, \"sum\": 7.838964462280273, \"min\": 7.838964462280273}}, \"EndTime\": 1601747364.444067, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747364.435892}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] Epoch[51] Batch[0] avg_epoch_loss=1.191043\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=1.19104254246\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] Epoch[51] Batch[5] avg_epoch_loss=1.126487\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=1.12648723523\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] Epoch[51] Batch [5]#011Speed: 3902.48 samples/sec#011loss=1.126487\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 549.5550632476807, \"sum\": 549.5550632476807, \"min\": 549.5550632476807}}, \"EndTime\": 1601747364.99374, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747364.444124}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1133.40145695 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=51, train loss <loss>=1.1304449439\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:24 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:25 INFO 139787867731776] Epoch[52] Batch[0] avg_epoch_loss=1.218768\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=1.21876764297\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:25 INFO 139787867731776] Epoch[52] Batch[5] avg_epoch_loss=1.089454\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=1.08945439259\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:25 INFO 139787867731776] Epoch[52] Batch [5]#011Speed: 4377.03 samples/sec#011loss=1.089454\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:25 INFO 139787867731776] Epoch[52] Batch[10] avg_epoch_loss=1.091583\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=1.09413762093\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:25 INFO 139787867731776] Epoch[52] Batch [10]#011Speed: 4249.51 samples/sec#011loss=1.094138\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:25 INFO 139787867731776] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 539.5059585571289, \"sum\": 539.5059585571289, \"min\": 539.5059585571289}}, \"EndTime\": 1601747365.534404, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747364.993819}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:25 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1215.70644861 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:25 INFO 139787867731776] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=52, train loss <loss>=1.09158313274\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:25 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:25 INFO 139787867731776] Epoch[53] Batch[0] avg_epoch_loss=1.119849\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=1.11984944344\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] Epoch[53] Batch[5] avg_epoch_loss=1.107965\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=1.10796515147\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] Epoch[53] Batch [5]#011Speed: 3825.23 samples/sec#011loss=1.107965\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] Epoch[53] Batch[10] avg_epoch_loss=1.126368\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=53, batch=10 train loss <loss>=1.14845211506\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] Epoch[53] Batch [10]#011Speed: 4192.71 samples/sec#011loss=1.148452\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 553.898811340332, \"sum\": 553.898811340332, \"min\": 553.898811340332}}, \"EndTime\": 1601747366.088688, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747365.53447}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1167.88482085 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=53, train loss <loss>=1.12636831674\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] Epoch[54] Batch[0] avg_epoch_loss=1.198086\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=1.19808614254\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] Epoch[54] Batch[5] avg_epoch_loss=1.104689\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=1.10468945901\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] Epoch[54] Batch [5]#011Speed: 4335.45 samples/sec#011loss=1.104689\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] Epoch[54] Batch[10] avg_epoch_loss=1.103986\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=54, batch=10 train loss <loss>=1.10314216614\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] Epoch[54] Batch [10]#011Speed: 4232.86 samples/sec#011loss=1.103142\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 581.0849666595459, \"sum\": 581.0849666595459, \"min\": 581.0849666595459}}, \"EndTime\": 1601747366.670165, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747366.088753}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1142.49127795 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=54, train loss <loss>=1.10398614407\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:26 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] Epoch[55] Batch[0] avg_epoch_loss=1.164347\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=1.16434705257\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] Epoch[55] Batch[5] avg_epoch_loss=1.102115\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=1.10211479664\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] Epoch[55] Batch [5]#011Speed: 3829.31 samples/sec#011loss=1.102115\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 532.804012298584, \"sum\": 532.804012298584, \"min\": 532.804012298584}}, \"EndTime\": 1601747367.203415, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747366.670232}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1176.59444889 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=55, train loss <loss>=1.16061954498\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] Epoch[56] Batch[0] avg_epoch_loss=0.996079\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=0.996078550816\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] Epoch[56] Batch[5] avg_epoch_loss=1.106794\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=1.10679371158\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] Epoch[56] Batch [5]#011Speed: 3852.96 samples/sec#011loss=1.106794\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] Epoch[56] Batch[10] avg_epoch_loss=1.117563\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=1.13048672676\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] Epoch[56] Batch [10]#011Speed: 3791.16 samples/sec#011loss=1.130487\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 546.7641353607178, \"sum\": 546.7641353607178, \"min\": 546.7641353607178}}, \"EndTime\": 1601747367.750626, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747367.203478}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1206.89450052 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=56, train loss <loss>=1.11756326394\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:27 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] Epoch[57] Batch[0] avg_epoch_loss=1.095473\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=1.09547328949\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] Epoch[57] Batch[5] avg_epoch_loss=1.106670\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=1.10667037964\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] Epoch[57] Batch [5]#011Speed: 4289.26 samples/sec#011loss=1.106670\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] Epoch[57] Batch[10] avg_epoch_loss=1.119350\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=1.13456473351\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] Epoch[57] Batch [10]#011Speed: 4204.55 samples/sec#011loss=1.134565\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 589.8778438568115, \"sum\": 589.8778438568115, \"min\": 589.8778438568115}}, \"EndTime\": 1601747368.340874, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747367.750691}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1123.78619631 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=57, train loss <loss>=1.1193496314\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] Epoch[58] Batch[0] avg_epoch_loss=1.095742\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=1.09574210644\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] Epoch[58] Batch[5] avg_epoch_loss=1.102309\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=1.10230871042\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] Epoch[58] Batch [5]#011Speed: 4319.13 samples/sec#011loss=1.102309\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] Epoch[58] Batch[10] avg_epoch_loss=1.137317\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=1.17932682037\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] Epoch[58] Batch [10]#011Speed: 4269.12 samples/sec#011loss=1.179327\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 585.1249694824219, \"sum\": 585.1249694824219, \"min\": 585.1249694824219}}, \"EndTime\": 1601747368.926362, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747368.340937}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1149.99889676 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=58, train loss <loss>=1.13731694221\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:28 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:29 INFO 139787867731776] Epoch[59] Batch[0] avg_epoch_loss=1.069819\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=1.06981945038\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:29 INFO 139787867731776] Epoch[59] Batch[5] avg_epoch_loss=1.066339\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=1.06633935372\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:29 INFO 139787867731776] Epoch[59] Batch [5]#011Speed: 4316.85 samples/sec#011loss=1.066339\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:29 INFO 139787867731776] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 564.094066619873, \"sum\": 564.094066619873, \"min\": 564.094066619873}}, \"EndTime\": 1601747369.49082, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747368.926425}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:29 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1088.28603304 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:29 INFO 139787867731776] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=59, train loss <loss>=1.11477462053\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:29 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:29 INFO 139787867731776] Epoch[60] Batch[0] avg_epoch_loss=1.011740\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=1.01174032688\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:29 INFO 139787867731776] Epoch[60] Batch[5] avg_epoch_loss=1.091204\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=1.09120366971\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:29 INFO 139787867731776] Epoch[60] Batch [5]#011Speed: 4279.92 samples/sec#011loss=1.091204\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:30 INFO 139787867731776] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 546.5550422668457, \"sum\": 546.5550422668457, \"min\": 546.5550422668457}}, \"EndTime\": 1601747370.037798, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747369.490886}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:30 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1156.09261301 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:30 INFO 139787867731776] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=60, train loss <loss>=1.0777803123\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:30 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:30 INFO 139787867731776] Epoch[61] Batch[0] avg_epoch_loss=1.171782\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=1.17178165913\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:30 INFO 139787867731776] Epoch[61] Batch[5] avg_epoch_loss=1.071671\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=1.07167139649\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:30 INFO 139787867731776] Epoch[61] Batch [5]#011Speed: 4358.13 samples/sec#011loss=1.071671\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:30 INFO 139787867731776] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 539.5400524139404, \"sum\": 539.5400524139404, \"min\": 539.5400524139404}}, \"EndTime\": 1601747370.578025, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747370.037858}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:30 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1139.66537671 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:30 INFO 139787867731776] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=61, train loss <loss>=1.04959592223\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:30 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:30 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_b28f7f25-da0a-47de-a5ce-2db042dbf0d5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.222009658813477, \"sum\": 6.222009658813477, \"min\": 6.222009658813477}}, \"EndTime\": 1601747370.584737, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747370.578089}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:30 INFO 139787867731776] Epoch[62] Batch[0] avg_epoch_loss=1.037115\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=1.03711521626\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] Epoch[62] Batch[5] avg_epoch_loss=1.043619\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=1.04361873865\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] Epoch[62] Batch [5]#011Speed: 4270.89 samples/sec#011loss=1.043619\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] Epoch[62] Batch[10] avg_epoch_loss=1.065008\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=1.09067621231\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] Epoch[62] Batch [10]#011Speed: 3675.08 samples/sec#011loss=1.090676\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 543.8640117645264, \"sum\": 543.8640117645264, \"min\": 543.8640117645264}}, \"EndTime\": 1601747371.128712, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747370.584794}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1205.95548929 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=62, train loss <loss>=1.06500849941\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] Epoch[63] Batch[0] avg_epoch_loss=1.065293\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=1.06529331207\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] Epoch[63] Batch[5] avg_epoch_loss=1.061761\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=1.06176088254\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] Epoch[63] Batch [5]#011Speed: 4333.70 samples/sec#011loss=1.061761\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] Epoch[63] Batch[10] avg_epoch_loss=1.080962\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=1.10400226116\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] Epoch[63] Batch [10]#011Speed: 3964.66 samples/sec#011loss=1.104002\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 534.8751544952393, \"sum\": 534.8751544952393, \"min\": 534.8751544952393}}, \"EndTime\": 1601747371.664078, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747371.128783}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1200.08323748 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=63, train loss <loss>=1.08096150918\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:31 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] Epoch[64] Batch[0] avg_epoch_loss=1.068365\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=1.06836509705\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] Epoch[64] Batch[5] avg_epoch_loss=1.067641\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=1.06764080127\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] Epoch[64] Batch [5]#011Speed: 3649.97 samples/sec#011loss=1.067641\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 527.6479721069336, \"sum\": 527.6479721069336, \"min\": 527.6479721069336}}, \"EndTime\": 1601747372.192102, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747371.664139}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1157.76027901 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=64, train loss <loss>=1.06925170422\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] Epoch[65] Batch[0] avg_epoch_loss=1.082369\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=1.08236885071\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] Epoch[65] Batch[5] avg_epoch_loss=1.078768\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=1.07876799504\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] Epoch[65] Batch [5]#011Speed: 4143.25 samples/sec#011loss=1.078768\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] Epoch[65] Batch[10] avg_epoch_loss=1.083498\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=1.08917400837\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] Epoch[65] Batch [10]#011Speed: 4209.59 samples/sec#011loss=1.089174\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 539.8209095001221, \"sum\": 539.8209095001221, \"min\": 539.8209095001221}}, \"EndTime\": 1601747372.732325, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747372.192167}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1250.18776075 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=65, train loss <loss>=1.0834980011\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:32 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:33 INFO 139787867731776] Epoch[66] Batch[0] avg_epoch_loss=1.059469\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=1.05946862698\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:33 INFO 139787867731776] Epoch[66] Batch[5] avg_epoch_loss=1.106077\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=1.10607721408\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:33 INFO 139787867731776] Epoch[66] Batch [5]#011Speed: 4169.75 samples/sec#011loss=1.106077\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:33 INFO 139787867731776] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 530.1380157470703, \"sum\": 530.1380157470703, \"min\": 530.1380157470703}}, \"EndTime\": 1601747373.262891, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747372.732391}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:33 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1178.63137151 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:33 INFO 139787867731776] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=66, train loss <loss>=1.07236218452\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:33 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:33 INFO 139787867731776] Epoch[67] Batch[0] avg_epoch_loss=1.029991\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=1.02999126911\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:33 INFO 139787867731776] Epoch[67] Batch[5] avg_epoch_loss=1.046625\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=1.04662546515\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:33 INFO 139787867731776] Epoch[67] Batch [5]#011Speed: 3715.18 samples/sec#011loss=1.046625\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:33 INFO 139787867731776] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 543.4730052947998, \"sum\": 543.4730052947998, \"min\": 543.4730052947998}}, \"EndTime\": 1601747373.806911, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747373.262995}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:33 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1147.98124102 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:33 INFO 139787867731776] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=67, train loss <loss>=1.06291764379\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:33 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] Epoch[68] Batch[0] avg_epoch_loss=1.014275\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=1.01427543163\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] Epoch[68] Batch[5] avg_epoch_loss=1.058333\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=1.05833320816\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] Epoch[68] Batch [5]#011Speed: 4335.10 samples/sec#011loss=1.058333\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 516.7458057403564, \"sum\": 516.7458057403564, \"min\": 516.7458057403564}}, \"EndTime\": 1601747374.324124, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747373.806973}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1213.13640555 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=68, train loss <loss>=1.07606831789\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] Epoch[69] Batch[0] avg_epoch_loss=1.218897\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=1.21889686584\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] Epoch[69] Batch[5] avg_epoch_loss=1.094238\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=1.09423846006\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] Epoch[69] Batch [5]#011Speed: 4358.89 samples/sec#011loss=1.094238\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] Epoch[69] Batch[10] avg_epoch_loss=1.187853\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=1.3001911521\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] Epoch[69] Batch [10]#011Speed: 3829.54 samples/sec#011loss=1.300191\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 539.3438339233398, \"sum\": 539.3438339233398, \"min\": 539.3438339233398}}, \"EndTime\": 1601747374.863948, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747374.324187}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1217.92989315 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=69, train loss <loss>=1.18785332008\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:34 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:35 INFO 139787867731776] Epoch[70] Batch[0] avg_epoch_loss=1.113263\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=1.1132632494\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:35 INFO 139787867731776] Epoch[70] Batch[5] avg_epoch_loss=1.121233\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=1.12123250961\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:35 INFO 139787867731776] Epoch[70] Batch [5]#011Speed: 3593.00 samples/sec#011loss=1.121233\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:35 INFO 139787867731776] processed a total of 596 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 546.9088554382324, \"sum\": 546.9088554382324, \"min\": 546.9088554382324}}, \"EndTime\": 1601747375.411296, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747374.864013}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:35 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1089.57396256 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:35 INFO 139787867731776] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=70, train loss <loss>=1.09385483265\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:35 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:35 INFO 139787867731776] Epoch[71] Batch[0] avg_epoch_loss=1.153257\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=1.15325665474\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:35 INFO 139787867731776] Epoch[71] Batch[5] avg_epoch_loss=1.071854\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=1.07185357809\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:35 INFO 139787867731776] Epoch[71] Batch [5]#011Speed: 4276.64 samples/sec#011loss=1.071854\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:35 INFO 139787867731776] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 522.8900909423828, \"sum\": 522.8900909423828, \"min\": 522.8900909423828}}, \"EndTime\": 1601747375.934596, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747375.411359}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:35 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1191.23879651 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:35 INFO 139787867731776] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=71, train loss <loss>=1.07996050119\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:35 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:36 INFO 139787867731776] Epoch[72] Batch[0] avg_epoch_loss=1.084441\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:36 INFO 139787867731776] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=1.08444142342\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:36 INFO 139787867731776] Epoch[72] Batch[5] avg_epoch_loss=1.107073\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:36 INFO 139787867731776] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=1.1070727706\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:36 INFO 139787867731776] Epoch[72] Batch [5]#011Speed: 4148.30 samples/sec#011loss=1.107073\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:36 INFO 139787867731776] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 544.4891452789307, \"sum\": 544.4891452789307, \"min\": 544.4891452789307}}, \"EndTime\": 1601747376.479505, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747375.934662}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:36 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1162.36114877 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:36 INFO 139787867731776] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:36 INFO 139787867731776] #quality_metric: host=algo-1, epoch=72, train loss <loss>=1.08903337717\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:36 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:36 INFO 139787867731776] Epoch[73] Batch[0] avg_epoch_loss=1.040753\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:36 INFO 139787867731776] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=1.04075264931\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:36 INFO 139787867731776] Epoch[73] Batch[5] avg_epoch_loss=1.045933\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:36 INFO 139787867731776] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=1.04593341549\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:36 INFO 139787867731776] Epoch[73] Batch [5]#011Speed: 4384.17 samples/sec#011loss=1.045933\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] Epoch[73] Batch[10] avg_epoch_loss=1.045149\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=1.04420793056\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] Epoch[73] Batch [10]#011Speed: 4181.75 samples/sec#011loss=1.044208\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 550.8859157562256, \"sum\": 550.8859157562256, \"min\": 550.8859157562256}}, \"EndTime\": 1601747377.030779, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747376.479566}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1232.32276545 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] #quality_metric: host=algo-1, epoch=73, train loss <loss>=1.04514910416\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_6ea824f9-7bd7-4580-a0f3-432ae72f6b59-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.767915725708008, \"sum\": 7.767915725708008, \"min\": 7.767915725708008}}, \"EndTime\": 1601747377.039015, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747377.030853}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] Epoch[74] Batch[0] avg_epoch_loss=1.153038\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=1.15303766727\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] Epoch[74] Batch[5] avg_epoch_loss=1.036967\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=1.03696718812\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] Epoch[74] Batch [5]#011Speed: 4306.46 samples/sec#011loss=1.036967\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] Epoch[74] Batch[10] avg_epoch_loss=1.032529\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=1.02720261812\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] Epoch[74] Batch [10]#011Speed: 4253.40 samples/sec#011loss=1.027203\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 570.7440376281738, \"sum\": 570.7440376281738, \"min\": 570.7440376281738}}, \"EndTime\": 1601747377.609885, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747377.039076}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1187.71070316 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] #quality_metric: host=algo-1, epoch=74, train loss <loss>=1.03252874721\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:37 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_ea45eccc-10b6-4efb-812c-15509a81d002-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.834911346435547, \"sum\": 7.834911346435547, \"min\": 7.834911346435547}}, \"EndTime\": 1601747377.618131, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747377.609946}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] Epoch[75] Batch[0] avg_epoch_loss=1.058140\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=1.05813992023\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] Epoch[75] Batch[5] avg_epoch_loss=1.080782\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=1.08078243335\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] Epoch[75] Batch [5]#011Speed: 3919.02 samples/sec#011loss=1.080782\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] Epoch[75] Batch[10] avg_epoch_loss=1.111145\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=1.14758033752\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] Epoch[75] Batch [10]#011Speed: 3987.09 samples/sec#011loss=1.147580\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 562.7009868621826, \"sum\": 562.7009868621826, \"min\": 562.7009868621826}}, \"EndTime\": 1601747378.180944, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747377.61819}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1151.38363074 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] #quality_metric: host=algo-1, epoch=75, train loss <loss>=1.11114511707\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] Epoch[76] Batch[0] avg_epoch_loss=1.018786\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=1.01878595352\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] Epoch[76] Batch[5] avg_epoch_loss=1.028387\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=1.02838738759\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] Epoch[76] Batch [5]#011Speed: 4186.01 samples/sec#011loss=1.028387\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 510.61511039733887, \"sum\": 510.61511039733887, \"min\": 510.61511039733887}}, \"EndTime\": 1601747378.692024, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747378.181014}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1180.69983614 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] #quality_metric: host=algo-1, epoch=76, train loss <loss>=1.04302223921\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:38 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] Epoch[77] Batch[0] avg_epoch_loss=1.087193\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=1.08719265461\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] Epoch[77] Batch[5] avg_epoch_loss=1.025715\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=1.02571528157\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] Epoch[77] Batch [5]#011Speed: 3518.30 samples/sec#011loss=1.025715\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] Epoch[77] Batch[10] avg_epoch_loss=1.048917\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=1.07675819397\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] Epoch[77] Batch [10]#011Speed: 3955.64 samples/sec#011loss=1.076758\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] processed a total of 695 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 573.0259418487549, \"sum\": 573.0259418487549, \"min\": 573.0259418487549}}, \"EndTime\": 1601747379.265456, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747378.692091}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1212.65825902 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] #quality_metric: host=algo-1, epoch=77, train loss <loss>=1.04891660539\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] Epoch[78] Batch[0] avg_epoch_loss=1.097780\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=1.09778034687\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] Epoch[78] Batch[5] avg_epoch_loss=1.055913\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=1.05591298143\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] Epoch[78] Batch [5]#011Speed: 4329.46 samples/sec#011loss=1.055913\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] Epoch[78] Batch[10] avg_epoch_loss=1.025707\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=0.989459013939\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] Epoch[78] Batch [10]#011Speed: 4246.51 samples/sec#011loss=0.989459\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 568.5811042785645, \"sum\": 568.5811042785645, \"min\": 568.5811042785645}}, \"EndTime\": 1601747379.83441, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747379.265522}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1160.59446787 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] #quality_metric: host=algo-1, epoch=78, train loss <loss>=1.02570663257\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:39 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_2ac9989d-1d53-4b4d-a33e-43c3e3308a01-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.8220367431640625, \"sum\": 7.8220367431640625, \"min\": 7.8220367431640625}}, \"EndTime\": 1601747379.842629, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747379.834474}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] Epoch[79] Batch[0] avg_epoch_loss=1.132956\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=1.13295590878\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] Epoch[79] Batch[5] avg_epoch_loss=1.040461\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=1.04046133161\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] Epoch[79] Batch [5]#011Speed: 3612.11 samples/sec#011loss=1.040461\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] Epoch[79] Batch[10] avg_epoch_loss=1.032436\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=1.02280505896\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] Epoch[79] Batch [10]#011Speed: 3423.52 samples/sec#011loss=1.022805\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 580.582857131958, \"sum\": 580.582857131958, \"min\": 580.582857131958}}, \"EndTime\": 1601747380.423326, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747379.842688}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1143.48105262 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] #quality_metric: host=algo-1, epoch=79, train loss <loss>=1.03243575313\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] Epoch[80] Batch[0] avg_epoch_loss=1.029726\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=1.02972579002\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] Epoch[80] Batch[5] avg_epoch_loss=1.027626\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=1.02762556076\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] Epoch[80] Batch [5]#011Speed: 4207.10 samples/sec#011loss=1.027626\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] Epoch[80] Batch[10] avg_epoch_loss=1.037115\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=1.04850320816\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] Epoch[80] Batch [10]#011Speed: 4200.18 samples/sec#011loss=1.048503\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 539.0710830688477, \"sum\": 539.0710830688477, \"min\": 539.0710830688477}}, \"EndTime\": 1601747380.962825, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747380.423393}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1242.66661478 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] #quality_metric: host=algo-1, epoch=80, train loss <loss>=1.03711540049\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:40 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:41 INFO 139787867731776] Epoch[81] Batch[0] avg_epoch_loss=1.098855\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:41 INFO 139787867731776] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=1.09885489941\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:41 INFO 139787867731776] Epoch[81] Batch[5] avg_epoch_loss=1.042441\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:41 INFO 139787867731776] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=1.04244107008\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:41 INFO 139787867731776] Epoch[81] Batch [5]#011Speed: 4227.94 samples/sec#011loss=1.042441\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:41 INFO 139787867731776] processed a total of 583 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 532.2809219360352, \"sum\": 532.2809219360352, \"min\": 532.2809219360352}}, \"EndTime\": 1601747381.495462, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747380.962889}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:41 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1095.1052959 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:41 INFO 139787867731776] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:41 INFO 139787867731776] #quality_metric: host=algo-1, epoch=81, train loss <loss>=1.03679384589\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:41 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:41 INFO 139787867731776] Epoch[82] Batch[0] avg_epoch_loss=1.030304\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:41 INFO 139787867731776] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=1.03030407429\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:41 INFO 139787867731776] Epoch[82] Batch[5] avg_epoch_loss=1.049489\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:41 INFO 139787867731776] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=1.04948925972\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:41 INFO 139787867731776] Epoch[82] Batch [5]#011Speed: 4339.46 samples/sec#011loss=1.049489\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] Epoch[82] Batch[10] avg_epoch_loss=1.053501\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=1.05831553936\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] Epoch[82] Batch [10]#011Speed: 4296.58 samples/sec#011loss=1.058316\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 525.1810550689697, \"sum\": 525.1810550689697, \"min\": 525.1810550689697}}, \"EndTime\": 1601747382.021122, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747381.495525}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1237.41449451 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=82, train loss <loss>=1.05350120501\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] Epoch[83] Batch[0] avg_epoch_loss=0.991871\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=0.99187117815\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] Epoch[83] Batch[5] avg_epoch_loss=1.055347\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=1.05534666777\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] Epoch[83] Batch [5]#011Speed: 4155.84 samples/sec#011loss=1.055347\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] Epoch[83] Batch[10] avg_epoch_loss=1.042307\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=1.02665991783\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] Epoch[83] Batch [10]#011Speed: 3979.25 samples/sec#011loss=1.026660\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] processed a total of 692 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 574.5851993560791, \"sum\": 574.5851993560791, \"min\": 574.5851993560791}}, \"EndTime\": 1601747382.596092, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747382.021196}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1204.1522037 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=83, train loss <loss>=1.04230723598\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] Epoch[84] Batch[0] avg_epoch_loss=1.001567\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=1.00156652927\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] Epoch[84] Batch[5] avg_epoch_loss=0.964177\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=0.964177062114\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] Epoch[84] Batch [5]#011Speed: 3624.01 samples/sec#011loss=0.964177\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 535.0539684295654, \"sum\": 535.0539684295654, \"min\": 535.0539684295654}}, \"EndTime\": 1601747383.13158, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747382.596157}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1182.84371015 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] #quality_metric: host=algo-1, epoch=84, train loss <loss>=0.963864338398\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_0d12eda8-57d2-40cd-a61b-4cadbf0c6682-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 5.743980407714844, \"sum\": 5.743980407714844, \"min\": 5.743980407714844}}, \"EndTime\": 1601747383.137861, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747383.131644}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] Epoch[85] Batch[0] avg_epoch_loss=0.945906\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=0.94590562582\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] Epoch[85] Batch[5] avg_epoch_loss=0.995242\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=0.99524212877\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] Epoch[85] Batch [5]#011Speed: 3929.52 samples/sec#011loss=0.995242\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] Epoch[85] Batch[10] avg_epoch_loss=0.970464\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=0.940731203556\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] Epoch[85] Batch [10]#011Speed: 3994.21 samples/sec#011loss=0.940731\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 549.3748188018799, \"sum\": 549.3748188018799, \"min\": 549.3748188018799}}, \"EndTime\": 1601747383.687349, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747383.137917}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1173.85601018 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] #quality_metric: host=algo-1, epoch=85, train loss <loss>=0.970464435491\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:43 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:44 INFO 139787867731776] Epoch[86] Batch[0] avg_epoch_loss=1.026204\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:44 INFO 139787867731776] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=1.02620446682\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:44 INFO 139787867731776] Epoch[86] Batch[5] avg_epoch_loss=1.059874\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:44 INFO 139787867731776] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=1.05987431606\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:44 INFO 139787867731776] Epoch[86] Batch [5]#011Speed: 4145.79 samples/sec#011loss=1.059874\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:44 INFO 139787867731776] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 518.2371139526367, \"sum\": 518.2371139526367, \"min\": 518.2371139526367}}, \"EndTime\": 1601747384.206001, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747383.687412}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:44 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1188.44333682 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:44 INFO 139787867731776] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:44 INFO 139787867731776] #quality_metric: host=algo-1, epoch=86, train loss <loss>=1.03592458963\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:44 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:44 INFO 139787867731776] Epoch[87] Batch[0] avg_epoch_loss=0.983991\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:44 INFO 139787867731776] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=0.98399066925\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:44 INFO 139787867731776] Epoch[87] Batch[5] avg_epoch_loss=1.017835\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:44 INFO 139787867731776] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=1.01783465346\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:44 INFO 139787867731776] Epoch[87] Batch [5]#011Speed: 4255.54 samples/sec#011loss=1.017835\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:44 INFO 139787867731776] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 536.9408130645752, \"sum\": 536.9408130645752, \"min\": 536.9408130645752}}, \"EndTime\": 1601747384.743395, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747384.206063}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:44 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1180.52792623 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:44 INFO 139787867731776] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:44 INFO 139787867731776] #quality_metric: host=algo-1, epoch=87, train loss <loss>=1.02148880363\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:44 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:45 INFO 139787867731776] Epoch[88] Batch[0] avg_epoch_loss=1.005544\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:45 INFO 139787867731776] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=1.00554358959\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:45 INFO 139787867731776] Epoch[88] Batch[5] avg_epoch_loss=0.981886\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:45 INFO 139787867731776] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=0.981886148453\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:45 INFO 139787867731776] Epoch[88] Batch [5]#011Speed: 4326.60 samples/sec#011loss=0.981886\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:45 INFO 139787867731776] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 537.553071975708, \"sum\": 537.553071975708, \"min\": 537.553071975708}}, \"EndTime\": 1601747385.281436, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747384.74347}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:45 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1177.37433531 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:45 INFO 139787867731776] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:45 INFO 139787867731776] #quality_metric: host=algo-1, epoch=88, train loss <loss>=0.996108287573\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:45 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:45 INFO 139787867731776] Epoch[89] Batch[0] avg_epoch_loss=1.068498\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:45 INFO 139787867731776] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=1.06849813461\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:45 INFO 139787867731776] Epoch[89] Batch[5] avg_epoch_loss=0.976059\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:45 INFO 139787867731776] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=0.976058552663\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:45 INFO 139787867731776] Epoch[89] Batch [5]#011Speed: 3940.15 samples/sec#011loss=0.976059\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:45 INFO 139787867731776] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 545.7289218902588, \"sum\": 545.7289218902588, \"min\": 545.7289218902588}}, \"EndTime\": 1601747385.827521, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747385.281491}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:45 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1135.89321033 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:45 INFO 139787867731776] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:45 INFO 139787867731776] #quality_metric: host=algo-1, epoch=89, train loss <loss>=0.964146363735\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:45 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] Epoch[90] Batch[0] avg_epoch_loss=0.961248\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=0.961247742176\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] Epoch[90] Batch[5] avg_epoch_loss=0.985034\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=0.985034475724\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] Epoch[90] Batch [5]#011Speed: 3826.10 samples/sec#011loss=0.985034\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 528.4209251403809, \"sum\": 528.4209251403809, \"min\": 528.4209251403809}}, \"EndTime\": 1601747386.356437, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747385.827585}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1197.68655733 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] #quality_metric: host=algo-1, epoch=90, train loss <loss>=1.00038502812\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] Epoch[91] Batch[0] avg_epoch_loss=0.954350\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=0.954349577427\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] Epoch[91] Batch[5] avg_epoch_loss=0.985608\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=0.985607633988\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] Epoch[91] Batch [5]#011Speed: 4143.79 samples/sec#011loss=0.985608\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] Epoch[91] Batch[10] avg_epoch_loss=1.007940\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=1.03473968506\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] Epoch[91] Batch [10]#011Speed: 4192.05 samples/sec#011loss=1.034740\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 588.670015335083, \"sum\": 588.670015335083, \"min\": 588.670015335083}}, \"EndTime\": 1601747386.94554, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747386.356503}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1122.69283497 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] #quality_metric: host=algo-1, epoch=91, train loss <loss>=1.00794038447\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:46 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:47 INFO 139787867731776] Epoch[92] Batch[0] avg_epoch_loss=0.926403\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:47 INFO 139787867731776] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=0.926402807236\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:47 INFO 139787867731776] Epoch[92] Batch[5] avg_epoch_loss=0.990508\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:47 INFO 139787867731776] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=0.99050796032\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:47 INFO 139787867731776] Epoch[92] Batch [5]#011Speed: 3683.28 samples/sec#011loss=0.990508\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:47 INFO 139787867731776] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 551.8569946289062, \"sum\": 551.8569946289062, \"min\": 551.8569946289062}}, \"EndTime\": 1601747387.497757, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747386.945602}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:47 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1081.57677339 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:47 INFO 139787867731776] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:47 INFO 139787867731776] #quality_metric: host=algo-1, epoch=92, train loss <loss>=1.01565673351\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:47 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:47 INFO 139787867731776] Epoch[93] Batch[0] avg_epoch_loss=0.887408\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:47 INFO 139787867731776] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=0.887407541275\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:47 INFO 139787867731776] Epoch[93] Batch[5] avg_epoch_loss=0.966798\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:47 INFO 139787867731776] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=0.966798186302\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:47 INFO 139787867731776] Epoch[93] Batch [5]#011Speed: 4256.23 samples/sec#011loss=0.966798\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] Epoch[93] Batch[10] avg_epoch_loss=0.959356\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=0.950424325466\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] Epoch[93] Batch [10]#011Speed: 4229.62 samples/sec#011loss=0.950424\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 536.9400978088379, \"sum\": 536.9400978088379, \"min\": 536.9400978088379}}, \"EndTime\": 1601747388.035148, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747387.49784}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1268.07098801 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] #quality_metric: host=algo-1, epoch=93, train loss <loss>=0.959355522286\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_b9e9b393-258f-4496-b26e-7d8a942e0d0e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.536006927490234, \"sum\": 6.536006927490234, \"min\": 6.536006927490234}}, \"EndTime\": 1601747388.04221, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747388.035212}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] Epoch[94] Batch[0] avg_epoch_loss=0.939492\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=0.939491510391\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] Epoch[94] Batch[5] avg_epoch_loss=0.983463\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=0.983462721109\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] Epoch[94] Batch [5]#011Speed: 3758.03 samples/sec#011loss=0.983463\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] Epoch[94] Batch[10] avg_epoch_loss=0.969789\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=0.953380310535\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] Epoch[94] Batch [10]#011Speed: 4032.81 samples/sec#011loss=0.953380\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 559.5691204071045, \"sum\": 559.5691204071045, \"min\": 559.5691204071045}}, \"EndTime\": 1601747388.601888, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747388.042261}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1159.60999732 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] #quality_metric: host=algo-1, epoch=94, train loss <loss>=0.969788898121\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] Epoch[95] Batch[0] avg_epoch_loss=1.021729\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:48 INFO 139787867731776] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=1.02172875404\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] Epoch[95] Batch[5] avg_epoch_loss=0.981811\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=0.981811304887\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] Epoch[95] Batch [5]#011Speed: 4006.97 samples/sec#011loss=0.981811\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] Epoch[95] Batch[10] avg_epoch_loss=0.936093\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=0.881231474876\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] Epoch[95] Batch [10]#011Speed: 4274.03 samples/sec#011loss=0.881231\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 538.0351543426514, \"sum\": 538.0351543426514, \"min\": 538.0351543426514}}, \"EndTime\": 1601747389.140398, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747388.601956}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1206.03673177 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] #quality_metric: host=algo-1, epoch=95, train loss <loss>=0.936093200337\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_c3d3792c-925c-4425-9898-17b2dc2a4b4f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.797002792358398, \"sum\": 7.797002792358398, \"min\": 7.797002792358398}}, \"EndTime\": 1601747389.148662, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747389.140459}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] Epoch[96] Batch[0] avg_epoch_loss=0.964095\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=0.964094996452\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] Epoch[96] Batch[5] avg_epoch_loss=1.010617\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=1.01061655084\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] Epoch[96] Batch [5]#011Speed: 3974.57 samples/sec#011loss=1.010617\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] Epoch[96] Batch[10] avg_epoch_loss=0.981028\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] #quality_metric: host=algo-1, epoch=96, batch=10 train loss <loss>=0.945521569252\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] Epoch[96] Batch [10]#011Speed: 4226.82 samples/sec#011loss=0.945522\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 568.8450336456299, \"sum\": 568.8450336456299, \"min\": 568.8450336456299}}, \"EndTime\": 1601747389.71762, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747389.148722}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1188.18312276 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] #quality_metric: host=algo-1, epoch=96, train loss <loss>=0.981027922847\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:49 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] Epoch[97] Batch[0] avg_epoch_loss=1.089700\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=1.08969962597\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] Epoch[97] Batch[5] avg_epoch_loss=1.040509\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=1.0405091246\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] Epoch[97] Batch [5]#011Speed: 4107.78 samples/sec#011loss=1.040509\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] Epoch[97] Batch[10] avg_epoch_loss=0.984721\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] #quality_metric: host=algo-1, epoch=97, batch=10 train loss <loss>=0.917774367332\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] Epoch[97] Batch [10]#011Speed: 4258.78 samples/sec#011loss=0.917774\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 576.8020153045654, \"sum\": 576.8020153045654, \"min\": 576.8020153045654}}, \"EndTime\": 1601747390.294783, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747389.717683}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1150.9993404 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] #quality_metric: host=algo-1, epoch=97, train loss <loss>=0.984720598568\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] Epoch[98] Batch[0] avg_epoch_loss=1.056275\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=1.05627548695\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] Epoch[98] Batch[5] avg_epoch_loss=1.008741\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=1.00874096155\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] Epoch[98] Batch [5]#011Speed: 3962.07 samples/sec#011loss=1.008741\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 547.4979877471924, \"sum\": 547.4979877471924, \"min\": 547.4979877471924}}, \"EndTime\": 1601747390.84265, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747390.294843}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1152.32396316 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] #quality_metric: host=algo-1, epoch=98, train loss <loss>=0.988030803204\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:50 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] Epoch[99] Batch[0] avg_epoch_loss=0.970753\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=0.970753252506\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] Epoch[99] Batch[5] avg_epoch_loss=0.947724\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=0.947723845641\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] Epoch[99] Batch [5]#011Speed: 4324.04 samples/sec#011loss=0.947724\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 513.8659477233887, \"sum\": 513.8659477233887, \"min\": 513.8659477233887}}, \"EndTime\": 1601747391.356966, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747390.842712}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1196.56813001 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] #quality_metric: host=algo-1, epoch=99, train loss <loss>=0.932539230585\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_97a490f7-9177-4469-8e92-ca213cefe150-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.575916290283203, \"sum\": 8.575916290283203, \"min\": 8.575916290283203}}, \"EndTime\": 1601747391.36603, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747391.357035}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] Epoch[100] Batch[0] avg_epoch_loss=0.964554\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=0.964553654194\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] Epoch[100] Batch[5] avg_epoch_loss=0.939574\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=0.939574301243\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] Epoch[100] Batch [5]#011Speed: 4276.47 samples/sec#011loss=0.939574\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 527.2359848022461, \"sum\": 527.2359848022461, \"min\": 527.2359848022461}}, \"EndTime\": 1601747391.893379, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747391.366091}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1149.14204748 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] #quality_metric: host=algo-1, epoch=100, train loss <loss>=0.940481293201\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:51 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] Epoch[101] Batch[0] avg_epoch_loss=0.908249\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=0.908248722553\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] Epoch[101] Batch[5] avg_epoch_loss=0.940626\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=0.940625945727\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] Epoch[101] Batch [5]#011Speed: 4317.04 samples/sec#011loss=0.940626\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 518.8820362091064, \"sum\": 518.8820362091064, \"min\": 518.8820362091064}}, \"EndTime\": 1601747392.412754, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747391.893464}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1196.57564348 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] #quality_metric: host=algo-1, epoch=101, train loss <loss>=0.939885306358\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] Epoch[102] Batch[0] avg_epoch_loss=0.933461\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=0.933461129665\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] Epoch[102] Batch[5] avg_epoch_loss=0.969900\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=0.969900478919\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] Epoch[102] Batch [5]#011Speed: 4321.23 samples/sec#011loss=0.969900\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] Epoch[102] Batch[10] avg_epoch_loss=0.972982\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] #quality_metric: host=algo-1, epoch=102, batch=10 train loss <loss>=0.976680111885\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] Epoch[102] Batch [10]#011Speed: 4271.17 samples/sec#011loss=0.976680\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 544.2240238189697, \"sum\": 544.2240238189697, \"min\": 544.2240238189697}}, \"EndTime\": 1601747392.957484, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747392.412821}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1251.11355602 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] #quality_metric: host=algo-1, epoch=102, train loss <loss>=0.972982130267\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:52 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:53 INFO 139787867731776] Epoch[103] Batch[0] avg_epoch_loss=0.883111\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:53 INFO 139787867731776] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=0.883110582829\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:53 INFO 139787867731776] Epoch[103] Batch[5] avg_epoch_loss=0.974150\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:53 INFO 139787867731776] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=0.974149634441\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:53 INFO 139787867731776] Epoch[103] Batch [5]#011Speed: 3659.86 samples/sec#011loss=0.974150\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:53 INFO 139787867731776] Epoch[103] Batch[10] avg_epoch_loss=0.937802\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:53 INFO 139787867731776] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=0.8941847682\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:53 INFO 139787867731776] Epoch[103] Batch [10]#011Speed: 3970.72 samples/sec#011loss=0.894185\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:53 INFO 139787867731776] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 552.6871681213379, \"sum\": 552.6871681213379, \"min\": 552.6871681213379}}, \"EndTime\": 1601747393.510602, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747392.957542}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:53 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1237.370414 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:53 INFO 139787867731776] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:53 INFO 139787867731776] #quality_metric: host=algo-1, epoch=103, train loss <loss>=0.937801967968\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:53 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:53 INFO 139787867731776] Epoch[104] Batch[0] avg_epoch_loss=0.913385\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:53 INFO 139787867731776] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=0.913385391235\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:53 INFO 139787867731776] Epoch[104] Batch[5] avg_epoch_loss=0.941486\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:53 INFO 139787867731776] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=0.941485732794\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:53 INFO 139787867731776] Epoch[104] Batch [5]#011Speed: 3754.37 samples/sec#011loss=0.941486\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:54 INFO 139787867731776] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 527.8289318084717, \"sum\": 527.8289318084717, \"min\": 527.8289318084717}}, \"EndTime\": 1601747394.03889, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747393.510669}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:54 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1168.73555918 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:54 INFO 139787867731776] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:54 INFO 139787867731776] #quality_metric: host=algo-1, epoch=104, train loss <loss>=0.943504190445\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:54 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:54 INFO 139787867731776] Epoch[105] Batch[0] avg_epoch_loss=0.901160\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:54 INFO 139787867731776] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=0.901159644127\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:54 INFO 139787867731776] Epoch[105] Batch[5] avg_epoch_loss=0.955809\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:54 INFO 139787867731776] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=0.955808619658\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:54 INFO 139787867731776] Epoch[105] Batch [5]#011Speed: 4350.60 samples/sec#011loss=0.955809\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:54 INFO 139787867731776] processed a total of 586 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 532.7160358428955, \"sum\": 532.7160358428955, \"min\": 532.7160358428955}}, \"EndTime\": 1601747394.571993, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747394.038952}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:54 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1099.73822436 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:54 INFO 139787867731776] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:54 INFO 139787867731776] #quality_metric: host=algo-1, epoch=105, train loss <loss>=0.956678235531\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:54 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:54 INFO 139787867731776] Epoch[106] Batch[0] avg_epoch_loss=0.956623\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:54 INFO 139787867731776] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=0.956622838974\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] Epoch[106] Batch[5] avg_epoch_loss=0.966026\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=0.966026326021\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] Epoch[106] Batch [5]#011Speed: 4213.36 samples/sec#011loss=0.966026\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 545.6020832061768, \"sum\": 545.6020832061768, \"min\": 545.6020832061768}}, \"EndTime\": 1601747395.118074, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747394.572058}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1136.15127313 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] #quality_metric: host=algo-1, epoch=106, train loss <loss>=0.945708686113\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] Epoch[107] Batch[0] avg_epoch_loss=1.029205\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=1.02920532227\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] Epoch[107] Batch[5] avg_epoch_loss=0.966905\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=0.966905424992\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] Epoch[107] Batch [5]#011Speed: 4311.04 samples/sec#011loss=0.966905\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] Epoch[107] Batch[10] avg_epoch_loss=1.004967\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] #quality_metric: host=algo-1, epoch=107, batch=10 train loss <loss>=1.05064034462\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] Epoch[107] Batch [10]#011Speed: 4250.96 samples/sec#011loss=1.050640\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 551.5210628509521, \"sum\": 551.5210628509521, \"min\": 551.5210628509521}}, \"EndTime\": 1601747395.670119, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747395.118142}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1178.35809445 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] #quality_metric: host=algo-1, epoch=107, train loss <loss>=1.0049667521\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:55 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] Epoch[108] Batch[0] avg_epoch_loss=0.975950\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=0.975949525833\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] Epoch[108] Batch[5] avg_epoch_loss=0.956952\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=0.956951806943\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] Epoch[108] Batch [5]#011Speed: 3913.85 samples/sec#011loss=0.956952\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 519.5209980010986, \"sum\": 519.5209980010986, \"min\": 519.5209980010986}}, \"EndTime\": 1601747396.190038, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747395.67018}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1214.37192846 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=108, train loss <loss>=0.942883688211\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] Epoch[109] Batch[0] avg_epoch_loss=0.980054\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=0.980054199696\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] Epoch[109] Batch[5] avg_epoch_loss=0.948019\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=0.948019464811\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] Epoch[109] Batch [5]#011Speed: 3837.58 samples/sec#011loss=0.948019\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] Epoch[109] Batch[10] avg_epoch_loss=0.934604\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=109, batch=10 train loss <loss>=0.918505620956\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] Epoch[109] Batch [10]#011Speed: 3924.46 samples/sec#011loss=0.918506\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 548.1898784637451, \"sum\": 548.1898784637451, \"min\": 548.1898784637451}}, \"EndTime\": 1601747396.738647, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747396.190099}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1242.0887811 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=109, train loss <loss>=0.934604081241\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:56 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] Epoch[110] Batch[0] avg_epoch_loss=0.842217\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=0.842216908932\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] Epoch[110] Batch[5] avg_epoch_loss=0.901017\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=0.901016831398\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] Epoch[110] Batch [5]#011Speed: 4019.10 samples/sec#011loss=0.901017\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 524.9619483947754, \"sum\": 524.9619483947754, \"min\": 524.9619483947754}}, \"EndTime\": 1601747397.264032, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747396.738698}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1199.8837175 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=110, train loss <loss>=0.89642444849\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_2218a0b6-7c5b-49a9-82eb-c22d338efe39-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.285905838012695, \"sum\": 6.285905838012695, \"min\": 6.285905838012695}}, \"EndTime\": 1601747397.27078, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747397.264095}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] Epoch[111] Batch[0] avg_epoch_loss=0.969884\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=0.969884335995\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] Epoch[111] Batch[5] avg_epoch_loss=0.917895\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=0.917894502481\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] Epoch[111] Batch [5]#011Speed: 4008.01 samples/sec#011loss=0.917895\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] Epoch[111] Batch[10] avg_epoch_loss=0.948349\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=111, batch=10 train loss <loss>=0.984895086288\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] Epoch[111] Batch [10]#011Speed: 4209.09 samples/sec#011loss=0.984895\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 545.1068878173828, \"sum\": 545.1068878173828, \"min\": 545.1068878173828}}, \"EndTime\": 1601747397.81599, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747397.270829}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1181.22024725 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=111, train loss <loss>=0.948349313302\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:57 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] Epoch[112] Batch[0] avg_epoch_loss=1.014941\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=1.0149409771\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] Epoch[112] Batch[5] avg_epoch_loss=0.934410\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=0.934410413106\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] Epoch[112] Batch [5]#011Speed: 4301.93 samples/sec#011loss=0.934410\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] Epoch[112] Batch[10] avg_epoch_loss=0.968331\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=112, batch=10 train loss <loss>=1.00903599262\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] Epoch[112] Batch [10]#011Speed: 4224.15 samples/sec#011loss=1.009036\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 531.3208103179932, \"sum\": 531.3208103179932, \"min\": 531.3208103179932}}, \"EndTime\": 1601747398.347738, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747397.816052}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1257.01179533 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=112, train loss <loss>=0.968331131068\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] Epoch[113] Batch[0] avg_epoch_loss=0.861135\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=0.861135184765\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] Epoch[113] Batch[5] avg_epoch_loss=0.963463\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=0.963462819656\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] Epoch[113] Batch [5]#011Speed: 4298.01 samples/sec#011loss=0.963463\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 514.6260261535645, \"sum\": 514.6260261535645, \"min\": 514.6260261535645}}, \"EndTime\": 1601747398.862808, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747398.347802}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1169.5745053 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=113, train loss <loss>=0.94466085434\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:58 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] Epoch[114] Batch[0] avg_epoch_loss=1.020509\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=1.02050888538\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] Epoch[114] Batch[5] avg_epoch_loss=0.960203\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=0.960203448931\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] Epoch[114] Batch [5]#011Speed: 4165.15 samples/sec#011loss=0.960203\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] Epoch[114] Batch[10] avg_epoch_loss=0.934121\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=114, batch=10 train loss <loss>=0.902821230888\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] Epoch[114] Batch [10]#011Speed: 4330.23 samples/sec#011loss=0.902821\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 527.9679298400879, \"sum\": 527.9679298400879, \"min\": 527.9679298400879}}, \"EndTime\": 1601747399.39124, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747398.86287}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1242.28316938 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=114, train loss <loss>=0.934120622548\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] Epoch[115] Batch[0] avg_epoch_loss=0.936371\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=0.936370670795\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] Epoch[115] Batch[5] avg_epoch_loss=0.891402\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=0.891401787599\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] Epoch[115] Batch [5]#011Speed: 4246.90 samples/sec#011loss=0.891402\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 565.6108856201172, \"sum\": 565.6108856201172, \"min\": 565.6108856201172}}, \"EndTime\": 1601747399.957211, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747399.391302}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1120.72156703 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=115, train loss <loss>=0.906221091747\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:49:59 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:00 INFO 139787867731776] Epoch[116] Batch[0] avg_epoch_loss=0.979862\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=0.979861974716\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:00 INFO 139787867731776] Epoch[116] Batch[5] avg_epoch_loss=0.901977\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=0.901976774136\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:00 INFO 139787867731776] Epoch[116] Batch [5]#011Speed: 3065.46 samples/sec#011loss=0.901977\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:00 INFO 139787867731776] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 601.1240482330322, \"sum\": 601.1240482330322, \"min\": 601.1240482330322}}, \"EndTime\": 1601747400.558726, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747399.957278}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:00 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1059.52301844 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:00 INFO 139787867731776] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=116, train loss <loss>=0.893502926826\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:00 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:00 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_86441d25-1226-463b-8243-2f420bbe6dde-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.462024688720703, \"sum\": 7.462024688720703, \"min\": 7.462024688720703}}, \"EndTime\": 1601747400.566628, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747400.558786}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:00 INFO 139787867731776] Epoch[117] Batch[0] avg_epoch_loss=0.921430\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=0.921430170536\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:01 INFO 139787867731776] Epoch[117] Batch[5] avg_epoch_loss=0.880782\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=0.880781511466\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:01 INFO 139787867731776] Epoch[117] Batch [5]#011Speed: 4038.63 samples/sec#011loss=0.880782\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:01 INFO 139787867731776] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 529.7760963439941, \"sum\": 529.7760963439941, \"min\": 529.7760963439941}}, \"EndTime\": 1601747401.096512, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747400.566685}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:01 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1175.75578597 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:01 INFO 139787867731776] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=117, train loss <loss>=0.870540988445\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:01 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:01 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_fd0e577a-7f2d-4bc8-942a-612818a48cc1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.940126419067383, \"sum\": 6.940126419067383, \"min\": 6.940126419067383}}, \"EndTime\": 1601747401.103924, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747401.096578}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:01 INFO 139787867731776] Epoch[118] Batch[0] avg_epoch_loss=0.957769\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=0.957768857479\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:01 INFO 139787867731776] Epoch[118] Batch[5] avg_epoch_loss=0.971080\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=0.971080362797\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:01 INFO 139787867731776] Epoch[118] Batch [5]#011Speed: 3554.41 samples/sec#011loss=0.971080\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:01 INFO 139787867731776] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 533.9479446411133, \"sum\": 533.9479446411133, \"min\": 533.9479446411133}}, \"EndTime\": 1601747401.637984, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747401.103982}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:01 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1174.05369034 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:01 INFO 139787867731776] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=118, train loss <loss>=0.961836057901\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:01 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] Epoch[119] Batch[0] avg_epoch_loss=0.982312\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=0.982311666012\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] Epoch[119] Batch[5] avg_epoch_loss=0.899214\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=0.899213622014\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] Epoch[119] Batch [5]#011Speed: 3824.60 samples/sec#011loss=0.899214\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] Epoch[119] Batch[10] avg_epoch_loss=0.917318\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=119, batch=10 train loss <loss>=0.939042532444\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] Epoch[119] Batch [10]#011Speed: 3610.39 samples/sec#011loss=0.939043\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 580.0859928131104, \"sum\": 580.0859928131104, \"min\": 580.0859928131104}}, \"EndTime\": 1601747402.218471, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747401.638051}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1113.43029864 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=119, train loss <loss>=0.917317672209\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] Epoch[120] Batch[0] avg_epoch_loss=1.228061\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=1.22806084156\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] Epoch[120] Batch[5] avg_epoch_loss=1.175120\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=1.17511987686\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] Epoch[120] Batch [5]#011Speed: 4020.87 samples/sec#011loss=1.175120\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 525.0091552734375, \"sum\": 525.0091552734375, \"min\": 525.0091552734375}}, \"EndTime\": 1601747402.743921, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747402.218543}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1211.16543836 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=120, train loss <loss>=1.13730434179\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:02 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] Epoch[121] Batch[0] avg_epoch_loss=1.090119\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=1.09011948109\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] Epoch[121] Batch[5] avg_epoch_loss=1.059156\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=1.05915584167\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] Epoch[121] Batch [5]#011Speed: 4032.31 samples/sec#011loss=1.059156\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] Epoch[121] Batch[10] avg_epoch_loss=1.007745\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=121, batch=10 train loss <loss>=0.9460521698\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] Epoch[121] Batch [10]#011Speed: 4397.18 samples/sec#011loss=0.946052\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] processed a total of 729 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 594.2671298980713, \"sum\": 594.2671298980713, \"min\": 594.2671298980713}}, \"EndTime\": 1601747403.338593, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747402.743994}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1226.51930266 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=121, train loss <loss>=1.03060227633\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] Epoch[122] Batch[0] avg_epoch_loss=0.889249\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=0.889249265194\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] Epoch[122] Batch[5] avg_epoch_loss=0.931452\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=0.931452363729\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] Epoch[122] Batch [5]#011Speed: 4371.70 samples/sec#011loss=0.931452\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] Epoch[122] Batch[10] avg_epoch_loss=0.955732\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=122, batch=10 train loss <loss>=0.98486686945\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] Epoch[122] Batch [10]#011Speed: 4317.33 samples/sec#011loss=0.984867\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 582.0720195770264, \"sum\": 582.0720195770264, \"min\": 582.0720195770264}}, \"EndTime\": 1601747403.921071, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747403.338661}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1102.78194083 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=122, train loss <loss>=0.955731684511\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:03 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] Epoch[123] Batch[0] avg_epoch_loss=0.900003\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=0.90000295639\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] Epoch[123] Batch[5] avg_epoch_loss=0.934923\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=0.934923479954\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] Epoch[123] Batch [5]#011Speed: 4039.54 samples/sec#011loss=0.934923\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] Epoch[123] Batch[10] avg_epoch_loss=0.940983\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=123, batch=10 train loss <loss>=0.948255050182\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] Epoch[123] Batch [10]#011Speed: 4121.65 samples/sec#011loss=0.948255\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 543.5061454772949, \"sum\": 543.5061454772949, \"min\": 543.5061454772949}}, \"EndTime\": 1601747404.465029, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747403.92113}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1249.07563403 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=123, train loss <loss>=0.940983284603\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] Epoch[124] Batch[0] avg_epoch_loss=0.865147\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=0.865146875381\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] Epoch[124] Batch[5] avg_epoch_loss=0.861354\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=0.861354490121\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] Epoch[124] Batch [5]#011Speed: 4251.42 samples/sec#011loss=0.861354\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 520.5371379852295, \"sum\": 520.5371379852295, \"min\": 520.5371379852295}}, \"EndTime\": 1601747404.985993, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747404.465093}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1213.90918212 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=124, train loss <loss>=0.872521722317\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:04 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:05 INFO 139787867731776] Epoch[125] Batch[0] avg_epoch_loss=0.839250\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=0.839250206947\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:05 INFO 139787867731776] Epoch[125] Batch[5] avg_epoch_loss=0.908992\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=0.908991823594\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:05 INFO 139787867731776] Epoch[125] Batch [5]#011Speed: 3853.75 samples/sec#011loss=0.908992\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:05 INFO 139787867731776] Epoch[125] Batch[10] avg_epoch_loss=0.879873\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=125, batch=10 train loss <loss>=0.844930684566\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:05 INFO 139787867731776] Epoch[125] Batch [10]#011Speed: 3127.84 samples/sec#011loss=0.844931\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:05 INFO 139787867731776] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 569.9160099029541, \"sum\": 569.9160099029541, \"min\": 569.9160099029541}}, \"EndTime\": 1601747405.556388, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747404.986054}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:05 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1156.14242064 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:05 INFO 139787867731776] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=125, train loss <loss>=0.879873124036\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:05 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:05 INFO 139787867731776] Epoch[126] Batch[0] avg_epoch_loss=0.986907\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=0.986907362938\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] Epoch[126] Batch[5] avg_epoch_loss=0.919932\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=0.919932295879\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] Epoch[126] Batch [5]#011Speed: 3685.28 samples/sec#011loss=0.919932\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] Epoch[126] Batch[10] avg_epoch_loss=0.880041\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=126, batch=10 train loss <loss>=0.832170784473\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] Epoch[126] Batch [10]#011Speed: 4215.74 samples/sec#011loss=0.832171\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 548.1150150299072, \"sum\": 548.1150150299072, \"min\": 548.1150150299072}}, \"EndTime\": 1601747406.105468, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747405.556443}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1251.36128524 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=126, train loss <loss>=0.880040699785\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] Epoch[127] Batch[0] avg_epoch_loss=0.853393\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=0.853392541409\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] Epoch[127] Batch[5] avg_epoch_loss=0.853406\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=0.853406349818\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] Epoch[127] Batch [5]#011Speed: 3563.31 samples/sec#011loss=0.853406\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 528.2590389251709, \"sum\": 528.2590389251709, \"min\": 528.2590389251709}}, \"EndTime\": 1601747406.634189, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747406.105527}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1188.58110732 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=127, train loss <loss>=0.855090194941\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:06 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_1532fe67-902a-48af-8ecf-8d458f756334-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.812023162841797, \"sum\": 7.812023162841797, \"min\": 7.812023162841797}}, \"EndTime\": 1601747406.642476, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747406.634257}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:07 INFO 139787867731776] Epoch[128] Batch[0] avg_epoch_loss=0.776245\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=0.776245057583\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:07 INFO 139787867731776] Epoch[128] Batch[5] avg_epoch_loss=0.867660\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=0.867659777403\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:07 INFO 139787867731776] Epoch[128] Batch [5]#011Speed: 4276.87 samples/sec#011loss=0.867660\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:07 INFO 139787867731776] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 564.2528533935547, \"sum\": 564.2528533935547, \"min\": 564.2528533935547}}, \"EndTime\": 1601747407.206842, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747406.642536}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:07 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1134.06021892 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:07 INFO 139787867731776] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=128, train loss <loss>=0.862135392427\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:07 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:07 INFO 139787867731776] Epoch[129] Batch[0] avg_epoch_loss=0.847874\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=0.847873687744\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:07 INFO 139787867731776] Epoch[129] Batch[5] avg_epoch_loss=0.887270\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=0.887269904216\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:07 INFO 139787867731776] Epoch[129] Batch [5]#011Speed: 4266.79 samples/sec#011loss=0.887270\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:07 INFO 139787867731776] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 523.6918926239014, \"sum\": 523.6918926239014, \"min\": 523.6918926239014}}, \"EndTime\": 1601747407.731056, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747407.206904}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:07 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1162.67731421 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:07 INFO 139787867731776] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=129, train loss <loss>=0.862109428644\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:07 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] Epoch[130] Batch[0] avg_epoch_loss=0.850145\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=0.850144684315\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] Epoch[130] Batch[5] avg_epoch_loss=0.817167\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=0.817166845004\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] Epoch[130] Batch [5]#011Speed: 3871.24 samples/sec#011loss=0.817167\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 534.7039699554443, \"sum\": 534.7039699554443, \"min\": 534.7039699554443}}, \"EndTime\": 1601747408.266164, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747407.731123}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1131.26592989 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=130, train loss <loss>=0.798917639256\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_34cc481b-1a0a-4381-92d7-41fd379ead86-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.835149765014648, \"sum\": 7.835149765014648, \"min\": 7.835149765014648}}, \"EndTime\": 1601747408.274438, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747408.266228}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] Epoch[131] Batch[0] avg_epoch_loss=0.916521\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=0.91652148962\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] Epoch[131] Batch[5] avg_epoch_loss=0.905189\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=0.905189454556\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] Epoch[131] Batch [5]#011Speed: 4312.20 samples/sec#011loss=0.905189\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 527.9319286346436, \"sum\": 527.9319286346436, \"min\": 527.9319286346436}}, \"EndTime\": 1601747408.802485, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747408.2745}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1160.92325948 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=131, train loss <loss>=0.845324027538\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:08 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] Epoch[132] Batch[0] avg_epoch_loss=0.941085\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=0.941085100174\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] Epoch[132] Batch[5] avg_epoch_loss=0.894835\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=0.894835412502\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] Epoch[132] Batch [5]#011Speed: 4327.19 samples/sec#011loss=0.894835\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] Epoch[132] Batch[10] avg_epoch_loss=0.876672\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=132, batch=10 train loss <loss>=0.854875695705\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] Epoch[132] Batch [10]#011Speed: 4232.76 samples/sec#011loss=0.854876\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 564.5489692687988, \"sum\": 564.5489692687988, \"min\": 564.5489692687988}}, \"EndTime\": 1601747409.36748, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747408.802551}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1174.20584151 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=132, train loss <loss>=0.876671904867\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] Epoch[133] Batch[0] avg_epoch_loss=0.821443\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=0.821443259716\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] Epoch[133] Batch[5] avg_epoch_loss=0.868984\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=0.8689836363\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] Epoch[133] Batch [5]#011Speed: 4114.60 samples/sec#011loss=0.868984\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 515.5689716339111, \"sum\": 515.5689716339111, \"min\": 515.5689716339111}}, \"EndTime\": 1601747409.883458, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747409.367539}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1190.71190008 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=133, train loss <loss>=0.864582532644\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:09 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:10 INFO 139787867731776] Epoch[134] Batch[0] avg_epoch_loss=0.851457\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=0.851456940174\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:10 INFO 139787867731776] Epoch[134] Batch[5] avg_epoch_loss=0.865970\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=0.865970045328\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:10 INFO 139787867731776] Epoch[134] Batch [5]#011Speed: 3953.65 samples/sec#011loss=0.865970\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:10 INFO 139787867731776] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 521.5499401092529, \"sum\": 521.5499401092529, \"min\": 521.5499401092529}}, \"EndTime\": 1601747410.405433, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747409.88352}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:10 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1219.23373736 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:10 INFO 139787867731776] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=134, train loss <loss>=0.860854405165\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:10 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:10 INFO 139787867731776] Epoch[135] Batch[0] avg_epoch_loss=0.815739\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=0.81573933363\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:10 INFO 139787867731776] Epoch[135] Batch[5] avg_epoch_loss=0.838549\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=0.838548968236\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:10 INFO 139787867731776] Epoch[135] Batch [5]#011Speed: 3982.38 samples/sec#011loss=0.838549\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:10 INFO 139787867731776] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 549.3700504302979, \"sum\": 549.3700504302979, \"min\": 549.3700504302979}}, \"EndTime\": 1601747410.955277, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747410.405492}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:10 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1146.5903665 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:10 INFO 139787867731776] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=135, train loss <loss>=0.839320880175\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:10 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:11 INFO 139787867731776] Epoch[136] Batch[0] avg_epoch_loss=0.822099\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=0.82209867239\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:11 INFO 139787867731776] Epoch[136] Batch[5] avg_epoch_loss=0.865147\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=0.865147382021\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:11 INFO 139787867731776] Epoch[136] Batch [5]#011Speed: 4330.30 samples/sec#011loss=0.865147\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:11 INFO 139787867731776] Epoch[136] Batch[10] avg_epoch_loss=0.840204\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=136, batch=10 train loss <loss>=0.81027173996\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:11 INFO 139787867731776] Epoch[136] Batch [10]#011Speed: 4314.24 samples/sec#011loss=0.810272\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:11 INFO 139787867731776] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 529.8330783843994, \"sum\": 529.8330783843994, \"min\": 529.8330783843994}}, \"EndTime\": 1601747411.485568, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747410.955337}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:11 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1211.487585 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:11 INFO 139787867731776] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=136, train loss <loss>=0.840203908357\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:11 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:11 INFO 139787867731776] Epoch[137] Batch[0] avg_epoch_loss=0.817559\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=0.817559480667\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:11 INFO 139787867731776] Epoch[137] Batch[5] avg_epoch_loss=0.846430\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=0.846430341403\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:11 INFO 139787867731776] Epoch[137] Batch [5]#011Speed: 4031.76 samples/sec#011loss=0.846430\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] Epoch[137] Batch[10] avg_epoch_loss=0.841297\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=137, batch=10 train loss <loss>=0.835137832165\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] Epoch[137] Batch [10]#011Speed: 4201.08 samples/sec#011loss=0.835138\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 571.3241100311279, \"sum\": 571.3241100311279, \"min\": 571.3241100311279}}, \"EndTime\": 1601747412.057256, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747411.485634}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1137.52332378 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=137, train loss <loss>=0.841297382658\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] Epoch[138] Batch[0] avg_epoch_loss=0.705539\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=0.705538511276\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] Epoch[138] Batch[5] avg_epoch_loss=0.827878\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=0.827878117561\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] Epoch[138] Batch [5]#011Speed: 3837.07 samples/sec#011loss=0.827878\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] Epoch[138] Batch[10] avg_epoch_loss=0.861985\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=138, batch=10 train loss <loss>=0.902914237976\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] Epoch[138] Batch [10]#011Speed: 3623.64 samples/sec#011loss=0.902914\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 568.0978298187256, \"sum\": 568.0978298187256, \"min\": 568.0978298187256}}, \"EndTime\": 1601747412.625719, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747412.05732}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1184.44633863 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=138, train loss <loss>=0.861985445023\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:12 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] Epoch[139] Batch[0] avg_epoch_loss=0.909478\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=0.90947842598\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] Epoch[139] Batch[5] avg_epoch_loss=0.862816\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=0.862815956275\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] Epoch[139] Batch [5]#011Speed: 3663.17 samples/sec#011loss=0.862816\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] Epoch[139] Batch[10] avg_epoch_loss=0.870867\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=139, batch=10 train loss <loss>=0.880529344082\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] Epoch[139] Batch [10]#011Speed: 3972.30 samples/sec#011loss=0.880529\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 558.2408905029297, \"sum\": 558.2408905029297, \"min\": 558.2408905029297}}, \"EndTime\": 1601747413.184387, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747412.625786}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1198.20319058 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=139, train loss <loss>=0.870867496187\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] Epoch[140] Batch[0] avg_epoch_loss=0.829323\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=0.829323232174\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] Epoch[140] Batch[5] avg_epoch_loss=0.841413\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=0.841412524382\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] Epoch[140] Batch [5]#011Speed: 4014.30 samples/sec#011loss=0.841413\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 544.9390411376953, \"sum\": 544.9390411376953, \"min\": 544.9390411376953}}, \"EndTime\": 1601747413.729719, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747413.184452}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1166.91003483 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=140, train loss <loss>=0.825407606363\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:13 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] Epoch[141] Batch[0] avg_epoch_loss=0.943234\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=0.943234324455\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] Epoch[141] Batch[5] avg_epoch_loss=0.867967\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=0.867967416843\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] Epoch[141] Batch [5]#011Speed: 3657.43 samples/sec#011loss=0.867967\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] Epoch[141] Batch[10] avg_epoch_loss=0.883313\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=141, batch=10 train loss <loss>=0.901727247238\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] Epoch[141] Batch [10]#011Speed: 3948.13 samples/sec#011loss=0.901727\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 548.9850044250488, \"sum\": 548.9850044250488, \"min\": 548.9850044250488}}, \"EndTime\": 1601747414.279182, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747413.729778}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1169.23281285 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=141, train loss <loss>=0.883312794295\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] Epoch[142] Batch[0] avg_epoch_loss=0.892512\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=0.892511606216\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] Epoch[142] Batch[5] avg_epoch_loss=0.842376\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=0.842376470566\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] Epoch[142] Batch [5]#011Speed: 3550.07 samples/sec#011loss=0.842376\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 564.262866973877, \"sum\": 564.262866973877, \"min\": 564.262866973877}}, \"EndTime\": 1601747414.84381, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747414.279245}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1084.40843391 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=142, train loss <loss>=0.848723548651\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:14 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] Epoch[143] Batch[0] avg_epoch_loss=0.783930\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=0.783929944038\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] Epoch[143] Batch[5] avg_epoch_loss=0.830475\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=0.830474644899\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] Epoch[143] Batch [5]#011Speed: 4290.35 samples/sec#011loss=0.830475\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 515.4249668121338, \"sum\": 515.4249668121338, \"min\": 515.4249668121338}}, \"EndTime\": 1601747415.359694, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747414.843877}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1235.65504703 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=143, train loss <loss>=0.83047645092\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] Epoch[144] Batch[0] avg_epoch_loss=0.796439\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=0.796439468861\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] Epoch[144] Batch[5] avg_epoch_loss=0.831770\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=0.831770211458\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] Epoch[144] Batch [5]#011Speed: 4323.22 samples/sec#011loss=0.831770\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] Epoch[144] Batch[10] avg_epoch_loss=0.840943\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=144, batch=10 train loss <loss>=0.851949751377\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] Epoch[144] Batch [10]#011Speed: 4257.12 samples/sec#011loss=0.851950\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 541.9080257415771, \"sum\": 541.9080257415771, \"min\": 541.9080257415771}}, \"EndTime\": 1601747415.902099, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747415.359758}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1215.8606165 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=144, train loss <loss>=0.840942729603\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:15 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:16 INFO 139787867731776] Epoch[145] Batch[0] avg_epoch_loss=0.877523\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=0.877523303032\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:16 INFO 139787867731776] Epoch[145] Batch[5] avg_epoch_loss=0.841693\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=0.841692815224\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:16 INFO 139787867731776] Epoch[145] Batch [5]#011Speed: 4319.82 samples/sec#011loss=0.841693\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:16 INFO 139787867731776] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 511.35993003845215, \"sum\": 511.35993003845215, \"min\": 511.35993003845215}}, \"EndTime\": 1601747416.413889, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747415.902162}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:16 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1220.03944408 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:16 INFO 139787867731776] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=145, train loss <loss>=0.836392962933\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:16 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:16 INFO 139787867731776] Epoch[146] Batch[0] avg_epoch_loss=0.700060\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=0.700059950352\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:16 INFO 139787867731776] Epoch[146] Batch[5] avg_epoch_loss=0.804482\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=0.804481774569\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:16 INFO 139787867731776] Epoch[146] Batch [5]#011Speed: 4296.81 samples/sec#011loss=0.804482\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:16 INFO 139787867731776] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 536.0109806060791, \"sum\": 536.0109806060791, \"min\": 536.0109806060791}}, \"EndTime\": 1601747416.950303, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747416.413958}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:16 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1165.81242954 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:16 INFO 139787867731776] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=146, train loss <loss>=0.813461339474\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:16 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:17 INFO 139787867731776] Epoch[147] Batch[0] avg_epoch_loss=0.850813\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=0.850813210011\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:17 INFO 139787867731776] Epoch[147] Batch[5] avg_epoch_loss=0.790458\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=0.790458043416\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:17 INFO 139787867731776] Epoch[147] Batch [5]#011Speed: 4060.30 samples/sec#011loss=0.790458\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:17 INFO 139787867731776] Epoch[147] Batch[10] avg_epoch_loss=0.770193\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=147, batch=10 train loss <loss>=0.745875823498\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:17 INFO 139787867731776] Epoch[147] Batch [10]#011Speed: 4212.85 samples/sec#011loss=0.745876\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:17 INFO 139787867731776] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 545.0038909912109, \"sum\": 545.0038909912109, \"min\": 545.0038909912109}}, \"EndTime\": 1601747417.495729, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747416.950369}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:17 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1201.62074756 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:17 INFO 139787867731776] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=147, train loss <loss>=0.770193397999\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:17 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:17 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_e6f64c5d-7a36-480d-9bb3-6c806e2b5181-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.206989288330078, \"sum\": 6.206989288330078, \"min\": 6.206989288330078}}, \"EndTime\": 1601747417.502442, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747417.495788}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:17 INFO 139787867731776] Epoch[148] Batch[0] avg_epoch_loss=0.779532\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=0.779532492161\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:17 INFO 139787867731776] Epoch[148] Batch[5] avg_epoch_loss=0.836872\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=0.836872041225\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:17 INFO 139787867731776] Epoch[148] Batch [5]#011Speed: 3834.58 samples/sec#011loss=0.836872\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:18 INFO 139787867731776] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 528.770923614502, \"sum\": 528.770923614502, \"min\": 528.770923614502}}, \"EndTime\": 1601747418.031331, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747417.502507}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:18 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1183.65761654 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:18 INFO 139787867731776] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=148, train loss <loss>=0.825698286295\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:18 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:18 INFO 139787867731776] Epoch[149] Batch[0] avg_epoch_loss=0.915829\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=0.91582852602\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:18 INFO 139787867731776] Epoch[149] Batch[5] avg_epoch_loss=0.795008\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=0.795007695754\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:18 INFO 139787867731776] Epoch[149] Batch [5]#011Speed: 4296.69 samples/sec#011loss=0.795008\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:18 INFO 139787867731776] Epoch[149] Batch[10] avg_epoch_loss=0.779565\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=149, batch=10 train loss <loss>=0.761034560204\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:18 INFO 139787867731776] Epoch[149] Batch [10]#011Speed: 4081.43 samples/sec#011loss=0.761035\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:18 INFO 139787867731776] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 586.7741107940674, \"sum\": 586.7741107940674, \"min\": 586.7741107940674}}, \"EndTime\": 1601747418.618521, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747418.031399}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:18 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1156.99344219 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:18 INFO 139787867731776] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=149, train loss <loss>=0.779565361413\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:18 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] Epoch[150] Batch[0] avg_epoch_loss=0.876310\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=0.876310348511\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] Epoch[150] Batch[5] avg_epoch_loss=0.826843\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=0.826843430599\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] Epoch[150] Batch [5]#011Speed: 4210.08 samples/sec#011loss=0.826843\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] Epoch[150] Batch[10] avg_epoch_loss=0.767681\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=150, batch=10 train loss <loss>=0.696685904264\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] Epoch[150] Batch [10]#011Speed: 4265.99 samples/sec#011loss=0.696686\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 566.403865814209, \"sum\": 566.403865814209, \"min\": 566.403865814209}}, \"EndTime\": 1601747419.1853, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747418.618584}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1135.03982075 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=150, train loss <loss>=0.767680918629\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_05617bc4-a97a-4e06-8e89-382cce62df1a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.589174270629883, \"sum\": 6.589174270629883, \"min\": 6.589174270629883}}, \"EndTime\": 1601747419.192378, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747419.185365}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] Epoch[151] Batch[0] avg_epoch_loss=0.744122\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=0.744121789932\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] Epoch[151] Batch[5] avg_epoch_loss=0.791274\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=0.791273643573\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] Epoch[151] Batch [5]#011Speed: 4142.66 samples/sec#011loss=0.791274\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 511.8119716644287, \"sum\": 511.8119716644287, \"min\": 511.8119716644287}}, \"EndTime\": 1601747419.704302, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747419.192435}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1220.92237988 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=151, train loss <loss>=0.763374096155\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:19 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_4900eec2-1824-4c24-ac21-b1c7c7b2eb03-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.956027984619141, \"sum\": 7.956027984619141, \"min\": 7.956027984619141}}, \"EndTime\": 1601747419.71276, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747419.704367}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] Epoch[152] Batch[0] avg_epoch_loss=0.842826\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=0.842826068401\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] Epoch[152] Batch[5] avg_epoch_loss=0.794074\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=0.794073909521\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] Epoch[152] Batch [5]#011Speed: 4355.53 samples/sec#011loss=0.794074\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] Epoch[152] Batch[10] avg_epoch_loss=0.777967\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=152, batch=10 train loss <loss>=0.758637666702\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] Epoch[152] Batch [10]#011Speed: 3768.30 samples/sec#011loss=0.758638\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 591.9511318206787, \"sum\": 591.9511318206787, \"min\": 591.9511318206787}}, \"EndTime\": 1601747420.304831, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747419.712823}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1108.02553949 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=152, train loss <loss>=0.777966526422\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] Epoch[153] Batch[0] avg_epoch_loss=0.780710\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=0.780710101128\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] Epoch[153] Batch[5] avg_epoch_loss=0.795422\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=0.795422077179\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] Epoch[153] Batch [5]#011Speed: 3976.20 samples/sec#011loss=0.795422\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] Epoch[153] Batch[10] avg_epoch_loss=0.745227\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=153, batch=10 train loss <loss>=0.684993082285\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] Epoch[153] Batch [10]#011Speed: 4236.83 samples/sec#011loss=0.684993\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 547.9190349578857, \"sum\": 547.9190349578857, \"min\": 547.9190349578857}}, \"EndTime\": 1601747420.853208, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747420.304892}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1178.80421295 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=153, train loss <loss>=0.7452270795\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:20 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_4d181147-5c2b-4e2c-9f50-2af536509ff0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.115053176879883, \"sum\": 8.115053176879883, \"min\": 8.115053176879883}}, \"EndTime\": 1601747420.861722, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747420.853271}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] Epoch[154] Batch[0] avg_epoch_loss=1.074111\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=1.07411122322\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] Epoch[154] Batch[5] avg_epoch_loss=0.953240\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=0.953239917755\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] Epoch[154] Batch [5]#011Speed: 4255.09 samples/sec#011loss=0.953240\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] processed a total of 578 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 558.5439205169678, \"sum\": 558.5439205169678, \"min\": 558.5439205169678}}, \"EndTime\": 1601747421.420381, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747420.861784}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1034.65381992 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=154, train loss <loss>=0.891109418869\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] Epoch[155] Batch[0] avg_epoch_loss=0.974837\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=0.974837243557\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] Epoch[155] Batch[5] avg_epoch_loss=0.886190\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=0.886189639568\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] Epoch[155] Batch [5]#011Speed: 3945.70 samples/sec#011loss=0.886190\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] Epoch[155] Batch[10] avg_epoch_loss=0.857413\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=155, batch=10 train loss <loss>=0.822881829739\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] Epoch[155] Batch [10]#011Speed: 4275.62 samples/sec#011loss=0.822882\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 569.0140724182129, \"sum\": 569.0140724182129, \"min\": 569.0140724182129}}, \"EndTime\": 1601747421.989838, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747421.420447}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1157.95630673 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=155, train loss <loss>=0.857413362373\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:21 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:22 INFO 139787867731776] Epoch[156] Batch[0] avg_epoch_loss=0.883432\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=0.88343167305\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:22 INFO 139787867731776] Epoch[156] Batch[5] avg_epoch_loss=0.852896\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=0.852896094322\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:22 INFO 139787867731776] Epoch[156] Batch [5]#011Speed: 4330.71 samples/sec#011loss=0.852896\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:22 INFO 139787867731776] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 524.9888896942139, \"sum\": 524.9888896942139, \"min\": 524.9888896942139}}, \"EndTime\": 1601747422.515216, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747421.989901}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:22 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1173.13866349 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:22 INFO 139787867731776] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=156, train loss <loss>=0.827782595158\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:22 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:22 INFO 139787867731776] Epoch[157] Batch[0] avg_epoch_loss=0.761336\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=0.761335670948\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:22 INFO 139787867731776] Epoch[157] Batch[5] avg_epoch_loss=0.788453\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=0.788453112046\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:22 INFO 139787867731776] Epoch[157] Batch [5]#011Speed: 4290.12 samples/sec#011loss=0.788453\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:23 INFO 139787867731776] processed a total of 575 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 502.6400089263916, \"sum\": 502.6400089263916, \"min\": 502.6400089263916}}, \"EndTime\": 1601747423.018349, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747422.515279}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:23 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1143.73689911 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:23 INFO 139787867731776] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=157, train loss <loss>=0.795340869162\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:23 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:23 INFO 139787867731776] Epoch[158] Batch[0] avg_epoch_loss=0.730836\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=0.730836451054\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:23 INFO 139787867731776] Epoch[158] Batch[5] avg_epoch_loss=0.780531\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=0.7805313766\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:23 INFO 139787867731776] Epoch[158] Batch [5]#011Speed: 4323.11 samples/sec#011loss=0.780531\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:23 INFO 139787867731776] Epoch[158] Batch[10] avg_epoch_loss=0.773798\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=158, batch=10 train loss <loss>=0.765718972683\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:23 INFO 139787867731776] Epoch[158] Batch [10]#011Speed: 4017.38 samples/sec#011loss=0.765719\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:23 INFO 139787867731776] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 586.230993270874, \"sum\": 586.230993270874, \"min\": 586.230993270874}}, \"EndTime\": 1601747423.604987, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747423.018415}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:23 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1149.53511539 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:23 INFO 139787867731776] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=158, train loss <loss>=0.773798465729\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:23 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:23 INFO 139787867731776] Epoch[159] Batch[0] avg_epoch_loss=0.848474\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=0.848474085331\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] Epoch[159] Batch[5] avg_epoch_loss=0.760810\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=0.760810236136\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] Epoch[159] Batch [5]#011Speed: 4163.36 samples/sec#011loss=0.760810\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] Epoch[159] Batch[10] avg_epoch_loss=0.894922\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=159, batch=10 train loss <loss>=1.05585643053\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] Epoch[159] Batch [10]#011Speed: 4092.64 samples/sec#011loss=1.055856\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 542.4318313598633, \"sum\": 542.4318313598633, \"min\": 542.4318313598633}}, \"EndTime\": 1601747424.147834, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747423.605047}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1187.05082404 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=159, train loss <loss>=0.894922142679\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] Epoch[160] Batch[0] avg_epoch_loss=0.853855\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=0.853855073452\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] Epoch[160] Batch[5] avg_epoch_loss=0.818471\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=0.818471421798\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] Epoch[160] Batch [5]#011Speed: 3617.46 samples/sec#011loss=0.818471\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 531.019926071167, \"sum\": 531.019926071167, \"min\": 531.019926071167}}, \"EndTime\": 1601747424.679236, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747424.147896}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1205.0346918 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=160, train loss <loss>=0.7805300951\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:24 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] Epoch[161] Batch[0] avg_epoch_loss=0.819126\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=0.819125950336\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] Epoch[161] Batch[5] avg_epoch_loss=0.750870\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=0.75086971124\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] Epoch[161] Batch [5]#011Speed: 3973.91 samples/sec#011loss=0.750870\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] Epoch[161] Batch[10] avg_epoch_loss=0.763203\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=161, batch=10 train loss <loss>=0.778001928329\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] Epoch[161] Batch [10]#011Speed: 4280.24 samples/sec#011loss=0.778002\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 551.5191555023193, \"sum\": 551.5191555023193, \"min\": 551.5191555023193}}, \"EndTime\": 1601747425.231197, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747424.679291}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1196.50374653 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=161, train loss <loss>=0.76320253719\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] Epoch[162] Batch[0] avg_epoch_loss=0.944176\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=0.944175958633\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] Epoch[162] Batch[5] avg_epoch_loss=0.884344\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=0.884344418844\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] Epoch[162] Batch [5]#011Speed: 3489.95 samples/sec#011loss=0.884344\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] processed a total of 584 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 544.8050498962402, \"sum\": 544.8050498962402, \"min\": 544.8050498962402}}, \"EndTime\": 1601747425.776769, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747425.231257}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1071.01926369 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=162, train loss <loss>=0.86441552639\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:25 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] Epoch[163] Batch[0] avg_epoch_loss=1.079705\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=1.07970535755\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] Epoch[163] Batch[5] avg_epoch_loss=1.044599\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=1.04459885756\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] Epoch[163] Batch [5]#011Speed: 4194.03 samples/sec#011loss=1.044599\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] Epoch[163] Batch[10] avg_epoch_loss=0.972041\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=163, batch=10 train loss <loss>=0.884970796108\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] Epoch[163] Batch [10]#011Speed: 4292.32 samples/sec#011loss=0.884971\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] processed a total of 716 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 564.4729137420654, \"sum\": 564.4729137420654, \"min\": 564.4729137420654}}, \"EndTime\": 1601747426.341813, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747425.776849}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1268.18820451 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=163, train loss <loss>=0.962408388654\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] Epoch[164] Batch[0] avg_epoch_loss=0.868683\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=0.868682980537\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] Epoch[164] Batch[5] avg_epoch_loss=0.882202\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=0.882201939821\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] Epoch[164] Batch [5]#011Speed: 4361.53 samples/sec#011loss=0.882202\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 513.5939121246338, \"sum\": 513.5939121246338, \"min\": 513.5939121246338}}, \"EndTime\": 1601747426.856158, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747426.341892}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1187.47505445 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=164, train loss <loss>=0.882402783632\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:26 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] Epoch[165] Batch[0] avg_epoch_loss=0.995530\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=0.995529711246\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] Epoch[165] Batch[5] avg_epoch_loss=0.851711\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=0.851710548004\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] Epoch[165] Batch [5]#011Speed: 4303.22 samples/sec#011loss=0.851711\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 527.0779132843018, \"sum\": 527.0779132843018, \"min\": 527.0779132843018}}, \"EndTime\": 1601747427.383683, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747426.856223}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1183.66349304 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=165, train loss <loss>=0.837498658895\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] Epoch[166] Batch[0] avg_epoch_loss=0.783290\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=0.783290028572\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] Epoch[166] Batch[5] avg_epoch_loss=0.780228\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=0.780227671067\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] Epoch[166] Batch [5]#011Speed: 3833.99 samples/sec#011loss=0.780228\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] Epoch[166] Batch[10] avg_epoch_loss=0.787214\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=166, batch=10 train loss <loss>=0.795597970486\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] Epoch[166] Batch [10]#011Speed: 3848.75 samples/sec#011loss=0.795598\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] processed a total of 703 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 559.3068599700928, \"sum\": 559.3068599700928, \"min\": 559.3068599700928}}, \"EndTime\": 1601747427.943487, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747427.38375}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1256.68361028 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=166, train loss <loss>=0.787214170803\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:27 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:28 INFO 139787867731776] Epoch[167] Batch[0] avg_epoch_loss=0.838983\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=0.838982820511\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:28 INFO 139787867731776] Epoch[167] Batch[5] avg_epoch_loss=0.741345\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=0.741345137358\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:28 INFO 139787867731776] Epoch[167] Batch [5]#011Speed: 4108.99 samples/sec#011loss=0.741345\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:28 INFO 139787867731776] Epoch[167] Batch[10] avg_epoch_loss=0.698503\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=167, batch=10 train loss <loss>=0.647092929482\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:28 INFO 139787867731776] Epoch[167] Batch [10]#011Speed: 4246.85 samples/sec#011loss=0.647093\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:28 INFO 139787867731776] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 539.7820472717285, \"sum\": 539.7820472717285, \"min\": 539.7820472717285}}, \"EndTime\": 1601747428.483644, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747427.943558}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:28 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1209.54612087 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:28 INFO 139787867731776] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=167, train loss <loss>=0.698503224687\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:28 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:28 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_67d049f8-1418-43e2-a013-4c2d89f8234e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.175041198730469, \"sum\": 6.175041198730469, \"min\": 6.175041198730469}}, \"EndTime\": 1601747428.490298, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747428.483703}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:28 INFO 139787867731776] Epoch[168] Batch[0] avg_epoch_loss=0.737296\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=0.737295985222\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:28 INFO 139787867731776] Epoch[168] Batch[5] avg_epoch_loss=0.772223\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=0.772223065297\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:28 INFO 139787867731776] Epoch[168] Batch [5]#011Speed: 4091.28 samples/sec#011loss=0.772223\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] Epoch[168] Batch[10] avg_epoch_loss=0.749475\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=168, batch=10 train loss <loss>=0.722177040577\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] Epoch[168] Batch [10]#011Speed: 3928.17 samples/sec#011loss=0.722177\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 541.5921211242676, \"sum\": 541.5921211242676, \"min\": 541.5921211242676}}, \"EndTime\": 1601747429.032004, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747428.490354}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1201.80101408 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=168, train loss <loss>=0.749474872242\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] Epoch[169] Batch[0] avg_epoch_loss=0.797652\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=0.797652363777\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] Epoch[169] Batch[5] avg_epoch_loss=0.775406\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=0.775405675173\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] Epoch[169] Batch [5]#011Speed: 4371.70 samples/sec#011loss=0.775406\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] Epoch[169] Batch[10] avg_epoch_loss=0.786334\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=169, batch=10 train loss <loss>=0.799447393417\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] Epoch[169] Batch [10]#011Speed: 4309.82 samples/sec#011loss=0.799447\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 527.1260738372803, \"sum\": 527.1260738372803, \"min\": 527.1260738372803}}, \"EndTime\": 1601747429.55958, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747429.032065}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1219.60965798 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=169, train loss <loss>=0.78633372892\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] Epoch[170] Batch[0] avg_epoch_loss=0.861641\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=0.861640930176\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] Epoch[170] Batch[5] avg_epoch_loss=0.824307\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=0.824307024479\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] Epoch[170] Batch [5]#011Speed: 4074.72 samples/sec#011loss=0.824307\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] Epoch[170] Batch[10] avg_epoch_loss=0.831436\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=170, batch=10 train loss <loss>=0.839990258217\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] Epoch[170] Batch [10]#011Speed: 4049.96 samples/sec#011loss=0.839990\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 560.1627826690674, \"sum\": 560.1627826690674, \"min\": 560.1627826690674}}, \"EndTime\": 1601747430.120098, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747429.559643}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1165.53661792 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=170, train loss <loss>=0.831435767087\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] Epoch[171] Batch[0] avg_epoch_loss=0.689812\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=0.689812123775\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] Epoch[171] Batch[5] avg_epoch_loss=0.826634\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=0.826634407043\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] Epoch[171] Batch [5]#011Speed: 4307.21 samples/sec#011loss=0.826634\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] Epoch[171] Batch[10] avg_epoch_loss=0.844178\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=171, batch=10 train loss <loss>=0.865230429173\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] Epoch[171] Batch [10]#011Speed: 4262.91 samples/sec#011loss=0.865230\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 577.9531002044678, \"sum\": 577.9531002044678, \"min\": 577.9531002044678}}, \"EndTime\": 1601747430.698409, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747430.120162}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1126.20407018 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=171, train loss <loss>=0.844178053466\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:30 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] Epoch[172] Batch[0] avg_epoch_loss=0.940720\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=0.940719544888\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] Epoch[172] Batch[5] avg_epoch_loss=0.821306\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=0.821305572987\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] Epoch[172] Batch [5]#011Speed: 4204.28 samples/sec#011loss=0.821306\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] Epoch[172] Batch[10] avg_epoch_loss=0.807798\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=172, batch=10 train loss <loss>=0.791589868069\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] Epoch[172] Batch [10]#011Speed: 4327.08 samples/sec#011loss=0.791590\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 560.5709552764893, \"sum\": 560.5709552764893, \"min\": 560.5709552764893}}, \"EndTime\": 1601747431.259347, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747430.698473}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1153.99093125 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=172, train loss <loss>=0.807798434388\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] Epoch[173] Batch[0] avg_epoch_loss=0.848692\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=0.84869235754\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] Epoch[173] Batch[5] avg_epoch_loss=0.787455\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=0.787455439568\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] Epoch[173] Batch [5]#011Speed: 3704.04 samples/sec#011loss=0.787455\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 546.6601848602295, \"sum\": 546.6601848602295, \"min\": 546.6601848602295}}, \"EndTime\": 1601747431.806365, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747431.25941}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1122.97687594 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=173, train loss <loss>=0.774005383253\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:31 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:32 INFO 139787867731776] Epoch[174] Batch[0] avg_epoch_loss=0.711751\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=0.711751103401\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:32 INFO 139787867731776] Epoch[174] Batch[5] avg_epoch_loss=0.783442\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=0.783441652854\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:32 INFO 139787867731776] Epoch[174] Batch [5]#011Speed: 3857.42 samples/sec#011loss=0.783442\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:32 INFO 139787867731776] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 553.3120632171631, \"sum\": 553.3120632171631, \"min\": 553.3120632171631}}, \"EndTime\": 1601747432.360079, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747431.806434}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:32 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1102.261084 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:32 INFO 139787867731776] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=174, train loss <loss>=0.743265354633\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:32 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:32 INFO 139787867731776] Epoch[175] Batch[0] avg_epoch_loss=0.724235\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=0.724235117435\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:32 INFO 139787867731776] Epoch[175] Batch[5] avg_epoch_loss=0.768192\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=0.768192221721\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:32 INFO 139787867731776] Epoch[175] Batch [5]#011Speed: 4186.95 samples/sec#011loss=0.768192\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:32 INFO 139787867731776] processed a total of 566 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 547.814130783081, \"sum\": 547.814130783081, \"min\": 547.814130783081}}, \"EndTime\": 1601747432.9083, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747432.360144}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:32 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1033.02945426 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:32 INFO 139787867731776] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=175, train loss <loss>=0.777352048291\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:32 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:33 INFO 139787867731776] Epoch[176] Batch[0] avg_epoch_loss=0.712178\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=0.712177693844\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:33 INFO 139787867731776] Epoch[176] Batch[5] avg_epoch_loss=0.757498\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=0.75749823451\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:33 INFO 139787867731776] Epoch[176] Batch [5]#011Speed: 3956.72 samples/sec#011loss=0.757498\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:33 INFO 139787867731776] processed a total of 600 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 525.0849723815918, \"sum\": 525.0849723815918, \"min\": 525.0849723815918}}, \"EndTime\": 1601747433.433891, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747432.908362}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:33 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1142.45432829 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:33 INFO 139787867731776] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=176, train loss <loss>=0.755617362261\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:33 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:33 INFO 139787867731776] Epoch[177] Batch[0] avg_epoch_loss=0.857858\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=0.857857823372\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:33 INFO 139787867731776] Epoch[177] Batch[5] avg_epoch_loss=0.759059\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=0.759058992068\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:33 INFO 139787867731776] Epoch[177] Batch [5]#011Speed: 3492.81 samples/sec#011loss=0.759059\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:34 INFO 139787867731776] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 573.7090110778809, \"sum\": 573.7090110778809, \"min\": 573.7090110778809}}, \"EndTime\": 1601747434.008029, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747433.433957}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:34 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1064.74186597 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:34 INFO 139787867731776] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=177, train loss <loss>=0.748167639971\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:34 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:34 INFO 139787867731776] Epoch[178] Batch[0] avg_epoch_loss=0.760409\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=178, batch=0 train loss <loss>=0.760409355164\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:34 INFO 139787867731776] Epoch[178] Batch[5] avg_epoch_loss=0.806639\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=178, batch=5 train loss <loss>=0.806638578574\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:34 INFO 139787867731776] Epoch[178] Batch [5]#011Speed: 4054.18 samples/sec#011loss=0.806639\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:34 INFO 139787867731776] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 544.0549850463867, \"sum\": 544.0549850463867, \"min\": 544.0549850463867}}, \"EndTime\": 1601747434.552915, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747434.00811}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:34 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1155.91887152 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:34 INFO 139787867731776] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=178, train loss <loss>=0.787015378475\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:34 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:34 INFO 139787867731776] Epoch[179] Batch[0] avg_epoch_loss=0.909695\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=179, batch=0 train loss <loss>=0.909694671631\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] Epoch[179] Batch[5] avg_epoch_loss=0.820352\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=179, batch=5 train loss <loss>=0.820352127155\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] Epoch[179] Batch [5]#011Speed: 3734.69 samples/sec#011loss=0.820352\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] Epoch[179] Batch[10] avg_epoch_loss=0.747580\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=179, batch=10 train loss <loss>=0.66025415659\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] Epoch[179] Batch [10]#011Speed: 4189.96 samples/sec#011loss=0.660254\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 539.9680137634277, \"sum\": 539.9680137634277, \"min\": 539.9680137634277}}, \"EndTime\": 1601747435.093398, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747434.552977}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1201.70974293 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=179, train loss <loss>=0.747580322352\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] Epoch[180] Batch[0] avg_epoch_loss=0.723318\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=180, batch=0 train loss <loss>=0.723317503929\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] Epoch[180] Batch[5] avg_epoch_loss=0.762899\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=180, batch=5 train loss <loss>=0.762899398804\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] Epoch[180] Batch [5]#011Speed: 4078.61 samples/sec#011loss=0.762899\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] Epoch[180] Batch[10] avg_epoch_loss=0.767189\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=180, batch=10 train loss <loss>=0.772336053848\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] Epoch[180] Batch [10]#011Speed: 4058.29 samples/sec#011loss=0.772336\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 596.1449146270752, \"sum\": 596.1449146270752, \"min\": 596.1449146270752}}, \"EndTime\": 1601747435.689904, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747435.093463}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1122.02951996 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=180, train loss <loss>=0.76718878746\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:35 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] Epoch[181] Batch[0] avg_epoch_loss=0.732455\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] #quality_metric: host=algo-1, epoch=181, batch=0 train loss <loss>=0.732454895973\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] Epoch[181] Batch[5] avg_epoch_loss=0.740639\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] #quality_metric: host=algo-1, epoch=181, batch=5 train loss <loss>=0.740638653437\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] Epoch[181] Batch [5]#011Speed: 4273.51 samples/sec#011loss=0.740639\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] Epoch[181] Batch[10] avg_epoch_loss=0.796624\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] #quality_metric: host=algo-1, epoch=181, batch=10 train loss <loss>=0.863805651665\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] Epoch[181] Batch [10]#011Speed: 4180.33 samples/sec#011loss=0.863806\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 550.0750541687012, \"sum\": 550.0750541687012, \"min\": 550.0750541687012}}, \"EndTime\": 1601747436.240417, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747435.689969}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1179.64368312 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] #quality_metric: host=algo-1, epoch=181, train loss <loss>=0.796623652632\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] Epoch[182] Batch[0] avg_epoch_loss=0.706629\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] #quality_metric: host=algo-1, epoch=182, batch=0 train loss <loss>=0.706628859043\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] Epoch[182] Batch[5] avg_epoch_loss=0.780014\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] #quality_metric: host=algo-1, epoch=182, batch=5 train loss <loss>=0.780013749997\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] Epoch[182] Batch [5]#011Speed: 4245.61 samples/sec#011loss=0.780014\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 571.6209411621094, \"sum\": 571.6209411621094, \"min\": 571.6209411621094}}, \"EndTime\": 1601747436.81242, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747436.240479}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1077.46015298 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] #quality_metric: host=algo-1, epoch=182, train loss <loss>=0.761927163601\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:36 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] Epoch[183] Batch[0] avg_epoch_loss=0.785600\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] #quality_metric: host=algo-1, epoch=183, batch=0 train loss <loss>=0.785599529743\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] Epoch[183] Batch[5] avg_epoch_loss=0.740308\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] #quality_metric: host=algo-1, epoch=183, batch=5 train loss <loss>=0.740308483442\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] Epoch[183] Batch [5]#011Speed: 3859.75 samples/sec#011loss=0.740308\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] Epoch[183] Batch[10] avg_epoch_loss=0.726438\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] #quality_metric: host=algo-1, epoch=183, batch=10 train loss <loss>=0.709793257713\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] Epoch[183] Batch [10]#011Speed: 3838.40 samples/sec#011loss=0.709793\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 556.5121173858643, \"sum\": 556.5121173858643, \"min\": 556.5121173858643}}, \"EndTime\": 1601747437.369466, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747436.812482}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1165.99322447 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] #quality_metric: host=algo-1, epoch=183, train loss <loss>=0.726437926292\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] Epoch[184] Batch[0] avg_epoch_loss=0.761171\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] #quality_metric: host=algo-1, epoch=184, batch=0 train loss <loss>=0.761170566082\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] Epoch[184] Batch[5] avg_epoch_loss=0.752726\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] #quality_metric: host=algo-1, epoch=184, batch=5 train loss <loss>=0.752725859483\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] Epoch[184] Batch [5]#011Speed: 4191.67 samples/sec#011loss=0.752726\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 518.5019969940186, \"sum\": 518.5019969940186, \"min\": 518.5019969940186}}, \"EndTime\": 1601747437.888416, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747437.369529}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1153.10454099 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] #quality_metric: host=algo-1, epoch=184, train loss <loss>=0.864545804262\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:37 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:38 INFO 139787867731776] Epoch[185] Batch[0] avg_epoch_loss=0.745861\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:38 INFO 139787867731776] #quality_metric: host=algo-1, epoch=185, batch=0 train loss <loss>=0.745861291885\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:38 INFO 139787867731776] Epoch[185] Batch[5] avg_epoch_loss=0.702069\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:38 INFO 139787867731776] #quality_metric: host=algo-1, epoch=185, batch=5 train loss <loss>=0.702068636815\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:38 INFO 139787867731776] Epoch[185] Batch [5]#011Speed: 3983.37 samples/sec#011loss=0.702069\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:38 INFO 139787867731776] Epoch[185] Batch[10] avg_epoch_loss=0.704519\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:38 INFO 139787867731776] #quality_metric: host=algo-1, epoch=185, batch=10 train loss <loss>=0.707460093498\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:38 INFO 139787867731776] Epoch[185] Batch [10]#011Speed: 3791.65 samples/sec#011loss=0.707460\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:38 INFO 139787867731776] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 583.0309391021729, \"sum\": 583.0309391021729, \"min\": 583.0309391021729}}, \"EndTime\": 1601747438.471903, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747437.888483}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:38 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1159.28705643 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:38 INFO 139787867731776] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:38 INFO 139787867731776] #quality_metric: host=algo-1, epoch=185, train loss <loss>=0.704519298944\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:38 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:38 INFO 139787867731776] Epoch[186] Batch[0] avg_epoch_loss=0.723690\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:38 INFO 139787867731776] #quality_metric: host=algo-1, epoch=186, batch=0 train loss <loss>=0.723689675331\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:38 INFO 139787867731776] Epoch[186] Batch[5] avg_epoch_loss=0.717882\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:38 INFO 139787867731776] #quality_metric: host=algo-1, epoch=186, batch=5 train loss <loss>=0.71788173914\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:38 INFO 139787867731776] Epoch[186] Batch [5]#011Speed: 3812.57 samples/sec#011loss=0.717882\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:39 INFO 139787867731776] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 540.8859252929688, \"sum\": 540.8859252929688, \"min\": 540.8859252929688}}, \"EndTime\": 1601747439.013201, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747438.471963}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:39 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1162.69220677 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:39 INFO 139787867731776] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:39 INFO 139787867731776] #quality_metric: host=algo-1, epoch=186, train loss <loss>=0.72516451478\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:39 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:39 INFO 139787867731776] Epoch[187] Batch[0] avg_epoch_loss=0.768754\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:39 INFO 139787867731776] #quality_metric: host=algo-1, epoch=187, batch=0 train loss <loss>=0.768754243851\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:39 INFO 139787867731776] Epoch[187] Batch[5] avg_epoch_loss=0.723369\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:39 INFO 139787867731776] #quality_metric: host=algo-1, epoch=187, batch=5 train loss <loss>=0.723369300365\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:39 INFO 139787867731776] Epoch[187] Batch [5]#011Speed: 4284.19 samples/sec#011loss=0.723369\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:39 INFO 139787867731776] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 520.8749771118164, \"sum\": 520.8749771118164, \"min\": 520.8749771118164}}, \"EndTime\": 1601747439.534551, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747439.013269}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:39 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1213.11476766 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:39 INFO 139787867731776] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:39 INFO 139787867731776] #quality_metric: host=algo-1, epoch=187, train loss <loss>=0.725282925367\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:39 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:39 INFO 139787867731776] Epoch[188] Batch[0] avg_epoch_loss=0.767673\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:39 INFO 139787867731776] #quality_metric: host=algo-1, epoch=188, batch=0 train loss <loss>=0.767672777176\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] Epoch[188] Batch[5] avg_epoch_loss=0.695649\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] #quality_metric: host=algo-1, epoch=188, batch=5 train loss <loss>=0.695649236441\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] Epoch[188] Batch [5]#011Speed: 3983.92 samples/sec#011loss=0.695649\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] Epoch[188] Batch[10] avg_epoch_loss=0.685940\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] #quality_metric: host=algo-1, epoch=188, batch=10 train loss <loss>=0.674289226532\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] Epoch[188] Batch [10]#011Speed: 3530.41 samples/sec#011loss=0.674289\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 561.3970756530762, \"sum\": 561.3970756530762, \"min\": 561.3970756530762}}, \"EndTime\": 1601747440.096375, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747439.534616}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1209.26331952 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] #quality_metric: host=algo-1, epoch=188, train loss <loss>=0.685940141028\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_ea292573-8b0f-438d-a14e-c288874dcecf-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.468057632446289, \"sum\": 6.468057632446289, \"min\": 6.468057632446289}}, \"EndTime\": 1601747440.10335, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747440.096443}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] Epoch[189] Batch[0] avg_epoch_loss=0.754349\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] #quality_metric: host=algo-1, epoch=189, batch=0 train loss <loss>=0.754348874092\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] Epoch[189] Batch[5] avg_epoch_loss=0.755139\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] #quality_metric: host=algo-1, epoch=189, batch=5 train loss <loss>=0.755138893922\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] Epoch[189] Batch [5]#011Speed: 4172.42 samples/sec#011loss=0.755139\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 516.3049697875977, \"sum\": 516.3049697875977, \"min\": 516.3049697875977}}, \"EndTime\": 1601747440.61976, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747440.103401}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1225.79202583 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] #quality_metric: host=algo-1, epoch=189, train loss <loss>=0.746956259012\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:40 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] Epoch[190] Batch[0] avg_epoch_loss=0.750121\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] #quality_metric: host=algo-1, epoch=190, batch=0 train loss <loss>=0.750121414661\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] Epoch[190] Batch[5] avg_epoch_loss=0.718135\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] #quality_metric: host=algo-1, epoch=190, batch=5 train loss <loss>=0.71813522776\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] Epoch[190] Batch [5]#011Speed: 4022.37 samples/sec#011loss=0.718135\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] Epoch[190] Batch[10] avg_epoch_loss=0.678052\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] #quality_metric: host=algo-1, epoch=190, batch=10 train loss <loss>=0.629953151941\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] Epoch[190] Batch [10]#011Speed: 4287.53 samples/sec#011loss=0.629953\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 576.9250392913818, \"sum\": 576.9250392913818, \"min\": 576.9250392913818}}, \"EndTime\": 1601747441.197085, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747440.619825}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1143.82357559 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] #quality_metric: host=algo-1, epoch=190, train loss <loss>=0.678052466024\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_8db95411-e87c-41ca-b91c-1382c5dec9a8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.391048431396484, \"sum\": 6.391048431396484, \"min\": 6.391048431396484}}, \"EndTime\": 1601747441.203923, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747441.197141}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] Epoch[191] Batch[0] avg_epoch_loss=0.759619\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] #quality_metric: host=algo-1, epoch=191, batch=0 train loss <loss>=0.75961881876\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] Epoch[191] Batch[5] avg_epoch_loss=0.710422\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] #quality_metric: host=algo-1, epoch=191, batch=5 train loss <loss>=0.710421631734\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] Epoch[191] Batch [5]#011Speed: 4319.78 samples/sec#011loss=0.710422\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 514.0609741210938, \"sum\": 514.0609741210938, \"min\": 514.0609741210938}}, \"EndTime\": 1601747441.718092, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747441.203978}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1236.97152892 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] #quality_metric: host=algo-1, epoch=191, train loss <loss>=0.735557818413\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:41 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] Epoch[192] Batch[0] avg_epoch_loss=0.749377\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=192, batch=0 train loss <loss>=0.749377131462\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] Epoch[192] Batch[5] avg_epoch_loss=0.705940\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=192, batch=5 train loss <loss>=0.705940395594\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] Epoch[192] Batch [5]#011Speed: 3967.51 samples/sec#011loss=0.705940\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] Epoch[192] Batch[10] avg_epoch_loss=0.655620\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=192, batch=10 train loss <loss>=0.595235770941\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] Epoch[192] Batch [10]#011Speed: 4017.32 samples/sec#011loss=0.595236\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 599.1718769073486, \"sum\": 599.1718769073486, \"min\": 599.1718769073486}}, \"EndTime\": 1601747442.317675, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747441.718159}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1069.64176862 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=192, train loss <loss>=0.655620111661\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_98982fef-cc97-4145-9c9c-61a4acf08697-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.581949234008789, \"sum\": 7.581949234008789, \"min\": 7.581949234008789}}, \"EndTime\": 1601747442.325644, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747442.31774}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] Epoch[193] Batch[0] avg_epoch_loss=0.660240\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=193, batch=0 train loss <loss>=0.660240232944\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] Epoch[193] Batch[5] avg_epoch_loss=0.693495\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=193, batch=5 train loss <loss>=0.693495144447\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] Epoch[193] Batch [5]#011Speed: 3060.06 samples/sec#011loss=0.693495\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] Epoch[193] Batch[10] avg_epoch_loss=0.744509\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=193, batch=10 train loss <loss>=0.805726182461\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] Epoch[193] Batch [10]#011Speed: 3858.82 samples/sec#011loss=0.805726\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 585.1719379425049, \"sum\": 585.1719379425049, \"min\": 585.1719379425049}}, \"EndTime\": 1601747442.91093, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747442.325703}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1098.65313979 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=193, train loss <loss>=0.744509252635\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:42 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:43 INFO 139787867731776] Epoch[194] Batch[0] avg_epoch_loss=0.810884\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:43 INFO 139787867731776] #quality_metric: host=algo-1, epoch=194, batch=0 train loss <loss>=0.810883581638\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:43 INFO 139787867731776] Epoch[194] Batch[5] avg_epoch_loss=0.749408\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:43 INFO 139787867731776] #quality_metric: host=algo-1, epoch=194, batch=5 train loss <loss>=0.749407519897\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:43 INFO 139787867731776] Epoch[194] Batch [5]#011Speed: 3766.08 samples/sec#011loss=0.749408\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:43 INFO 139787867731776] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 528.1870365142822, \"sum\": 528.1870365142822, \"min\": 528.1870365142822}}, \"EndTime\": 1601747443.439592, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747442.910989}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:43 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1156.56844944 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:43 INFO 139787867731776] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:43 INFO 139787867731776] #quality_metric: host=algo-1, epoch=194, train loss <loss>=0.721519640088\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:43 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:43 INFO 139787867731776] Epoch[195] Batch[0] avg_epoch_loss=0.637417\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:43 INFO 139787867731776] #quality_metric: host=algo-1, epoch=195, batch=0 train loss <loss>=0.637416601181\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:43 INFO 139787867731776] Epoch[195] Batch[5] avg_epoch_loss=0.699473\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:43 INFO 139787867731776] #quality_metric: host=algo-1, epoch=195, batch=5 train loss <loss>=0.699472894271\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:43 INFO 139787867731776] Epoch[195] Batch [5]#011Speed: 4255.10 samples/sec#011loss=0.699473\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:43 INFO 139787867731776] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 541.1498546600342, \"sum\": 541.1498546600342, \"min\": 541.1498546600342}}, \"EndTime\": 1601747443.981151, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747443.43966}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:43 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1162.12473179 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:43 INFO 139787867731776] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:43 INFO 139787867731776] #quality_metric: host=algo-1, epoch=195, train loss <loss>=0.716655790806\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:43 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:44 INFO 139787867731776] Epoch[196] Batch[0] avg_epoch_loss=0.647048\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:44 INFO 139787867731776] #quality_metric: host=algo-1, epoch=196, batch=0 train loss <loss>=0.647048354149\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:44 INFO 139787867731776] Epoch[196] Batch[5] avg_epoch_loss=0.690077\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:44 INFO 139787867731776] #quality_metric: host=algo-1, epoch=196, batch=5 train loss <loss>=0.690076937278\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:44 INFO 139787867731776] Epoch[196] Batch [5]#011Speed: 4255.94 samples/sec#011loss=0.690077\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:44 INFO 139787867731776] Epoch[196] Batch[10] avg_epoch_loss=0.688140\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:44 INFO 139787867731776] #quality_metric: host=algo-1, epoch=196, batch=10 train loss <loss>=0.685815834999\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:44 INFO 139787867731776] Epoch[196] Batch [10]#011Speed: 4249.39 samples/sec#011loss=0.685816\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:44 INFO 139787867731776] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 554.1801452636719, \"sum\": 554.1801452636719, \"min\": 554.1801452636719}}, \"EndTime\": 1601747444.535734, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747443.981221}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:44 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1235.88204247 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:44 INFO 139787867731776] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:44 INFO 139787867731776] #quality_metric: host=algo-1, epoch=196, train loss <loss>=0.688140072606\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:44 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:44 INFO 139787867731776] Epoch[197] Batch[0] avg_epoch_loss=0.886088\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:44 INFO 139787867731776] #quality_metric: host=algo-1, epoch=197, batch=0 train loss <loss>=0.886087596416\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:44 INFO 139787867731776] Epoch[197] Batch[5] avg_epoch_loss=0.704885\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:44 INFO 139787867731776] #quality_metric: host=algo-1, epoch=197, batch=5 train loss <loss>=0.704884966214\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:44 INFO 139787867731776] Epoch[197] Batch [5]#011Speed: 3988.38 samples/sec#011loss=0.704885\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:45 INFO 139787867731776] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 518.5871124267578, \"sum\": 518.5871124267578, \"min\": 518.5871124267578}}, \"EndTime\": 1601747445.054668, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747444.535788}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:45 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1195.32586757 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:45 INFO 139787867731776] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:45 INFO 139787867731776] #quality_metric: host=algo-1, epoch=197, train loss <loss>=0.679113021493\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:45 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:45 INFO 139787867731776] Epoch[198] Batch[0] avg_epoch_loss=0.831322\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:45 INFO 139787867731776] #quality_metric: host=algo-1, epoch=198, batch=0 train loss <loss>=0.831321835518\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:45 INFO 139787867731776] Epoch[198] Batch[5] avg_epoch_loss=0.741311\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:45 INFO 139787867731776] #quality_metric: host=algo-1, epoch=198, batch=5 train loss <loss>=0.741311262051\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:45 INFO 139787867731776] Epoch[198] Batch [5]#011Speed: 4229.11 samples/sec#011loss=0.741311\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:45 INFO 139787867731776] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 522.150993347168, \"sum\": 522.150993347168, \"min\": 522.150993347168}}, \"EndTime\": 1601747445.577276, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747445.054737}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:45 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1202.50309845 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:45 INFO 139787867731776] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:45 INFO 139787867731776] #quality_metric: host=algo-1, epoch=198, train loss <loss>=0.725196021795\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:45 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:45 INFO 139787867731776] Epoch[199] Batch[0] avg_epoch_loss=0.595162\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:45 INFO 139787867731776] #quality_metric: host=algo-1, epoch=199, batch=0 train loss <loss>=0.595162153244\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] Epoch[199] Batch[5] avg_epoch_loss=0.717157\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] #quality_metric: host=algo-1, epoch=199, batch=5 train loss <loss>=0.717156608899\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] Epoch[199] Batch [5]#011Speed: 4259.96 samples/sec#011loss=0.717157\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 530.5240154266357, \"sum\": 530.5240154266357, \"min\": 530.5240154266357}}, \"EndTime\": 1601747446.108249, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747445.577334}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1191.05073243 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] #quality_metric: host=algo-1, epoch=199, train loss <loss>=0.731825083494\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] Epoch[200] Batch[0] avg_epoch_loss=0.736079\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] #quality_metric: host=algo-1, epoch=200, batch=0 train loss <loss>=0.736078500748\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] Epoch[200] Batch[5] avg_epoch_loss=0.700746\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] #quality_metric: host=algo-1, epoch=200, batch=5 train loss <loss>=0.70074587067\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] Epoch[200] Batch [5]#011Speed: 3618.97 samples/sec#011loss=0.700746\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] Epoch[200] Batch[10] avg_epoch_loss=0.706153\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] #quality_metric: host=algo-1, epoch=200, batch=10 train loss <loss>=0.712642621994\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] Epoch[200] Batch [10]#011Speed: 4256.50 samples/sec#011loss=0.712643\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 552.9201030731201, \"sum\": 552.9201030731201, \"min\": 552.9201030731201}}, \"EndTime\": 1601747446.661654, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747446.108316}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1200.70371392 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] #quality_metric: host=algo-1, epoch=200, train loss <loss>=0.706153484908\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:46 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] Epoch[201] Batch[0] avg_epoch_loss=0.742668\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] #quality_metric: host=algo-1, epoch=201, batch=0 train loss <loss>=0.742668092251\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] Epoch[201] Batch[5] avg_epoch_loss=0.709457\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] #quality_metric: host=algo-1, epoch=201, batch=5 train loss <loss>=0.70945734779\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] Epoch[201] Batch [5]#011Speed: 4380.20 samples/sec#011loss=0.709457\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 514.7290229797363, \"sum\": 514.7290229797363, \"min\": 514.7290229797363}}, \"EndTime\": 1601747447.176821, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747446.661712}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1204.30195488 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] #quality_metric: host=algo-1, epoch=201, train loss <loss>=0.742224925756\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] Epoch[202] Batch[0] avg_epoch_loss=0.709806\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] #quality_metric: host=algo-1, epoch=202, batch=0 train loss <loss>=0.709806203842\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] Epoch[202] Batch[5] avg_epoch_loss=0.692281\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] #quality_metric: host=algo-1, epoch=202, batch=5 train loss <loss>=0.692281464736\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] Epoch[202] Batch [5]#011Speed: 4307.51 samples/sec#011loss=0.692281\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] Epoch[202] Batch[10] avg_epoch_loss=0.689111\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] #quality_metric: host=algo-1, epoch=202, batch=10 train loss <loss>=0.685306584835\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] Epoch[202] Batch [10]#011Speed: 4319.36 samples/sec#011loss=0.685307\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 528.688907623291, \"sum\": 528.688907623291, \"min\": 528.688907623291}}, \"EndTime\": 1601747447.705989, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747447.176882}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1219.76588728 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] #quality_metric: host=algo-1, epoch=202, train loss <loss>=0.689111064781\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:47 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] Epoch[203] Batch[0] avg_epoch_loss=0.525178\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] #quality_metric: host=algo-1, epoch=203, batch=0 train loss <loss>=0.525178432465\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] Epoch[203] Batch[5] avg_epoch_loss=0.664009\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] #quality_metric: host=algo-1, epoch=203, batch=5 train loss <loss>=0.6640090247\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] Epoch[203] Batch [5]#011Speed: 3973.77 samples/sec#011loss=0.664009\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] Epoch[203] Batch[10] avg_epoch_loss=0.736032\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] #quality_metric: host=algo-1, epoch=203, batch=10 train loss <loss>=0.822459232807\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] Epoch[203] Batch [10]#011Speed: 4186.79 samples/sec#011loss=0.822459\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 540.7061576843262, \"sum\": 540.7061576843262, \"min\": 540.7061576843262}}, \"EndTime\": 1601747448.247135, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747447.706057}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1205.62587451 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] #quality_metric: host=algo-1, epoch=203, train loss <loss>=0.736031846567\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] Epoch[204] Batch[0] avg_epoch_loss=0.739764\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] #quality_metric: host=algo-1, epoch=204, batch=0 train loss <loss>=0.739764392376\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] Epoch[204] Batch[5] avg_epoch_loss=0.694786\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] #quality_metric: host=algo-1, epoch=204, batch=5 train loss <loss>=0.694786280394\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] Epoch[204] Batch [5]#011Speed: 4079.55 samples/sec#011loss=0.694786\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 539.1190052032471, \"sum\": 539.1190052032471, \"min\": 539.1190052032471}}, \"EndTime\": 1601747448.786624, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747448.247198}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1179.38322986 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] #quality_metric: host=algo-1, epoch=204, train loss <loss>=0.702467894554\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:48 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] Epoch[205] Batch[0] avg_epoch_loss=0.791713\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] #quality_metric: host=algo-1, epoch=205, batch=0 train loss <loss>=0.791713297367\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] Epoch[205] Batch[5] avg_epoch_loss=0.731937\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] #quality_metric: host=algo-1, epoch=205, batch=5 train loss <loss>=0.731937309106\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] Epoch[205] Batch [5]#011Speed: 3558.68 samples/sec#011loss=0.731937\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] Epoch[205] Batch[10] avg_epoch_loss=0.671896\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] #quality_metric: host=algo-1, epoch=205, batch=10 train loss <loss>=0.59984588027\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] Epoch[205] Batch [10]#011Speed: 4028.71 samples/sec#011loss=0.599846\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 553.3499717712402, \"sum\": 553.3499717712402, \"min\": 553.3499717712402}}, \"EndTime\": 1601747449.340673, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747448.786735}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1205.13723983 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] #quality_metric: host=algo-1, epoch=205, train loss <loss>=0.671895750544\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] Epoch[206] Batch[0] avg_epoch_loss=0.779525\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] #quality_metric: host=algo-1, epoch=206, batch=0 train loss <loss>=0.779525160789\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] Epoch[206] Batch[5] avg_epoch_loss=0.709930\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] #quality_metric: host=algo-1, epoch=206, batch=5 train loss <loss>=0.709930400054\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] Epoch[206] Batch [5]#011Speed: 4339.52 samples/sec#011loss=0.709930\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 513.1630897521973, \"sum\": 513.1630897521973, \"min\": 513.1630897521973}}, \"EndTime\": 1601747449.854307, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747449.340736}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1229.3938209 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] #quality_metric: host=algo-1, epoch=206, train loss <loss>=0.700044482946\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:49 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] Epoch[207] Batch[0] avg_epoch_loss=0.742826\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] #quality_metric: host=algo-1, epoch=207, batch=0 train loss <loss>=0.742826282978\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] Epoch[207] Batch[5] avg_epoch_loss=0.726010\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] #quality_metric: host=algo-1, epoch=207, batch=5 train loss <loss>=0.726010183493\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] Epoch[207] Batch [5]#011Speed: 4323.27 samples/sec#011loss=0.726010\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 532.249927520752, \"sum\": 532.249927520752, \"min\": 532.249927520752}}, \"EndTime\": 1601747450.386965, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747449.854374}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1189.0629412 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] #quality_metric: host=algo-1, epoch=207, train loss <loss>=0.709148663282\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] Epoch[208] Batch[0] avg_epoch_loss=0.673726\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] #quality_metric: host=algo-1, epoch=208, batch=0 train loss <loss>=0.673726439476\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] Epoch[208] Batch[5] avg_epoch_loss=0.682606\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] #quality_metric: host=algo-1, epoch=208, batch=5 train loss <loss>=0.682606180509\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] Epoch[208] Batch [5]#011Speed: 4302.13 samples/sec#011loss=0.682606\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] Epoch[208] Batch[10] avg_epoch_loss=0.704962\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] #quality_metric: host=algo-1, epoch=208, batch=10 train loss <loss>=0.731790077686\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] Epoch[208] Batch [10]#011Speed: 4245.38 samples/sec#011loss=0.731790\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 530.7481288909912, \"sum\": 530.7481288909912, \"min\": 530.7481288909912}}, \"EndTime\": 1601747450.918224, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747450.387033}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1256.49198636 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] #quality_metric: host=algo-1, epoch=208, train loss <loss>=0.704962497408\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:50 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:51 INFO 139787867731776] Epoch[209] Batch[0] avg_epoch_loss=0.791931\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:51 INFO 139787867731776] #quality_metric: host=algo-1, epoch=209, batch=0 train loss <loss>=0.791931152344\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:51 INFO 139787867731776] Epoch[209] Batch[5] avg_epoch_loss=0.641613\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:51 INFO 139787867731776] #quality_metric: host=algo-1, epoch=209, batch=5 train loss <loss>=0.641612817844\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:51 INFO 139787867731776] Epoch[209] Batch [5]#011Speed: 4285.07 samples/sec#011loss=0.641613\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:51 INFO 139787867731776] Epoch[209] Batch[10] avg_epoch_loss=0.641525\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:51 INFO 139787867731776] #quality_metric: host=algo-1, epoch=209, batch=10 train loss <loss>=0.64141920805\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:51 INFO 139787867731776] Epoch[209] Batch [10]#011Speed: 4109.93 samples/sec#011loss=0.641419\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:51 INFO 139787867731776] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 565.9000873565674, \"sum\": 565.9000873565674, \"min\": 565.9000873565674}}, \"EndTime\": 1601747451.484542, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747450.918288}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:51 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1185.51040194 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:51 INFO 139787867731776] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:51 INFO 139787867731776] #quality_metric: host=algo-1, epoch=209, train loss <loss>=0.641524813392\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:51 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:51 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_1e758375-f768-492d-bfe6-452c28ab1202-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.874011993408203, \"sum\": 7.874011993408203, \"min\": 7.874011993408203}}, \"EndTime\": 1601747451.492852, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747451.484603}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:51 INFO 139787867731776] Epoch[210] Batch[0] avg_epoch_loss=0.534348\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:51 INFO 139787867731776] #quality_metric: host=algo-1, epoch=210, batch=0 train loss <loss>=0.534348070621\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] Epoch[210] Batch[5] avg_epoch_loss=0.593411\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] #quality_metric: host=algo-1, epoch=210, batch=5 train loss <loss>=0.593410899242\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] Epoch[210] Batch [5]#011Speed: 3962.16 samples/sec#011loss=0.593411\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] Epoch[210] Batch[10] avg_epoch_loss=0.646353\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] #quality_metric: host=algo-1, epoch=210, batch=10 train loss <loss>=0.709882986546\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] Epoch[210] Batch [10]#011Speed: 4309.42 samples/sec#011loss=0.709883\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 592.9369926452637, \"sum\": 592.9369926452637, \"min\": 592.9369926452637}}, \"EndTime\": 1601747452.085903, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747451.492915}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1080.89709974 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] #quality_metric: host=algo-1, epoch=210, train loss <loss>=0.646352757107\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] Epoch[211] Batch[0] avg_epoch_loss=0.688376\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] #quality_metric: host=algo-1, epoch=211, batch=0 train loss <loss>=0.688376128674\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] Epoch[211] Batch[5] avg_epoch_loss=0.679565\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] #quality_metric: host=algo-1, epoch=211, batch=5 train loss <loss>=0.679565389951\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] Epoch[211] Batch [5]#011Speed: 4121.42 samples/sec#011loss=0.679565\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] Epoch[211] Batch[10] avg_epoch_loss=0.650819\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] #quality_metric: host=algo-1, epoch=211, batch=10 train loss <loss>=0.616322517395\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] Epoch[211] Batch [10]#011Speed: 4254.42 samples/sec#011loss=0.616323\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 581.35986328125, \"sum\": 581.35986328125, \"min\": 581.35986328125}}, \"EndTime\": 1601747452.667625, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747452.085965}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1169.48626079 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] #quality_metric: host=algo-1, epoch=211, train loss <loss>=0.650818629698\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:52 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] Epoch[212] Batch[0] avg_epoch_loss=0.639764\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] #quality_metric: host=algo-1, epoch=212, batch=0 train loss <loss>=0.639764070511\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] Epoch[212] Batch[5] avg_epoch_loss=0.661127\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] #quality_metric: host=algo-1, epoch=212, batch=5 train loss <loss>=0.661126921574\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] Epoch[212] Batch [5]#011Speed: 4186.08 samples/sec#011loss=0.661127\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 575.4361152648926, \"sum\": 575.4361152648926, \"min\": 575.4361152648926}}, \"EndTime\": 1601747453.243434, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747452.667688}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1087.64972967 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] #quality_metric: host=algo-1, epoch=212, train loss <loss>=0.635335797071\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_318f9cd9-6906-4535-a5f9-c1d301b6a3be-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.9669952392578125, \"sum\": 7.9669952392578125, \"min\": 7.9669952392578125}}, \"EndTime\": 1601747453.252001, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747453.243518}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] Epoch[213] Batch[0] avg_epoch_loss=0.726460\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] #quality_metric: host=algo-1, epoch=213, batch=0 train loss <loss>=0.726459681988\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] Epoch[213] Batch[5] avg_epoch_loss=0.733507\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] #quality_metric: host=algo-1, epoch=213, batch=5 train loss <loss>=0.733506828547\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] Epoch[213] Batch [5]#011Speed: 3847.44 samples/sec#011loss=0.733507\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] Epoch[213] Batch[10] avg_epoch_loss=0.785485\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] #quality_metric: host=algo-1, epoch=213, batch=10 train loss <loss>=0.847859215736\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] Epoch[213] Batch [10]#011Speed: 3988.98 samples/sec#011loss=0.847859\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 582.205057144165, \"sum\": 582.205057144165, \"min\": 582.205057144165}}, \"EndTime\": 1601747453.83432, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747453.252062}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1107.68743968 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] #quality_metric: host=algo-1, epoch=213, train loss <loss>=0.78548518636\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:53 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] Epoch[214] Batch[0] avg_epoch_loss=0.725390\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] #quality_metric: host=algo-1, epoch=214, batch=0 train loss <loss>=0.725390136242\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] Epoch[214] Batch[5] avg_epoch_loss=0.658078\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] #quality_metric: host=algo-1, epoch=214, batch=5 train loss <loss>=0.658077567816\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] Epoch[214] Batch [5]#011Speed: 3862.77 samples/sec#011loss=0.658078\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] Epoch[214] Batch[10] avg_epoch_loss=0.579828\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] #quality_metric: host=algo-1, epoch=214, batch=10 train loss <loss>=0.485928195715\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] Epoch[214] Batch [10]#011Speed: 4075.14 samples/sec#011loss=0.485928\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 542.4139499664307, \"sum\": 542.4139499664307, \"min\": 542.4139499664307}}, \"EndTime\": 1601747454.377177, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747453.834381}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1194.43693333 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] #quality_metric: host=algo-1, epoch=214, train loss <loss>=0.579827853224\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_9a2d0111-1d92-44ad-a4c2-76ca290e409a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.150960922241211, \"sum\": 6.150960922241211, \"min\": 6.150960922241211}}, \"EndTime\": 1601747454.383822, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747454.377246}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] Epoch[215] Batch[0] avg_epoch_loss=0.679232\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] #quality_metric: host=algo-1, epoch=215, batch=0 train loss <loss>=0.679232001305\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] Epoch[215] Batch[5] avg_epoch_loss=0.669877\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] #quality_metric: host=algo-1, epoch=215, batch=5 train loss <loss>=0.669877340396\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] Epoch[215] Batch [5]#011Speed: 4231.57 samples/sec#011loss=0.669877\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] Epoch[215] Batch[10] avg_epoch_loss=0.630751\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] #quality_metric: host=algo-1, epoch=215, batch=10 train loss <loss>=0.583798527718\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] Epoch[215] Batch [10]#011Speed: 3677.49 samples/sec#011loss=0.583799\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 545.3600883483887, \"sum\": 545.3600883483887, \"min\": 545.3600883483887}}, \"EndTime\": 1601747454.929286, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747454.383872}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1261.3389469 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] #quality_metric: host=algo-1, epoch=215, train loss <loss>=0.63075060736\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:54 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:55 INFO 139787867731776] Epoch[216] Batch[0] avg_epoch_loss=0.634438\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:55 INFO 139787867731776] #quality_metric: host=algo-1, epoch=216, batch=0 train loss <loss>=0.634438157082\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:55 INFO 139787867731776] Epoch[216] Batch[5] avg_epoch_loss=0.670901\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:55 INFO 139787867731776] #quality_metric: host=algo-1, epoch=216, batch=5 train loss <loss>=0.670900583267\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:55 INFO 139787867731776] Epoch[216] Batch [5]#011Speed: 3997.20 samples/sec#011loss=0.670901\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:55 INFO 139787867731776] Epoch[216] Batch[10] avg_epoch_loss=0.689620\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:55 INFO 139787867731776] #quality_metric: host=algo-1, epoch=216, batch=10 train loss <loss>=0.712083077431\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:55 INFO 139787867731776] Epoch[216] Batch [10]#011Speed: 3843.52 samples/sec#011loss=0.712083\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:55 INFO 139787867731776] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 549.1180419921875, \"sum\": 549.1180419921875, \"min\": 549.1180419921875}}, \"EndTime\": 1601747455.478851, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747454.929349}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:55 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1181.71668701 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:55 INFO 139787867731776] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:55 INFO 139787867731776] #quality_metric: host=algo-1, epoch=216, train loss <loss>=0.689619898796\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:55 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:55 INFO 139787867731776] Epoch[217] Batch[0] avg_epoch_loss=0.653178\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:55 INFO 139787867731776] #quality_metric: host=algo-1, epoch=217, batch=0 train loss <loss>=0.65317773819\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:55 INFO 139787867731776] Epoch[217] Batch[5] avg_epoch_loss=0.633977\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:55 INFO 139787867731776] #quality_metric: host=algo-1, epoch=217, batch=5 train loss <loss>=0.633976727724\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:55 INFO 139787867731776] Epoch[217] Batch [5]#011Speed: 4320.93 samples/sec#011loss=0.633977\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] Epoch[217] Batch[10] avg_epoch_loss=0.612682\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=217, batch=10 train loss <loss>=0.587128695846\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] Epoch[217] Batch [10]#011Speed: 3576.82 samples/sec#011loss=0.587129\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 553.0388355255127, \"sum\": 553.0388355255127, \"min\": 553.0388355255127}}, \"EndTime\": 1601747456.032292, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747455.478902}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1158.83629003 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=217, train loss <loss>=0.612682167779\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] Epoch[218] Batch[0] avg_epoch_loss=0.979317\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=218, batch=0 train loss <loss>=0.979317247868\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] Epoch[218] Batch[5] avg_epoch_loss=0.869646\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=218, batch=5 train loss <loss>=0.869646290938\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] Epoch[218] Batch [5]#011Speed: 4223.54 samples/sec#011loss=0.869646\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] Epoch[218] Batch[10] avg_epoch_loss=0.758026\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=218, batch=10 train loss <loss>=0.624082702398\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] Epoch[218] Batch [10]#011Speed: 4229.90 samples/sec#011loss=0.624083\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 578.2620906829834, \"sum\": 578.2620906829834, \"min\": 578.2620906829834}}, \"EndTime\": 1601747456.611025, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747456.03236}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1155.009808 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=218, train loss <loss>=0.758026477965\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] Epoch[219] Batch[0] avg_epoch_loss=0.812675\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=219, batch=0 train loss <loss>=0.812675058842\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] Epoch[219] Batch[5] avg_epoch_loss=0.778296\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=219, batch=5 train loss <loss>=0.778295735518\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] Epoch[219] Batch [5]#011Speed: 4197.82 samples/sec#011loss=0.778296\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] Epoch[219] Batch[10] avg_epoch_loss=0.736459\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=219, batch=10 train loss <loss>=0.686254256964\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] Epoch[219] Batch [10]#011Speed: 3682.73 samples/sec#011loss=0.686254\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 545.9010601043701, \"sum\": 545.9010601043701, \"min\": 545.9010601043701}}, \"EndTime\": 1601747457.157316, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747456.611085}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1208.78222721 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=219, train loss <loss>=0.736458699812\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] Epoch[220] Batch[0] avg_epoch_loss=0.654647\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=220, batch=0 train loss <loss>=0.654647052288\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] Epoch[220] Batch[5] avg_epoch_loss=0.734115\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=220, batch=5 train loss <loss>=0.734114676714\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] Epoch[220] Batch [5]#011Speed: 4153.89 samples/sec#011loss=0.734115\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 535.423994064331, \"sum\": 535.423994064331, \"min\": 535.423994064331}}, \"EndTime\": 1601747457.693292, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747457.157385}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1161.50117804 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=220, train loss <loss>=0.751663166285\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:57 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] Epoch[221] Batch[0] avg_epoch_loss=0.656443\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=221, batch=0 train loss <loss>=0.656443417072\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] Epoch[221] Batch[5] avg_epoch_loss=0.711836\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=221, batch=5 train loss <loss>=0.711836179097\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] Epoch[221] Batch [5]#011Speed: 3595.15 samples/sec#011loss=0.711836\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] Epoch[221] Batch[10] avg_epoch_loss=0.753076\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=221, batch=10 train loss <loss>=0.802564394474\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] Epoch[221] Batch [10]#011Speed: 4065.04 samples/sec#011loss=0.802564\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 550.957202911377, \"sum\": 550.957202911377, \"min\": 550.957202911377}}, \"EndTime\": 1601747458.244753, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747457.693354}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1221.30701126 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=221, train loss <loss>=0.753076276996\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] Epoch[222] Batch[0] avg_epoch_loss=0.627986\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=222, batch=0 train loss <loss>=0.627985596657\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] Epoch[222] Batch[5] avg_epoch_loss=0.653543\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=222, batch=5 train loss <loss>=0.653542647759\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] Epoch[222] Batch [5]#011Speed: 4264.96 samples/sec#011loss=0.653543\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] Epoch[222] Batch[10] avg_epoch_loss=0.611459\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=222, batch=10 train loss <loss>=0.560957714915\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] Epoch[222] Batch [10]#011Speed: 4137.36 samples/sec#011loss=0.560958\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 538.7082099914551, \"sum\": 538.7082099914551, \"min\": 538.7082099914551}}, \"EndTime\": 1601747458.783871, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747458.244814}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1198.96754781 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=222, train loss <loss>=0.611458587376\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:58 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] Epoch[223] Batch[0] avg_epoch_loss=0.574398\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=223, batch=0 train loss <loss>=0.574398040771\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] Epoch[223] Batch[5] avg_epoch_loss=0.644197\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=223, batch=5 train loss <loss>=0.644196530183\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] Epoch[223] Batch [5]#011Speed: 3688.04 samples/sec#011loss=0.644197\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] Epoch[223] Batch[10] avg_epoch_loss=0.677588\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=223, batch=10 train loss <loss>=0.717658662796\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] Epoch[223] Batch [10]#011Speed: 3699.25 samples/sec#011loss=0.717659\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 565.0730133056641, \"sum\": 565.0730133056641, \"min\": 565.0730133056641}}, \"EndTime\": 1601747459.34936, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747458.783927}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1139.48630532 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=223, train loss <loss>=0.677588408644\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] Epoch[224] Batch[0] avg_epoch_loss=0.775160\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=224, batch=0 train loss <loss>=0.775160372257\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] Epoch[224] Batch[5] avg_epoch_loss=0.793694\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=224, batch=5 train loss <loss>=0.79369388024\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] Epoch[224] Batch [5]#011Speed: 3876.63 samples/sec#011loss=0.793694\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] Epoch[224] Batch[10] avg_epoch_loss=0.743140\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=224, batch=10 train loss <loss>=0.682476305962\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] Epoch[224] Batch [10]#011Speed: 4123.83 samples/sec#011loss=0.682476\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 552.2348880767822, \"sum\": 552.2348880767822, \"min\": 552.2348880767822}}, \"EndTime\": 1601747459.902004, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747459.349424}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1189.50418277 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=224, train loss <loss>=0.743140437386\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:50:59 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:00 INFO 139787867731776] Epoch[225] Batch[0] avg_epoch_loss=0.878614\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=225, batch=0 train loss <loss>=0.878613650799\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:00 INFO 139787867731776] Epoch[225] Batch[5] avg_epoch_loss=0.770638\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=225, batch=5 train loss <loss>=0.770638455947\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:00 INFO 139787867731776] Epoch[225] Batch [5]#011Speed: 3756.71 samples/sec#011loss=0.770638\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:00 INFO 139787867731776] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 525.9840488433838, \"sum\": 525.9840488433838, \"min\": 525.9840488433838}}, \"EndTime\": 1601747460.428449, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747459.902066}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:00 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1174.71267422 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:00 INFO 139787867731776] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=225, train loss <loss>=0.723961526155\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:00 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:00 INFO 139787867731776] Epoch[226] Batch[0] avg_epoch_loss=0.765719\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=226, batch=0 train loss <loss>=0.765718698502\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:00 INFO 139787867731776] Epoch[226] Batch[5] avg_epoch_loss=0.727296\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=226, batch=5 train loss <loss>=0.727296014627\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:00 INFO 139787867731776] Epoch[226] Batch [5]#011Speed: 4189.97 samples/sec#011loss=0.727296\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:00 INFO 139787867731776] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 532.2458744049072, \"sum\": 532.2458744049072, \"min\": 532.2458744049072}}, \"EndTime\": 1601747460.961123, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747460.428518}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:00 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1179.69496238 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:00 INFO 139787867731776] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=226, train loss <loss>=0.667886272073\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:00 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:01 INFO 139787867731776] Epoch[227] Batch[0] avg_epoch_loss=0.599717\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=227, batch=0 train loss <loss>=0.599717020988\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:01 INFO 139787867731776] Epoch[227] Batch[5] avg_epoch_loss=0.658262\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=227, batch=5 train loss <loss>=0.658261815707\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:01 INFO 139787867731776] Epoch[227] Batch [5]#011Speed: 4265.36 samples/sec#011loss=0.658262\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:01 INFO 139787867731776] Epoch[227] Batch[10] avg_epoch_loss=0.611934\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=227, batch=10 train loss <loss>=0.556340181828\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:01 INFO 139787867731776] Epoch[227] Batch [10]#011Speed: 4327.59 samples/sec#011loss=0.556340\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:01 INFO 139787867731776] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 586.6899490356445, \"sum\": 586.6899490356445, \"min\": 586.6899490356445}}, \"EndTime\": 1601747461.548213, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747460.961189}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:01 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1092.3989205 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:01 INFO 139787867731776] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=227, train loss <loss>=0.611933800307\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:01 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:01 INFO 139787867731776] Epoch[228] Batch[0] avg_epoch_loss=0.659415\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=228, batch=0 train loss <loss>=0.659415483475\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] Epoch[228] Batch[5] avg_epoch_loss=0.678072\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=228, batch=5 train loss <loss>=0.678072224061\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] Epoch[228] Batch [5]#011Speed: 4151.31 samples/sec#011loss=0.678072\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 573.6560821533203, \"sum\": 573.6560821533203, \"min\": 573.6560821533203}}, \"EndTime\": 1601747462.122226, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747461.548275}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1075.38480283 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=228, train loss <loss>=0.673778140545\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] Epoch[229] Batch[0] avg_epoch_loss=0.696841\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=229, batch=0 train loss <loss>=0.696841239929\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] Epoch[229] Batch[5] avg_epoch_loss=0.670463\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=229, batch=5 train loss <loss>=0.670463492473\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] Epoch[229] Batch [5]#011Speed: 4316.10 samples/sec#011loss=0.670463\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] Epoch[229] Batch[10] avg_epoch_loss=0.630135\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=229, batch=10 train loss <loss>=0.581739899516\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] Epoch[229] Batch [10]#011Speed: 4226.04 samples/sec#011loss=0.581740\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 556.696891784668, \"sum\": 556.696891784668, \"min\": 556.696891784668}}, \"EndTime\": 1601747462.679303, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747462.122288}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1158.42822165 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=229, train loss <loss>=0.630134586583\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:02 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] Epoch[230] Batch[0] avg_epoch_loss=0.804353\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=230, batch=0 train loss <loss>=0.804352998734\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] Epoch[230] Batch[5] avg_epoch_loss=0.636333\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=230, batch=5 train loss <loss>=0.636332859596\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] Epoch[230] Batch [5]#011Speed: 4094.59 samples/sec#011loss=0.636333\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] Epoch[230] Batch[10] avg_epoch_loss=0.575409\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=230, batch=10 train loss <loss>=0.502300477028\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] Epoch[230] Batch [10]#011Speed: 4222.30 samples/sec#011loss=0.502300\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 540.3170585632324, \"sum\": 540.3170585632324, \"min\": 540.3170585632324}}, \"EndTime\": 1601747463.219981, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747462.679366}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1199.08948737 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=230, train loss <loss>=0.575409049338\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_79777add-37fd-40d9-972d-4184fe3b913c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.209850311279297, \"sum\": 6.209850311279297, \"min\": 6.209850311279297}}, \"EndTime\": 1601747463.226664, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747463.220044}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] Epoch[231] Batch[0] avg_epoch_loss=0.568327\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=231, batch=0 train loss <loss>=0.568327248096\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] Epoch[231] Batch[5] avg_epoch_loss=0.595444\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=231, batch=5 train loss <loss>=0.595444192489\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] Epoch[231] Batch [5]#011Speed: 4336.28 samples/sec#011loss=0.595444\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] Epoch[231] Batch[10] avg_epoch_loss=0.566448\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=231, batch=10 train loss <loss>=0.531652247906\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] Epoch[231] Batch [10]#011Speed: 3657.82 samples/sec#011loss=0.531652\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 546.6101169586182, \"sum\": 546.6101169586182, \"min\": 546.6101169586182}}, \"EndTime\": 1601747463.773384, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747463.22672}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1231.01551783 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=231, train loss <loss>=0.566447854042\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:03 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_8f621e08-d886-4a16-8f5e-44143e64826f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.955074310302734, \"sum\": 7.955074310302734, \"min\": 7.955074310302734}}, \"EndTime\": 1601747463.781768, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747463.773447}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] Epoch[232] Batch[0] avg_epoch_loss=0.519640\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=232, batch=0 train loss <loss>=0.519639670849\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] Epoch[232] Batch[5] avg_epoch_loss=0.622234\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=232, batch=5 train loss <loss>=0.622234344482\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] Epoch[232] Batch [5]#011Speed: 3631.75 samples/sec#011loss=0.622234\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] Epoch[232] Batch[10] avg_epoch_loss=0.533996\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=232, batch=10 train loss <loss>=0.428109359741\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] Epoch[232] Batch [10]#011Speed: 4173.46 samples/sec#011loss=0.428109\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 580.5118083953857, \"sum\": 580.5118083953857, \"min\": 580.5118083953857}}, \"EndTime\": 1601747464.36239, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747463.781827}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1114.36486351 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=232, train loss <loss>=0.533995715055\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_c89d1a45-9d3f-4bf0-b4ba-7c8c27e83c17-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.165981292724609, \"sum\": 6.165981292724609, \"min\": 6.165981292724609}}, \"EndTime\": 1601747464.369009, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747464.362451}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] Epoch[233] Batch[0] avg_epoch_loss=0.615924\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=233, batch=0 train loss <loss>=0.615924477577\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] Epoch[233] Batch[5] avg_epoch_loss=0.633190\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=233, batch=5 train loss <loss>=0.633189837138\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] Epoch[233] Batch [5]#011Speed: 4341.45 samples/sec#011loss=0.633190\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] Epoch[233] Batch[10] avg_epoch_loss=0.626023\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=233, batch=10 train loss <loss>=0.617423838377\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] Epoch[233] Batch [10]#011Speed: 4288.22 samples/sec#011loss=0.617424\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 531.0859680175781, \"sum\": 531.0859680175781, \"min\": 531.0859680175781}}, \"EndTime\": 1601747464.900196, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747464.369059}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1208.60727514 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=233, train loss <loss>=0.626023474065\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:04 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:05 INFO 139787867731776] Epoch[234] Batch[0] avg_epoch_loss=0.936347\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=234, batch=0 train loss <loss>=0.936346948147\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:05 INFO 139787867731776] Epoch[234] Batch[5] avg_epoch_loss=0.905205\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=234, batch=5 train loss <loss>=0.905204643806\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:05 INFO 139787867731776] Epoch[234] Batch [5]#011Speed: 4117.35 samples/sec#011loss=0.905205\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:05 INFO 139787867731776] Epoch[234] Batch[10] avg_epoch_loss=0.909371\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=234, batch=10 train loss <loss>=0.914371395111\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:05 INFO 139787867731776] Epoch[234] Batch [10]#011Speed: 3685.54 samples/sec#011loss=0.914371\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:05 INFO 139787867731776] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 579.9150466918945, \"sum\": 579.9150466918945, \"min\": 579.9150466918945}}, \"EndTime\": 1601747465.480486, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747464.900272}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:05 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1146.52171197 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:05 INFO 139787867731776] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=234, train loss <loss>=0.909371348945\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:05 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:05 INFO 139787867731776] Epoch[235] Batch[0] avg_epoch_loss=0.702568\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=235, batch=0 train loss <loss>=0.702568352222\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:05 INFO 139787867731776] Epoch[235] Batch[5] avg_epoch_loss=0.711540\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=235, batch=5 train loss <loss>=0.711539953947\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:05 INFO 139787867731776] Epoch[235] Batch [5]#011Speed: 3849.53 samples/sec#011loss=0.711540\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:06 INFO 139787867731776] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 527.1449089050293, \"sum\": 527.1449089050293, \"min\": 527.1449089050293}}, \"EndTime\": 1601747466.008052, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747465.480555}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:06 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1206.27240934 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:06 INFO 139787867731776] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=235, train loss <loss>=0.69980379343\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:06 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:06 INFO 139787867731776] Epoch[236] Batch[0] avg_epoch_loss=0.666775\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=236, batch=0 train loss <loss>=0.666774988174\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:06 INFO 139787867731776] Epoch[236] Batch[5] avg_epoch_loss=0.680248\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=236, batch=5 train loss <loss>=0.680247575045\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:06 INFO 139787867731776] Epoch[236] Batch [5]#011Speed: 3889.43 samples/sec#011loss=0.680248\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:06 INFO 139787867731776] processed a total of 580 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 548.2230186462402, \"sum\": 548.2230186462402, \"min\": 548.2230186462402}}, \"EndTime\": 1601747466.55677, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747466.008119}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:06 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1057.7703956 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:06 INFO 139787867731776] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=236, train loss <loss>=0.618354387023\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:06 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:06 INFO 139787867731776] Epoch[237] Batch[0] avg_epoch_loss=0.781630\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=237, batch=0 train loss <loss>=0.781629741192\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] Epoch[237] Batch[5] avg_epoch_loss=0.717408\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=237, batch=5 train loss <loss>=0.717408061028\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] Epoch[237] Batch [5]#011Speed: 4349.19 samples/sec#011loss=0.717408\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 519.3009376525879, \"sum\": 519.3009376525879, \"min\": 519.3009376525879}}, \"EndTime\": 1601747467.076479, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747466.556838}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1174.43877441 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=237, train loss <loss>=0.707181507349\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] Epoch[238] Batch[0] avg_epoch_loss=0.676307\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=238, batch=0 train loss <loss>=0.676306545734\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] Epoch[238] Batch[5] avg_epoch_loss=0.655528\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=238, batch=5 train loss <loss>=0.655528237422\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] Epoch[238] Batch [5]#011Speed: 4028.20 samples/sec#011loss=0.655528\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] Epoch[238] Batch[10] avg_epoch_loss=0.638816\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=238, batch=10 train loss <loss>=0.61876116991\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] Epoch[238] Batch [10]#011Speed: 4332.87 samples/sec#011loss=0.618761\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 590.303897857666, \"sum\": 590.303897857666, \"min\": 590.303897857666}}, \"EndTime\": 1601747467.667212, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747467.076545}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1095.88229132 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=238, train loss <loss>=0.638815934008\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:07 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:08 INFO 139787867731776] Epoch[239] Batch[0] avg_epoch_loss=0.554259\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=239, batch=0 train loss <loss>=0.554259419441\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:08 INFO 139787867731776] Epoch[239] Batch[5] avg_epoch_loss=0.611066\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=239, batch=5 train loss <loss>=0.61106556654\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:08 INFO 139787867731776] Epoch[239] Batch [5]#011Speed: 3940.35 samples/sec#011loss=0.611066\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:08 INFO 139787867731776] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 529.994010925293, \"sum\": 529.994010925293, \"min\": 529.994010925293}}, \"EndTime\": 1601747468.197572, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747467.667271}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:08 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1199.79173209 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:08 INFO 139787867731776] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=239, train loss <loss>=0.602597296238\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:08 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:08 INFO 139787867731776] Epoch[240] Batch[0] avg_epoch_loss=0.538063\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=240, batch=0 train loss <loss>=0.538062632084\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:08 INFO 139787867731776] Epoch[240] Batch[5] avg_epoch_loss=0.582877\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=240, batch=5 train loss <loss>=0.582876980305\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:08 INFO 139787867731776] Epoch[240] Batch [5]#011Speed: 4280.31 samples/sec#011loss=0.582877\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:08 INFO 139787867731776] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 520.9169387817383, \"sum\": 520.9169387817383, \"min\": 520.9169387817383}}, \"EndTime\": 1601747468.7189, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747468.197638}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:08 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1153.48244547 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:08 INFO 139787867731776] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=240, train loss <loss>=0.562945371866\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:08 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:09 INFO 139787867731776] Epoch[241] Batch[0] avg_epoch_loss=0.715525\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=241, batch=0 train loss <loss>=0.715525150299\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:09 INFO 139787867731776] Epoch[241] Batch[5] avg_epoch_loss=0.590944\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=241, batch=5 train loss <loss>=0.590944161018\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:09 INFO 139787867731776] Epoch[241] Batch [5]#011Speed: 4166.83 samples/sec#011loss=0.590944\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:09 INFO 139787867731776] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 516.0479545593262, \"sum\": 516.0479545593262, \"min\": 516.0479545593262}}, \"EndTime\": 1601747469.23545, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747468.718983}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:09 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1230.30060194 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:09 INFO 139787867731776] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=241, train loss <loss>=0.63150510788\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:09 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:09 INFO 139787867731776] Epoch[242] Batch[0] avg_epoch_loss=0.513204\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=242, batch=0 train loss <loss>=0.513204276562\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:09 INFO 139787867731776] Epoch[242] Batch[5] avg_epoch_loss=0.577655\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=242, batch=5 train loss <loss>=0.577655265729\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:09 INFO 139787867731776] Epoch[242] Batch [5]#011Speed: 3651.43 samples/sec#011loss=0.577655\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:09 INFO 139787867731776] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 539.808988571167, \"sum\": 539.808988571167, \"min\": 539.808988571167}}, \"EndTime\": 1601747469.775712, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747469.235506}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:09 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1172.4329877 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:09 INFO 139787867731776] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=242, train loss <loss>=0.591822952032\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:09 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] Epoch[243] Batch[0] avg_epoch_loss=0.540323\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=243, batch=0 train loss <loss>=0.540323257446\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] Epoch[243] Batch[5] avg_epoch_loss=0.595821\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=243, batch=5 train loss <loss>=0.595821330945\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] Epoch[243] Batch [5]#011Speed: 4016.47 samples/sec#011loss=0.595821\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] Epoch[243] Batch[10] avg_epoch_loss=0.584952\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=243, batch=10 train loss <loss>=0.571907937527\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] Epoch[243] Batch [10]#011Speed: 3876.91 samples/sec#011loss=0.571908\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 546.3688373565674, \"sum\": 546.3688373565674, \"min\": 546.3688373565674}}, \"EndTime\": 1601747470.322626, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747469.775774}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1189.37618227 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=243, train loss <loss>=0.584951606664\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] Epoch[244] Batch[0] avg_epoch_loss=0.645499\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=244, batch=0 train loss <loss>=0.645499229431\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] Epoch[244] Batch[5] avg_epoch_loss=0.607560\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=244, batch=5 train loss <loss>=0.607560406129\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] Epoch[244] Batch [5]#011Speed: 3977.39 samples/sec#011loss=0.607560\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] processed a total of 592 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 526.0651111602783, \"sum\": 526.0651111602783, \"min\": 526.0651111602783}}, \"EndTime\": 1601747470.849264, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747470.322688}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1125.09225063 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=244, train loss <loss>=0.602070984244\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:10 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] Epoch[245] Batch[0] avg_epoch_loss=0.646029\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=245, batch=0 train loss <loss>=0.646029174328\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] Epoch[245] Batch[5] avg_epoch_loss=0.641057\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=245, batch=5 train loss <loss>=0.641056666772\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] Epoch[245] Batch [5]#011Speed: 4011.64 samples/sec#011loss=0.641057\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] Epoch[245] Batch[10] avg_epoch_loss=0.625640\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=245, batch=10 train loss <loss>=0.607139503956\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] Epoch[245] Batch [10]#011Speed: 4154.60 samples/sec#011loss=0.607140\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 553.9240837097168, \"sum\": 553.9240837097168, \"min\": 553.9240837097168}}, \"EndTime\": 1601747471.403723, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747470.849332}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1193.10198475 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=245, train loss <loss>=0.625639774583\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] Epoch[246] Batch[0] avg_epoch_loss=0.613859\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=246, batch=0 train loss <loss>=0.613858640194\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] Epoch[246] Batch[5] avg_epoch_loss=0.680660\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=246, batch=5 train loss <loss>=0.68066033721\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] Epoch[246] Batch [5]#011Speed: 4240.54 samples/sec#011loss=0.680660\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] Epoch[246] Batch[10] avg_epoch_loss=0.678580\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=246, batch=10 train loss <loss>=0.676083528996\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] Epoch[246] Batch [10]#011Speed: 4149.37 samples/sec#011loss=0.676084\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 563.866138458252, \"sum\": 563.866138458252, \"min\": 563.866138458252}}, \"EndTime\": 1601747471.967951, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747471.403786}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1177.39229191 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=246, train loss <loss>=0.67857996984\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:11 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:12 INFO 139787867731776] Epoch[247] Batch[0] avg_epoch_loss=0.703226\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=247, batch=0 train loss <loss>=0.703225672245\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:12 INFO 139787867731776] Epoch[247] Batch[5] avg_epoch_loss=0.611255\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=247, batch=5 train loss <loss>=0.611254662275\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:12 INFO 139787867731776] Epoch[247] Batch [5]#011Speed: 3751.85 samples/sec#011loss=0.611255\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:12 INFO 139787867731776] processed a total of 593 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 563.7650489807129, \"sum\": 563.7650489807129, \"min\": 563.7650489807129}}, \"EndTime\": 1601747472.532073, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747471.968013}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:12 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1051.69428241 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:12 INFO 139787867731776] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=247, train loss <loss>=0.592183598876\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:12 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:12 INFO 139787867731776] Epoch[248] Batch[0] avg_epoch_loss=0.556511\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=248, batch=0 train loss <loss>=0.556510567665\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] Epoch[248] Batch[5] avg_epoch_loss=0.609570\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=248, batch=5 train loss <loss>=0.609569996595\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] Epoch[248] Batch [5]#011Speed: 3648.43 samples/sec#011loss=0.609570\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] Epoch[248] Batch[10] avg_epoch_loss=0.666381\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=248, batch=10 train loss <loss>=0.734554731846\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] Epoch[248] Batch [10]#011Speed: 3811.93 samples/sec#011loss=0.734555\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 567.0599937438965, \"sum\": 567.0599937438965, \"min\": 567.0599937438965}}, \"EndTime\": 1601747473.099631, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747472.532128}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1156.64032359 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=248, train loss <loss>=0.666381239891\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] Epoch[249] Batch[0] avg_epoch_loss=0.585916\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=249, batch=0 train loss <loss>=0.585915982723\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] Epoch[249] Batch[5] avg_epoch_loss=0.594259\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=249, batch=5 train loss <loss>=0.594258944194\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] Epoch[249] Batch [5]#011Speed: 3777.10 samples/sec#011loss=0.594259\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 524.2230892181396, \"sum\": 524.2230892181396, \"min\": 524.2230892181396}}, \"EndTime\": 1601747473.624281, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747473.099696}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1172.94753719 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=249, train loss <loss>=0.596419173479\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:13 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:14 INFO 139787867731776] Epoch[250] Batch[0] avg_epoch_loss=0.639616\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=250, batch=0 train loss <loss>=0.639615535736\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:14 INFO 139787867731776] Epoch[250] Batch[5] avg_epoch_loss=0.666875\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=250, batch=5 train loss <loss>=0.666875332594\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:14 INFO 139787867731776] Epoch[250] Batch [5]#011Speed: 4287.24 samples/sec#011loss=0.666875\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:14 INFO 139787867731776] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 537.0471477508545, \"sum\": 537.0471477508545, \"min\": 537.0471477508545}}, \"EndTime\": 1601747474.161741, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747473.624345}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:14 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1178.45244324 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:14 INFO 139787867731776] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=250, train loss <loss>=0.650233745575\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:14 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:14 INFO 139787867731776] Epoch[251] Batch[0] avg_epoch_loss=0.629565\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=251, batch=0 train loss <loss>=0.629564881325\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:14 INFO 139787867731776] Epoch[251] Batch[5] avg_epoch_loss=0.655741\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=251, batch=5 train loss <loss>=0.655741035938\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:14 INFO 139787867731776] Epoch[251] Batch [5]#011Speed: 4293.62 samples/sec#011loss=0.655741\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:14 INFO 139787867731776] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 538.9959812164307, \"sum\": 538.9959812164307, \"min\": 538.9959812164307}}, \"EndTime\": 1601747474.70114, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747474.161807}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:14 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1142.65590626 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:14 INFO 139787867731776] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=251, train loss <loss>=0.640664553642\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:14 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] Epoch[252] Batch[0] avg_epoch_loss=0.534732\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=252, batch=0 train loss <loss>=0.53473174572\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] Epoch[252] Batch[5] avg_epoch_loss=0.578190\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=252, batch=5 train loss <loss>=0.578189909458\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] Epoch[252] Batch [5]#011Speed: 4031.49 samples/sec#011loss=0.578190\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] Epoch[252] Batch[10] avg_epoch_loss=0.554617\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=252, batch=10 train loss <loss>=0.526329654455\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] Epoch[252] Batch [10]#011Speed: 4036.33 samples/sec#011loss=0.526330\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 543.6360836029053, \"sum\": 543.6360836029053, \"min\": 543.6360836029053}}, \"EndTime\": 1601747475.245191, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747474.701207}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1197.31162249 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=252, train loss <loss>=0.554617066275\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] Epoch[253] Batch[0] avg_epoch_loss=0.573311\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=253, batch=0 train loss <loss>=0.573310911655\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] Epoch[253] Batch[5] avg_epoch_loss=0.604801\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=253, batch=5 train loss <loss>=0.604801426331\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] Epoch[253] Batch [5]#011Speed: 4105.94 samples/sec#011loss=0.604801\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] Epoch[253] Batch[10] avg_epoch_loss=0.608674\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=253, batch=10 train loss <loss>=0.613320672512\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] Epoch[253] Batch [10]#011Speed: 4169.77 samples/sec#011loss=0.613321\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 548.4659671783447, \"sum\": 548.4659671783447, \"min\": 548.4659671783447}}, \"EndTime\": 1601747475.794088, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747475.245244}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1206.78936736 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=253, train loss <loss>=0.608673810959\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:15 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] Epoch[254] Batch[0] avg_epoch_loss=0.564033\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=254, batch=0 train loss <loss>=0.564033031464\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] Epoch[254] Batch[5] avg_epoch_loss=0.588353\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=254, batch=5 train loss <loss>=0.588352948427\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] Epoch[254] Batch [5]#011Speed: 3276.61 samples/sec#011loss=0.588353\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 556.5099716186523, \"sum\": 556.5099716186523, \"min\": 556.5099716186523}}, \"EndTime\": 1601747476.35097, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747475.794153}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1101.32554085 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=254, train loss <loss>=0.574439477921\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] Epoch[255] Batch[0] avg_epoch_loss=0.658679\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=255, batch=0 train loss <loss>=0.658678889275\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] Epoch[255] Batch[5] avg_epoch_loss=0.594779\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=255, batch=5 train loss <loss>=0.594779163599\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] Epoch[255] Batch [5]#011Speed: 4323.16 samples/sec#011loss=0.594779\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] Epoch[255] Batch[10] avg_epoch_loss=0.581633\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=255, batch=10 train loss <loss>=0.56585740447\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] Epoch[255] Batch [10]#011Speed: 4224.54 samples/sec#011loss=0.565857\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 530.4930210113525, \"sum\": 530.4930210113525, \"min\": 530.4930210113525}}, \"EndTime\": 1601747476.882127, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747476.351032}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1219.38837604 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=255, train loss <loss>=0.58163290945\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:16 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] Epoch[256] Batch[0] avg_epoch_loss=0.663918\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=256, batch=0 train loss <loss>=0.663917779922\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] Epoch[256] Batch[5] avg_epoch_loss=0.763607\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=256, batch=5 train loss <loss>=0.763606667519\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] Epoch[256] Batch [5]#011Speed: 3950.81 samples/sec#011loss=0.763607\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] Epoch[256] Batch[10] avg_epoch_loss=0.717451\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=256, batch=10 train loss <loss>=0.662064945698\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] Epoch[256] Batch [10]#011Speed: 4091.82 samples/sec#011loss=0.662065\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 545.698881149292, \"sum\": 545.698881149292, \"min\": 545.698881149292}}, \"EndTime\": 1601747477.428299, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747476.882196}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1256.88938358 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=256, train loss <loss>=0.717451339418\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] Epoch[257] Batch[0] avg_epoch_loss=0.679002\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=257, batch=0 train loss <loss>=0.679001629353\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] Epoch[257] Batch[5] avg_epoch_loss=0.626754\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=257, batch=5 train loss <loss>=0.626754025618\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] Epoch[257] Batch [5]#011Speed: 4341.58 samples/sec#011loss=0.626754\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] Epoch[257] Batch[10] avg_epoch_loss=0.553531\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=257, batch=10 train loss <loss>=0.465664105117\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] Epoch[257] Batch [10]#011Speed: 4284.08 samples/sec#011loss=0.465664\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 569.4739818572998, \"sum\": 569.4739818572998, \"min\": 569.4739818572998}}, \"EndTime\": 1601747477.998179, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747477.428362}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1160.53710541 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=257, train loss <loss>=0.553531334481\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:17 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:18 INFO 139787867731776] Epoch[258] Batch[0] avg_epoch_loss=0.648089\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=258, batch=0 train loss <loss>=0.648088574409\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:18 INFO 139787867731776] Epoch[258] Batch[5] avg_epoch_loss=0.590430\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=258, batch=5 train loss <loss>=0.590429613988\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:18 INFO 139787867731776] Epoch[258] Batch [5]#011Speed: 4291.55 samples/sec#011loss=0.590430\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:18 INFO 139787867731776] Epoch[258] Batch[10] avg_epoch_loss=0.555547\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=258, batch=10 train loss <loss>=0.51368791759\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:18 INFO 139787867731776] Epoch[258] Batch [10]#011Speed: 3687.95 samples/sec#011loss=0.513688\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:18 INFO 139787867731776] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 538.938045501709, \"sum\": 538.938045501709, \"min\": 538.938045501709}}, \"EndTime\": 1601747478.537523, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747477.998241}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:18 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1194.74804298 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:18 INFO 139787867731776] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=258, train loss <loss>=0.555547024716\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:18 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:18 INFO 139787867731776] Epoch[259] Batch[0] avg_epoch_loss=0.559071\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=259, batch=0 train loss <loss>=0.559070706367\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:18 INFO 139787867731776] Epoch[259] Batch[5] avg_epoch_loss=0.570692\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=259, batch=5 train loss <loss>=0.570692141851\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:18 INFO 139787867731776] Epoch[259] Batch [5]#011Speed: 4242.12 samples/sec#011loss=0.570692\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:19 INFO 139787867731776] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 523.9119529724121, \"sum\": 523.9119529724121, \"min\": 523.9119529724121}}, \"EndTime\": 1601747479.061863, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747478.537584}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:19 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1207.99014678 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:19 INFO 139787867731776] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=259, train loss <loss>=0.581092047691\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:19 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:19 INFO 139787867731776] Epoch[260] Batch[0] avg_epoch_loss=0.576912\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=260, batch=0 train loss <loss>=0.57691192627\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:19 INFO 139787867731776] Epoch[260] Batch[5] avg_epoch_loss=0.560690\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=260, batch=5 train loss <loss>=0.560689806938\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:19 INFO 139787867731776] Epoch[260] Batch [5]#011Speed: 3604.82 samples/sec#011loss=0.560690\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:19 INFO 139787867731776] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 538.2928848266602, \"sum\": 538.2928848266602, \"min\": 538.2928848266602}}, \"EndTime\": 1601747479.600576, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747479.06193}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:19 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1136.68687767 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:19 INFO 139787867731776] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=260, train loss <loss>=0.555369019508\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:19 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:19 INFO 139787867731776] Epoch[261] Batch[0] avg_epoch_loss=0.466074\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=261, batch=0 train loss <loss>=0.466074079275\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] Epoch[261] Batch[5] avg_epoch_loss=0.549781\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=261, batch=5 train loss <loss>=0.549780557553\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] Epoch[261] Batch [5]#011Speed: 4303.45 samples/sec#011loss=0.549781\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] Epoch[261] Batch[10] avg_epoch_loss=0.537212\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=261, batch=10 train loss <loss>=0.522130793333\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] Epoch[261] Batch [10]#011Speed: 3708.85 samples/sec#011loss=0.522131\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 552.9298782348633, \"sum\": 552.9298782348633, \"min\": 552.9298782348633}}, \"EndTime\": 1601747480.153971, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747479.60066}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1159.08159122 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=261, train loss <loss>=0.537212482908\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] Epoch[262] Batch[0] avg_epoch_loss=0.826636\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=262, batch=0 train loss <loss>=0.826635718346\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] Epoch[262] Batch[5] avg_epoch_loss=0.788938\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=262, batch=5 train loss <loss>=0.788937966029\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] Epoch[262] Batch [5]#011Speed: 4282.48 samples/sec#011loss=0.788938\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] Epoch[262] Batch[10] avg_epoch_loss=0.731927\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=262, batch=10 train loss <loss>=0.663514769077\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] Epoch[262] Batch [10]#011Speed: 3738.32 samples/sec#011loss=0.663515\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 547.996997833252, \"sum\": 547.996997833252, \"min\": 547.996997833252}}, \"EndTime\": 1601747480.702386, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747480.154035}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1191.40606491 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=262, train loss <loss>=0.73192742196\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:20 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] Epoch[263] Batch[0] avg_epoch_loss=0.577923\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=263, batch=0 train loss <loss>=0.577923357487\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] Epoch[263] Batch[5] avg_epoch_loss=0.658894\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=263, batch=5 train loss <loss>=0.658893962701\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] Epoch[263] Batch [5]#011Speed: 3910.88 samples/sec#011loss=0.658894\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] Epoch[263] Batch[10] avg_epoch_loss=0.607923\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=263, batch=10 train loss <loss>=0.546757990122\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] Epoch[263] Batch [10]#011Speed: 4279.85 samples/sec#011loss=0.546758\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 565.2709007263184, \"sum\": 565.2709007263184, \"min\": 565.2709007263184}}, \"EndTime\": 1601747481.268025, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747480.702449}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1178.00188166 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=263, train loss <loss>=0.607923066074\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] Epoch[264] Batch[0] avg_epoch_loss=0.663807\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=264, batch=0 train loss <loss>=0.663806796074\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] Epoch[264] Batch[5] avg_epoch_loss=0.599334\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=264, batch=5 train loss <loss>=0.599334170421\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] Epoch[264] Batch [5]#011Speed: 4366.93 samples/sec#011loss=0.599334\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 568.0389404296875, \"sum\": 568.0389404296875, \"min\": 568.0389404296875}}, \"EndTime\": 1601747481.836479, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747481.268089}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1087.76776202 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=264, train loss <loss>=0.550950416923\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:21 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] Epoch[265] Batch[0] avg_epoch_loss=0.568136\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=265, batch=0 train loss <loss>=0.568136453629\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] Epoch[265] Batch[5] avg_epoch_loss=0.574724\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=265, batch=5 train loss <loss>=0.574723780155\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] Epoch[265] Batch [5]#011Speed: 4312.55 samples/sec#011loss=0.574724\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] Epoch[265] Batch[10] avg_epoch_loss=0.571369\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=265, batch=10 train loss <loss>=0.56734367609\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] Epoch[265] Batch [10]#011Speed: 4134.95 samples/sec#011loss=0.567344\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 550.7981777191162, \"sum\": 550.7981777191162, \"min\": 550.7981777191162}}, \"EndTime\": 1601747482.387716, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747481.836545}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1237.99194412 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=265, train loss <loss>=0.571369187398\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] Epoch[266] Batch[0] avg_epoch_loss=0.522057\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=266, batch=0 train loss <loss>=0.522056639194\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] Epoch[266] Batch[5] avg_epoch_loss=0.621655\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=266, batch=5 train loss <loss>=0.621654947599\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] Epoch[266] Batch [5]#011Speed: 4274.37 samples/sec#011loss=0.621655\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 553.7059307098389, \"sum\": 553.7059307098389, \"min\": 553.7059307098389}}, \"EndTime\": 1601747482.941783, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747482.38778}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1097.8752011 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=266, train loss <loss>=0.627091300488\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:22 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] Epoch[267] Batch[0] avg_epoch_loss=0.595440\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=267, batch=0 train loss <loss>=0.595439791679\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] Epoch[267] Batch[5] avg_epoch_loss=0.530928\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=267, batch=5 train loss <loss>=0.530927926302\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] Epoch[267] Batch [5]#011Speed: 4349.01 samples/sec#011loss=0.530928\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] Epoch[267] Batch[10] avg_epoch_loss=0.600274\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=267, batch=10 train loss <loss>=0.68348839879\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] Epoch[267] Batch [10]#011Speed: 4281.40 samples/sec#011loss=0.683488\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 527.9030799865723, \"sum\": 527.9030799865723, \"min\": 527.9030799865723}}, \"EndTime\": 1601747483.47018, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747482.941843}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1229.1786066 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=267, train loss <loss>=0.600273595615\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] Epoch[268] Batch[0] avg_epoch_loss=0.574859\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=268, batch=0 train loss <loss>=0.574858546257\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] Epoch[268] Batch[5] avg_epoch_loss=0.592776\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=268, batch=5 train loss <loss>=0.592776377996\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] Epoch[268] Batch [5]#011Speed: 3857.61 samples/sec#011loss=0.592776\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 528.7630558013916, \"sum\": 528.7630558013916, \"min\": 528.7630558013916}}, \"EndTime\": 1601747483.999341, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747483.470241}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1210.16356239 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=268, train loss <loss>=0.580512559414\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:23 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:24 INFO 139787867731776] Epoch[269] Batch[0] avg_epoch_loss=0.439390\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=269, batch=0 train loss <loss>=0.439389765263\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:24 INFO 139787867731776] Epoch[269] Batch[5] avg_epoch_loss=0.540115\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=269, batch=5 train loss <loss>=0.540115391215\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:24 INFO 139787867731776] Epoch[269] Batch [5]#011Speed: 3974.85 samples/sec#011loss=0.540115\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:24 INFO 139787867731776] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 514.585018157959, \"sum\": 514.585018157959, \"min\": 514.585018157959}}, \"EndTime\": 1601747484.514357, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747483.999402}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:24 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1173.54221674 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:24 INFO 139787867731776] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=269, train loss <loss>=0.536231324077\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:24 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:24 INFO 139787867731776] Epoch[270] Batch[0] avg_epoch_loss=0.567375\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=270, batch=0 train loss <loss>=0.567375421524\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] Epoch[270] Batch[5] avg_epoch_loss=0.554275\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=270, batch=5 train loss <loss>=0.554274976254\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] Epoch[270] Batch [5]#011Speed: 4333.57 samples/sec#011loss=0.554275\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] Epoch[270] Batch[10] avg_epoch_loss=0.605872\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=270, batch=10 train loss <loss>=0.667787492275\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] Epoch[270] Batch [10]#011Speed: 4294.25 samples/sec#011loss=0.667787\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 560.8439445495605, \"sum\": 560.8439445495605, \"min\": 560.8439445495605}}, \"EndTime\": 1601747485.075604, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747484.51442}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1165.88517343 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=270, train loss <loss>=0.605871574445\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] Epoch[271] Batch[0] avg_epoch_loss=0.583564\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=271, batch=0 train loss <loss>=0.583563625813\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] Epoch[271] Batch[5] avg_epoch_loss=0.585068\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=271, batch=5 train loss <loss>=0.58506812652\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] Epoch[271] Batch [5]#011Speed: 4223.47 samples/sec#011loss=0.585068\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] Epoch[271] Batch[10] avg_epoch_loss=0.572767\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=271, batch=10 train loss <loss>=0.558005976677\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] Epoch[271] Batch [10]#011Speed: 4262.29 samples/sec#011loss=0.558006\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 532.3150157928467, \"sum\": 532.3150157928467, \"min\": 532.3150157928467}}, \"EndTime\": 1601747485.608362, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747485.075675}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1239.64357971 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=271, train loss <loss>=0.572767149318\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] Epoch[272] Batch[0] avg_epoch_loss=0.542003\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=272, batch=0 train loss <loss>=0.542002558708\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] Epoch[272] Batch[5] avg_epoch_loss=0.574061\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=272, batch=5 train loss <loss>=0.574060911934\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] Epoch[272] Batch [5]#011Speed: 4119.19 samples/sec#011loss=0.574061\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 528.2270908355713, \"sum\": 528.2270908355713, \"min\": 528.2270908355713}}, \"EndTime\": 1601747486.136975, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747485.608427}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1162.15277422 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=272, train loss <loss>=0.531200540066\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_0a1cc77a-1fda-49f6-8cf9-c44e0d15923b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.293058395385742, \"sum\": 6.293058395385742, \"min\": 6.293058395385742}}, \"EndTime\": 1601747486.143813, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747486.137049}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] Epoch[273] Batch[0] avg_epoch_loss=0.479735\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=273, batch=0 train loss <loss>=0.479735285044\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] Epoch[273] Batch[5] avg_epoch_loss=0.502090\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=273, batch=5 train loss <loss>=0.502090017001\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] Epoch[273] Batch [5]#011Speed: 4333.17 samples/sec#011loss=0.502090\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] Epoch[273] Batch[10] avg_epoch_loss=0.486667\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=273, batch=10 train loss <loss>=0.468158721924\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] Epoch[273] Batch [10]#011Speed: 4305.65 samples/sec#011loss=0.468159\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 548.1359958648682, \"sum\": 548.1359958648682, \"min\": 548.1359958648682}}, \"EndTime\": 1601747486.692066, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747486.143873}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1202.06554192 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=273, train loss <loss>=0.486666701057\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:26 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_011df906-9741-4ef4-bf22-57f248c07280-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.104946136474609, \"sum\": 6.104946136474609, \"min\": 6.104946136474609}}, \"EndTime\": 1601747486.698618, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747486.692123}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:27 INFO 139787867731776] Epoch[274] Batch[0] avg_epoch_loss=0.517131\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=274, batch=0 train loss <loss>=0.517131388187\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:27 INFO 139787867731776] Epoch[274] Batch[5] avg_epoch_loss=0.541215\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=274, batch=5 train loss <loss>=0.541214644909\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:27 INFO 139787867731776] Epoch[274] Batch [5]#011Speed: 4263.49 samples/sec#011loss=0.541215\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:27 INFO 139787867731776] processed a total of 580 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 508.0299377441406, \"sum\": 508.0299377441406, \"min\": 508.0299377441406}}, \"EndTime\": 1601747487.206751, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747486.698674}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:27 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1141.46733227 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:27 INFO 139787867731776] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=274, train loss <loss>=0.511416101456\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:27 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:27 INFO 139787867731776] Epoch[275] Batch[0] avg_epoch_loss=0.674062\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=275, batch=0 train loss <loss>=0.674061775208\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:27 INFO 139787867731776] Epoch[275] Batch[5] avg_epoch_loss=0.625937\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=275, batch=5 train loss <loss>=0.625937312841\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:27 INFO 139787867731776] Epoch[275] Batch [5]#011Speed: 4295.37 samples/sec#011loss=0.625937\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:27 INFO 139787867731776] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 510.95008850097656, \"sum\": 510.95008850097656, \"min\": 510.95008850097656}}, \"EndTime\": 1601747487.718159, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747487.206812}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:27 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1228.8110829 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:27 INFO 139787867731776] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=275, train loss <loss>=0.620234829187\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:27 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] Epoch[276] Batch[0] avg_epoch_loss=0.596956\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=276, batch=0 train loss <loss>=0.59695571661\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] Epoch[276] Batch[5] avg_epoch_loss=0.609457\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=276, batch=5 train loss <loss>=0.609457165003\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] Epoch[276] Batch [5]#011Speed: 3750.19 samples/sec#011loss=0.609457\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] Epoch[276] Batch[10] avg_epoch_loss=0.675091\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=276, batch=10 train loss <loss>=0.753851759434\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] Epoch[276] Batch [10]#011Speed: 4288.45 samples/sec#011loss=0.753852\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 541.7981147766113, \"sum\": 541.7981147766113, \"min\": 541.7981147766113}}, \"EndTime\": 1601747488.260461, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747487.718235}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1203.19854702 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=276, train loss <loss>=0.675091071562\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] Epoch[277] Batch[0] avg_epoch_loss=0.783905\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=277, batch=0 train loss <loss>=0.783905446529\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] Epoch[277] Batch[5] avg_epoch_loss=0.876249\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=277, batch=5 train loss <loss>=0.876248677572\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] Epoch[277] Batch [5]#011Speed: 3927.06 samples/sec#011loss=0.876249\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 542.8750514984131, \"sum\": 542.8750514984131, \"min\": 542.8750514984131}}, \"EndTime\": 1601747488.803724, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747488.260523}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1154.77475946 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=277, train loss <loss>=0.837569028139\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:28 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:29 INFO 139787867731776] Epoch[278] Batch[0] avg_epoch_loss=0.774480\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=278, batch=0 train loss <loss>=0.774480104446\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:29 INFO 139787867731776] Epoch[278] Batch[5] avg_epoch_loss=0.677043\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=278, batch=5 train loss <loss>=0.677043209473\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:29 INFO 139787867731776] Epoch[278] Batch [5]#011Speed: 4279.38 samples/sec#011loss=0.677043\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:29 INFO 139787867731776] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 521.3940143585205, \"sum\": 521.3940143585205, \"min\": 521.3940143585205}}, \"EndTime\": 1601747489.325731, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747488.803781}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:29 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1221.50696552 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:29 INFO 139787867731776] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=278, train loss <loss>=0.681441485882\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:29 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:29 INFO 139787867731776] Epoch[279] Batch[0] avg_epoch_loss=0.602723\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=279, batch=0 train loss <loss>=0.602723479271\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:29 INFO 139787867731776] Epoch[279] Batch[5] avg_epoch_loss=0.570753\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=279, batch=5 train loss <loss>=0.570753400524\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:29 INFO 139787867731776] Epoch[279] Batch [5]#011Speed: 4074.92 samples/sec#011loss=0.570753\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:29 INFO 139787867731776] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 512.9098892211914, \"sum\": 512.9098892211914, \"min\": 512.9098892211914}}, \"EndTime\": 1601747489.839129, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747489.325791}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:29 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1173.48925257 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:29 INFO 139787867731776] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=279, train loss <loss>=0.558803902566\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:29 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:30 INFO 139787867731776] Epoch[280] Batch[0] avg_epoch_loss=0.382503\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=280, batch=0 train loss <loss>=0.382502764463\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:30 INFO 139787867731776] Epoch[280] Batch[5] avg_epoch_loss=0.531346\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=280, batch=5 train loss <loss>=0.531346226732\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:30 INFO 139787867731776] Epoch[280] Batch [5]#011Speed: 4329.06 samples/sec#011loss=0.531346\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:30 INFO 139787867731776] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 508.76498222351074, \"sum\": 508.76498222351074, \"min\": 508.76498222351074}}, \"EndTime\": 1601747490.348348, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747489.839189}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:30 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1190.90206133 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:30 INFO 139787867731776] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=280, train loss <loss>=0.572060284019\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:30 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:30 INFO 139787867731776] Epoch[281] Batch[0] avg_epoch_loss=0.548283\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=281, batch=0 train loss <loss>=0.548282682896\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:30 INFO 139787867731776] Epoch[281] Batch[5] avg_epoch_loss=0.553735\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=281, batch=5 train loss <loss>=0.553734540939\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:30 INFO 139787867731776] Epoch[281] Batch [5]#011Speed: 3744.10 samples/sec#011loss=0.553735\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:30 INFO 139787867731776] processed a total of 590 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 530.372142791748, \"sum\": 530.372142791748, \"min\": 530.372142791748}}, \"EndTime\": 1601747490.879231, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747490.34841}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:30 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1112.21895974 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:30 INFO 139787867731776] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=281, train loss <loss>=0.520976558328\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:30 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] Epoch[282] Batch[0] avg_epoch_loss=0.532668\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=282, batch=0 train loss <loss>=0.532667934895\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] Epoch[282] Batch[5] avg_epoch_loss=0.509764\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=282, batch=5 train loss <loss>=0.509763712684\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] Epoch[282] Batch [5]#011Speed: 3551.41 samples/sec#011loss=0.509764\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] Epoch[282] Batch[10] avg_epoch_loss=0.561872\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=282, batch=10 train loss <loss>=0.62440226078\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] Epoch[282] Batch [10]#011Speed: 3054.72 samples/sec#011loss=0.624402\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 576.348066329956, \"sum\": 576.348066329956, \"min\": 576.348066329956}}, \"EndTime\": 1601747491.456227, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747490.879296}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1118.92933231 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=282, train loss <loss>=0.561872143637\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] Epoch[283] Batch[0] avg_epoch_loss=0.531392\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=283, batch=0 train loss <loss>=0.531392395496\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] Epoch[283] Batch[5] avg_epoch_loss=0.540959\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=283, batch=5 train loss <loss>=0.540959194303\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] Epoch[283] Batch [5]#011Speed: 4220.58 samples/sec#011loss=0.540959\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 535.1250171661377, \"sum\": 535.1250171661377, \"min\": 535.1250171661377}}, \"EndTime\": 1601747491.99171, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747491.456292}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1184.55507545 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=283, train loss <loss>=0.534446948767\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:31 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:32 INFO 139787867731776] Epoch[284] Batch[0] avg_epoch_loss=0.499739\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=284, batch=0 train loss <loss>=0.499739140272\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:32 INFO 139787867731776] Epoch[284] Batch[5] avg_epoch_loss=0.545135\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=284, batch=5 train loss <loss>=0.545135016243\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:32 INFO 139787867731776] Epoch[284] Batch [5]#011Speed: 4316.68 samples/sec#011loss=0.545135\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:32 INFO 139787867731776] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 528.3229351043701, \"sum\": 528.3229351043701, \"min\": 528.3229351043701}}, \"EndTime\": 1601747492.520448, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747491.991778}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:32 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1180.87411494 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:32 INFO 139787867731776] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=284, train loss <loss>=0.549260771275\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:32 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:32 INFO 139787867731776] Epoch[285] Batch[0] avg_epoch_loss=0.496324\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=285, batch=0 train loss <loss>=0.496323734522\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:32 INFO 139787867731776] Epoch[285] Batch[5] avg_epoch_loss=0.554044\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=285, batch=5 train loss <loss>=0.554043789705\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:32 INFO 139787867731776] Epoch[285] Batch [5]#011Speed: 4344.52 samples/sec#011loss=0.554044\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:33 INFO 139787867731776] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 537.7438068389893, \"sum\": 537.7438068389893, \"min\": 537.7438068389893}}, \"EndTime\": 1601747493.058628, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747492.520517}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:33 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1184.38729228 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:33 INFO 139787867731776] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=285, train loss <loss>=0.526422640681\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:33 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:33 INFO 139787867731776] Epoch[286] Batch[0] avg_epoch_loss=0.533262\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=286, batch=0 train loss <loss>=0.53326189518\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:33 INFO 139787867731776] Epoch[286] Batch[5] avg_epoch_loss=0.523880\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=286, batch=5 train loss <loss>=0.523879597584\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:33 INFO 139787867731776] Epoch[286] Batch [5]#011Speed: 4068.30 samples/sec#011loss=0.523880\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:33 INFO 139787867731776] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 517.3859596252441, \"sum\": 517.3859596252441, \"min\": 517.3859596252441}}, \"EndTime\": 1601747493.576597, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747493.058683}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:33 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1232.84758336 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:33 INFO 139787867731776] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=286, train loss <loss>=0.542524898052\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:33 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:33 INFO 139787867731776] Epoch[287] Batch[0] avg_epoch_loss=0.606826\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=287, batch=0 train loss <loss>=0.606825530529\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:34 INFO 139787867731776] Epoch[287] Batch[5] avg_epoch_loss=0.588101\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=287, batch=5 train loss <loss>=0.588100622098\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:34 INFO 139787867731776] Epoch[287] Batch [5]#011Speed: 4221.03 samples/sec#011loss=0.588101\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:34 INFO 139787867731776] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 535.383939743042, \"sum\": 535.383939743042, \"min\": 535.383939743042}}, \"EndTime\": 1601747494.112413, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747493.576679}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:34 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1154.10930485 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:34 INFO 139787867731776] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=287, train loss <loss>=0.516981723905\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:34 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:34 INFO 139787867731776] Epoch[288] Batch[0] avg_epoch_loss=0.657043\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=288, batch=0 train loss <loss>=0.657043337822\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:34 INFO 139787867731776] Epoch[288] Batch[5] avg_epoch_loss=0.602389\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=288, batch=5 train loss <loss>=0.602388670047\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:34 INFO 139787867731776] Epoch[288] Batch [5]#011Speed: 4132.33 samples/sec#011loss=0.602389\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:34 INFO 139787867731776] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 542.0148372650146, \"sum\": 542.0148372650146, \"min\": 542.0148372650146}}, \"EndTime\": 1601747494.654864, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747494.112476}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:34 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1152.90870373 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:34 INFO 139787867731776] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=288, train loss <loss>=0.57918690145\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:34 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] Epoch[289] Batch[0] avg_epoch_loss=0.487960\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=289, batch=0 train loss <loss>=0.487959593534\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] Epoch[289] Batch[5] avg_epoch_loss=0.549353\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=289, batch=5 train loss <loss>=0.549353256822\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] Epoch[289] Batch [5]#011Speed: 4053.14 samples/sec#011loss=0.549353\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 520.449161529541, \"sum\": 520.449161529541, \"min\": 520.449161529541}}, \"EndTime\": 1601747495.175784, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747494.654922}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1183.38648552 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=289, train loss <loss>=0.504436004162\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] Epoch[290] Batch[0] avg_epoch_loss=0.488645\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=290, batch=0 train loss <loss>=0.488645195961\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] Epoch[290] Batch[5] avg_epoch_loss=0.514700\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=290, batch=5 train loss <loss>=0.514700407783\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] Epoch[290] Batch [5]#011Speed: 4322.23 samples/sec#011loss=0.514700\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] Epoch[290] Batch[10] avg_epoch_loss=0.555601\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=290, batch=10 train loss <loss>=0.604681271315\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] Epoch[290] Batch [10]#011Speed: 4271.91 samples/sec#011loss=0.604681\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 551.5189170837402, \"sum\": 551.5189170837402, \"min\": 551.5189170837402}}, \"EndTime\": 1601747495.727693, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747495.175847}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1169.29380142 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=290, train loss <loss>=0.555600800297\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:35 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] Epoch[291] Batch[0] avg_epoch_loss=0.543914\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] #quality_metric: host=algo-1, epoch=291, batch=0 train loss <loss>=0.543913662434\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] Epoch[291] Batch[5] avg_epoch_loss=0.515168\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] #quality_metric: host=algo-1, epoch=291, batch=5 train loss <loss>=0.515168150266\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] Epoch[291] Batch [5]#011Speed: 4138.64 samples/sec#011loss=0.515168\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] Epoch[291] Batch[10] avg_epoch_loss=0.663508\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] #quality_metric: host=algo-1, epoch=291, batch=10 train loss <loss>=0.841516458988\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] Epoch[291] Batch [10]#011Speed: 3968.99 samples/sec#011loss=0.841516\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 541.4021015167236, \"sum\": 541.4021015167236, \"min\": 541.4021015167236}}, \"EndTime\": 1601747496.269613, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747495.727757}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1211.45427255 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] #quality_metric: host=algo-1, epoch=291, train loss <loss>=0.663508290594\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] Epoch[292] Batch[0] avg_epoch_loss=0.564689\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] #quality_metric: host=algo-1, epoch=292, batch=0 train loss <loss>=0.564688503742\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] Epoch[292] Batch[5] avg_epoch_loss=0.550217\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] #quality_metric: host=algo-1, epoch=292, batch=5 train loss <loss>=0.550216684739\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] Epoch[292] Batch [5]#011Speed: 3992.57 samples/sec#011loss=0.550217\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 529.7331809997559, \"sum\": 529.7331809997559, \"min\": 529.7331809997559}}, \"EndTime\": 1601747496.799798, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747496.269677}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1170.18881645 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] #quality_metric: host=algo-1, epoch=292, train loss <loss>=0.547342658043\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:36 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] Epoch[293] Batch[0] avg_epoch_loss=0.552424\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] #quality_metric: host=algo-1, epoch=293, batch=0 train loss <loss>=0.552424490452\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] Epoch[293] Batch[5] avg_epoch_loss=0.529773\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] #quality_metric: host=algo-1, epoch=293, batch=5 train loss <loss>=0.529773448904\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] Epoch[293] Batch [5]#011Speed: 4252.44 samples/sec#011loss=0.529773\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] Epoch[293] Batch[10] avg_epoch_loss=0.519264\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] #quality_metric: host=algo-1, epoch=293, batch=10 train loss <loss>=0.506652444601\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] Epoch[293] Batch [10]#011Speed: 4285.41 samples/sec#011loss=0.506652\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 557.6989650726318, \"sum\": 557.6989650726318, \"min\": 557.6989650726318}}, \"EndTime\": 1601747497.357909, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747496.799862}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1229.85343877 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] #quality_metric: host=algo-1, epoch=293, train loss <loss>=0.519263901494\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] Epoch[294] Batch[0] avg_epoch_loss=0.611767\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] #quality_metric: host=algo-1, epoch=294, batch=0 train loss <loss>=0.611766517162\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] Epoch[294] Batch[5] avg_epoch_loss=0.547795\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] #quality_metric: host=algo-1, epoch=294, batch=5 train loss <loss>=0.547795196374\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] Epoch[294] Batch [5]#011Speed: 3973.03 samples/sec#011loss=0.547795\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 534.3649387359619, \"sum\": 534.3649387359619, \"min\": 534.3649387359619}}, \"EndTime\": 1601747497.892617, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747497.357971}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1176.83827523 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] #quality_metric: host=algo-1, epoch=294, train loss <loss>=0.549205577374\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:37 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:38 INFO 139787867731776] Epoch[295] Batch[0] avg_epoch_loss=0.586336\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:38 INFO 139787867731776] #quality_metric: host=algo-1, epoch=295, batch=0 train loss <loss>=0.586335778236\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:38 INFO 139787867731776] Epoch[295] Batch[5] avg_epoch_loss=0.575206\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:38 INFO 139787867731776] #quality_metric: host=algo-1, epoch=295, batch=5 train loss <loss>=0.575206428766\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:38 INFO 139787867731776] Epoch[295] Batch [5]#011Speed: 4325.49 samples/sec#011loss=0.575206\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:38 INFO 139787867731776] processed a total of 580 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 556.4310550689697, \"sum\": 556.4310550689697, \"min\": 556.4310550689697}}, \"EndTime\": 1601747498.455649, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747497.892704}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:38 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1042.17203254 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:38 INFO 139787867731776] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:38 INFO 139787867731776] #quality_metric: host=algo-1, epoch=295, train loss <loss>=0.513888336718\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:38 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:38 INFO 139787867731776] Epoch[296] Batch[0] avg_epoch_loss=0.395631\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:38 INFO 139787867731776] #quality_metric: host=algo-1, epoch=296, batch=0 train loss <loss>=0.395630717278\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:38 INFO 139787867731776] Epoch[296] Batch[5] avg_epoch_loss=0.500872\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:38 INFO 139787867731776] #quality_metric: host=algo-1, epoch=296, batch=5 train loss <loss>=0.500872304042\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:38 INFO 139787867731776] Epoch[296] Batch [5]#011Speed: 3476.83 samples/sec#011loss=0.500872\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:39 INFO 139787867731776] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 556.1630725860596, \"sum\": 556.1630725860596, \"min\": 556.1630725860596}}, \"EndTime\": 1601747499.012215, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747498.455715}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:39 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1085.8192404 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:39 INFO 139787867731776] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:39 INFO 139787867731776] #quality_metric: host=algo-1, epoch=296, train loss <loss>=0.493609961867\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:39 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:39 INFO 139787867731776] Epoch[297] Batch[0] avg_epoch_loss=0.494165\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:39 INFO 139787867731776] #quality_metric: host=algo-1, epoch=297, batch=0 train loss <loss>=0.494165480137\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:39 INFO 139787867731776] Epoch[297] Batch[5] avg_epoch_loss=0.488242\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:39 INFO 139787867731776] #quality_metric: host=algo-1, epoch=297, batch=5 train loss <loss>=0.488242099682\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:39 INFO 139787867731776] Epoch[297] Batch [5]#011Speed: 4271.05 samples/sec#011loss=0.488242\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:39 INFO 139787867731776] Epoch[297] Batch[10] avg_epoch_loss=0.535113\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:39 INFO 139787867731776] #quality_metric: host=algo-1, epoch=297, batch=10 train loss <loss>=0.591357952356\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:39 INFO 139787867731776] Epoch[297] Batch [10]#011Speed: 4232.98 samples/sec#011loss=0.591358\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:39 INFO 139787867731776] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 533.1950187683105, \"sum\": 533.1950187683105, \"min\": 533.1950187683105}}, \"EndTime\": 1601747499.546031, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747499.012281}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:39 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1226.34765628 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:39 INFO 139787867731776] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:39 INFO 139787867731776] #quality_metric: host=algo-1, epoch=297, train loss <loss>=0.535112941807\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:39 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:39 INFO 139787867731776] Epoch[298] Batch[0] avg_epoch_loss=0.507172\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:39 INFO 139787867731776] #quality_metric: host=algo-1, epoch=298, batch=0 train loss <loss>=0.507171750069\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] Epoch[298] Batch[5] avg_epoch_loss=0.523252\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] #quality_metric: host=algo-1, epoch=298, batch=5 train loss <loss>=0.523252402743\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] Epoch[298] Batch [5]#011Speed: 3745.36 samples/sec#011loss=0.523252\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] Epoch[298] Batch[10] avg_epoch_loss=0.497142\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] #quality_metric: host=algo-1, epoch=298, batch=10 train loss <loss>=0.465809309483\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] Epoch[298] Batch [10]#011Speed: 4100.14 samples/sec#011loss=0.465809\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 544.9810028076172, \"sum\": 544.9810028076172, \"min\": 544.9810028076172}}, \"EndTime\": 1601747500.091469, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747499.546096}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1196.18358692 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] #quality_metric: host=algo-1, epoch=298, train loss <loss>=0.497141905806\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] Epoch[299] Batch[0] avg_epoch_loss=0.596415\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] #quality_metric: host=algo-1, epoch=299, batch=0 train loss <loss>=0.59641456604\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] Epoch[299] Batch[5] avg_epoch_loss=0.624114\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] #quality_metric: host=algo-1, epoch=299, batch=5 train loss <loss>=0.624113604426\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] Epoch[299] Batch [5]#011Speed: 3803.68 samples/sec#011loss=0.624114\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 522.5489139556885, \"sum\": 522.5489139556885, \"min\": 522.5489139556885}}, \"EndTime\": 1601747500.614513, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747500.091524}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1182.44698403 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] #quality_metric: host=algo-1, epoch=299, train loss <loss>=0.60329861939\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:40 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] Epoch[300] Batch[0] avg_epoch_loss=0.427184\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] #quality_metric: host=algo-1, epoch=300, batch=0 train loss <loss>=0.427184402943\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] Epoch[300] Batch[5] avg_epoch_loss=0.490993\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] #quality_metric: host=algo-1, epoch=300, batch=5 train loss <loss>=0.490992724895\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] Epoch[300] Batch [5]#011Speed: 3906.30 samples/sec#011loss=0.490993\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 530.3139686584473, \"sum\": 530.3139686584473, \"min\": 530.3139686584473}}, \"EndTime\": 1601747501.145333, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747500.614577}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1195.30646759 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] #quality_metric: host=algo-1, epoch=300, train loss <loss>=0.506905829906\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] Epoch[301] Batch[0] avg_epoch_loss=0.553708\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] #quality_metric: host=algo-1, epoch=301, batch=0 train loss <loss>=0.553708434105\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] Epoch[301] Batch[5] avg_epoch_loss=0.535530\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] #quality_metric: host=algo-1, epoch=301, batch=5 train loss <loss>=0.535529648264\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] Epoch[301] Batch [5]#011Speed: 3953.89 samples/sec#011loss=0.535530\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] Epoch[301] Batch[10] avg_epoch_loss=0.533733\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] #quality_metric: host=algo-1, epoch=301, batch=10 train loss <loss>=0.531577020884\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] Epoch[301] Batch [10]#011Speed: 4285.24 samples/sec#011loss=0.531577\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 581.308126449585, \"sum\": 581.308126449585, \"min\": 581.308126449585}}, \"EndTime\": 1601747501.727058, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747501.145397}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1119.70109941 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] #quality_metric: host=algo-1, epoch=301, train loss <loss>=0.533732999455\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:41 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] Epoch[302] Batch[0] avg_epoch_loss=0.483106\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=302, batch=0 train loss <loss>=0.483105957508\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] Epoch[302] Batch[5] avg_epoch_loss=0.512216\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=302, batch=5 train loss <loss>=0.512215917309\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] Epoch[302] Batch [5]#011Speed: 3838.96 samples/sec#011loss=0.512216\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] Epoch[302] Batch[10] avg_epoch_loss=0.458444\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=302, batch=10 train loss <loss>=0.393917700648\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] Epoch[302] Batch [10]#011Speed: 4214.18 samples/sec#011loss=0.393918\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 549.0729808807373, \"sum\": 549.0729808807373, \"min\": 549.0729808807373}}, \"EndTime\": 1601747502.276596, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747501.727123}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1210.83887951 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=302, train loss <loss>=0.458444000645\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_c5b198df-9d98-4cb8-a967-adf32ca56df5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.880926132202148, \"sum\": 7.880926132202148, \"min\": 7.880926132202148}}, \"EndTime\": 1601747502.284937, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747502.276696}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] Epoch[303] Batch[0] avg_epoch_loss=0.412589\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=303, batch=0 train loss <loss>=0.412588506937\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] Epoch[303] Batch[5] avg_epoch_loss=0.514317\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=303, batch=5 train loss <loss>=0.514316628377\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] Epoch[303] Batch [5]#011Speed: 4319.31 samples/sec#011loss=0.514317\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] Epoch[303] Batch[10] avg_epoch_loss=0.472797\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=303, batch=10 train loss <loss>=0.422973799706\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] Epoch[303] Batch [10]#011Speed: 4138.27 samples/sec#011loss=0.422974\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 582.7281475067139, \"sum\": 582.7281475067139, \"min\": 582.7281475067139}}, \"EndTime\": 1601747502.867777, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747502.284997}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1123.84762495 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] #quality_metric: host=algo-1, epoch=303, train loss <loss>=0.472797160799\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:42 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] Epoch[304] Batch[0] avg_epoch_loss=0.579223\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] #quality_metric: host=algo-1, epoch=304, batch=0 train loss <loss>=0.579222857952\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] Epoch[304] Batch[5] avg_epoch_loss=0.659944\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] #quality_metric: host=algo-1, epoch=304, batch=5 train loss <loss>=0.659944127003\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] Epoch[304] Batch [5]#011Speed: 3840.06 samples/sec#011loss=0.659944\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] Epoch[304] Batch[10] avg_epoch_loss=0.663090\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] #quality_metric: host=algo-1, epoch=304, batch=10 train loss <loss>=0.666865563393\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] Epoch[304] Batch [10]#011Speed: 4227.26 samples/sec#011loss=0.666866\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 591.3431644439697, \"sum\": 591.3431644439697, \"min\": 591.3431644439697}}, \"EndTime\": 1601747503.459475, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747502.86784}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1159.88478279 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] #quality_metric: host=algo-1, epoch=304, train loss <loss>=0.663090234453\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] Epoch[305] Batch[0] avg_epoch_loss=0.643262\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] #quality_metric: host=algo-1, epoch=305, batch=0 train loss <loss>=0.643262028694\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] Epoch[305] Batch[5] avg_epoch_loss=0.574628\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] #quality_metric: host=algo-1, epoch=305, batch=5 train loss <loss>=0.574628392855\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] Epoch[305] Batch [5]#011Speed: 4139.76 samples/sec#011loss=0.574628\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] Epoch[305] Batch[10] avg_epoch_loss=0.528971\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] #quality_metric: host=algo-1, epoch=305, batch=10 train loss <loss>=0.474182003736\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:43 INFO 139787867731776] Epoch[305] Batch [10]#011Speed: 3904.63 samples/sec#011loss=0.474182\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:44 INFO 139787867731776] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 540.1961803436279, \"sum\": 540.1961803436279, \"min\": 540.1961803436279}}, \"EndTime\": 1601747504.000111, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747503.45954}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:44 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1208.61206185 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:44 INFO 139787867731776] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:44 INFO 139787867731776] #quality_metric: host=algo-1, epoch=305, train loss <loss>=0.528970943256\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:44 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:44 INFO 139787867731776] Epoch[306] Batch[0] avg_epoch_loss=0.543438\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:44 INFO 139787867731776] #quality_metric: host=algo-1, epoch=306, batch=0 train loss <loss>=0.543437898159\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:44 INFO 139787867731776] Epoch[306] Batch[5] avg_epoch_loss=0.526454\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:44 INFO 139787867731776] #quality_metric: host=algo-1, epoch=306, batch=5 train loss <loss>=0.526453743378\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:44 INFO 139787867731776] Epoch[306] Batch [5]#011Speed: 4361.22 samples/sec#011loss=0.526454\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:44 INFO 139787867731776] Epoch[306] Batch[10] avg_epoch_loss=0.499439\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:44 INFO 139787867731776] #quality_metric: host=algo-1, epoch=306, batch=10 train loss <loss>=0.467022106051\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:44 INFO 139787867731776] Epoch[306] Batch [10]#011Speed: 4260.92 samples/sec#011loss=0.467022\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:44 INFO 139787867731776] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 550.1940250396729, \"sum\": 550.1940250396729, \"min\": 550.1940250396729}}, \"EndTime\": 1601747504.550656, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747504.000174}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:44 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1199.37412671 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:44 INFO 139787867731776] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:44 INFO 139787867731776] #quality_metric: host=algo-1, epoch=306, train loss <loss>=0.499439362775\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:44 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:44 INFO 139787867731776] Epoch[307] Batch[0] avg_epoch_loss=0.775142\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:44 INFO 139787867731776] #quality_metric: host=algo-1, epoch=307, batch=0 train loss <loss>=0.775141537189\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:45 INFO 139787867731776] Epoch[307] Batch[5] avg_epoch_loss=0.669482\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:45 INFO 139787867731776] #quality_metric: host=algo-1, epoch=307, batch=5 train loss <loss>=0.669481992722\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:45 INFO 139787867731776] Epoch[307] Batch [5]#011Speed: 3932.95 samples/sec#011loss=0.669482\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:45 INFO 139787867731776] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 538.9208793640137, \"sum\": 538.9208793640137, \"min\": 538.9208793640137}}, \"EndTime\": 1601747505.089937, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747504.550719}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:45 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1181.7898782 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:45 INFO 139787867731776] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:45 INFO 139787867731776] #quality_metric: host=algo-1, epoch=307, train loss <loss>=0.632469797134\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:45 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:45 INFO 139787867731776] Epoch[308] Batch[0] avg_epoch_loss=0.674204\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:45 INFO 139787867731776] #quality_metric: host=algo-1, epoch=308, batch=0 train loss <loss>=0.674204468727\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:45 INFO 139787867731776] Epoch[308] Batch[5] avg_epoch_loss=0.627918\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:45 INFO 139787867731776] #quality_metric: host=algo-1, epoch=308, batch=5 train loss <loss>=0.627917925517\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:45 INFO 139787867731776] Epoch[308] Batch [5]#011Speed: 4113.64 samples/sec#011loss=0.627918\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:45 INFO 139787867731776] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 515.5000686645508, \"sum\": 515.5000686645508, \"min\": 515.5000686645508}}, \"EndTime\": 1601747505.605926, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747505.089999}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:45 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1165.64582499 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:45 INFO 139787867731776] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:45 INFO 139787867731776] #quality_metric: host=algo-1, epoch=308, train loss <loss>=0.577383098006\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:45 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:45 INFO 139787867731776] Epoch[309] Batch[0] avg_epoch_loss=0.481124\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:45 INFO 139787867731776] #quality_metric: host=algo-1, epoch=309, batch=0 train loss <loss>=0.481124311686\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] Epoch[309] Batch[5] avg_epoch_loss=0.480833\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] #quality_metric: host=algo-1, epoch=309, batch=5 train loss <loss>=0.480833093325\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] Epoch[309] Batch [5]#011Speed: 4163.04 samples/sec#011loss=0.480833\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] Epoch[309] Batch[10] avg_epoch_loss=0.551942\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] #quality_metric: host=algo-1, epoch=309, batch=10 train loss <loss>=0.637271791697\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] Epoch[309] Batch [10]#011Speed: 4316.39 samples/sec#011loss=0.637272\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 538.9721393585205, \"sum\": 538.9721393585205, \"min\": 538.9721393585205}}, \"EndTime\": 1601747506.145428, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747505.605988}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1222.49599776 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] #quality_metric: host=algo-1, epoch=309, train loss <loss>=0.551941592585\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] Epoch[310] Batch[0] avg_epoch_loss=0.517644\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] #quality_metric: host=algo-1, epoch=310, batch=0 train loss <loss>=0.517644047737\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] Epoch[310] Batch[5] avg_epoch_loss=0.499298\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] #quality_metric: host=algo-1, epoch=310, batch=5 train loss <loss>=0.499298264583\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] Epoch[310] Batch [5]#011Speed: 3337.26 samples/sec#011loss=0.499298\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] Epoch[310] Batch[10] avg_epoch_loss=0.474604\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] #quality_metric: host=algo-1, epoch=310, batch=10 train loss <loss>=0.444970864058\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] Epoch[310] Batch [10]#011Speed: 3360.85 samples/sec#011loss=0.444971\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 587.8579616546631, \"sum\": 587.8579616546631, \"min\": 587.8579616546631}}, \"EndTime\": 1601747506.733641, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747506.145489}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1146.33461674 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] #quality_metric: host=algo-1, epoch=310, train loss <loss>=0.474603991617\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:46 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] Epoch[311] Batch[0] avg_epoch_loss=0.537376\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] #quality_metric: host=algo-1, epoch=311, batch=0 train loss <loss>=0.537376224995\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] Epoch[311] Batch[5] avg_epoch_loss=0.483959\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] #quality_metric: host=algo-1, epoch=311, batch=5 train loss <loss>=0.483958904942\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] Epoch[311] Batch [5]#011Speed: 4320.88 samples/sec#011loss=0.483959\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] Epoch[311] Batch[10] avg_epoch_loss=0.472083\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] #quality_metric: host=algo-1, epoch=311, batch=10 train loss <loss>=0.457832223177\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] Epoch[311] Batch [10]#011Speed: 4214.30 samples/sec#011loss=0.457832\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 625.4208087921143, \"sum\": 625.4208087921143, \"min\": 625.4208087921143}}, \"EndTime\": 1601747507.359495, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747506.733711}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1034.35245473 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] #quality_metric: host=algo-1, epoch=311, train loss <loss>=0.472083140503\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] Epoch[312] Batch[0] avg_epoch_loss=0.560849\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] #quality_metric: host=algo-1, epoch=312, batch=0 train loss <loss>=0.560849487782\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] Epoch[312] Batch[5] avg_epoch_loss=0.571685\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] #quality_metric: host=algo-1, epoch=312, batch=5 train loss <loss>=0.571684718132\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] Epoch[312] Batch [5]#011Speed: 3801.74 samples/sec#011loss=0.571685\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 578.5269737243652, \"sum\": 578.5269737243652, \"min\": 578.5269737243652}}, \"EndTime\": 1601747507.938383, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747507.359556}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1071.51682967 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] #quality_metric: host=algo-1, epoch=312, train loss <loss>=0.56061194241\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:47 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:48 INFO 139787867731776] Epoch[313] Batch[0] avg_epoch_loss=0.571119\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:48 INFO 139787867731776] #quality_metric: host=algo-1, epoch=313, batch=0 train loss <loss>=0.571118712425\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:48 INFO 139787867731776] Epoch[313] Batch[5] avg_epoch_loss=0.564814\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:48 INFO 139787867731776] #quality_metric: host=algo-1, epoch=313, batch=5 train loss <loss>=0.564813688397\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:48 INFO 139787867731776] Epoch[313] Batch [5]#011Speed: 4386.63 samples/sec#011loss=0.564814\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:48 INFO 139787867731776] Epoch[313] Batch[10] avg_epoch_loss=0.502056\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:48 INFO 139787867731776] #quality_metric: host=algo-1, epoch=313, batch=10 train loss <loss>=0.426745927334\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:48 INFO 139787867731776] Epoch[313] Batch [10]#011Speed: 4301.42 samples/sec#011loss=0.426746\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:48 INFO 139787867731776] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 536.433219909668, \"sum\": 536.433219909668, \"min\": 536.433219909668}}, \"EndTime\": 1601747508.475313, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747507.938444}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:48 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1222.69656458 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:48 INFO 139787867731776] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:48 INFO 139787867731776] #quality_metric: host=algo-1, epoch=313, train loss <loss>=0.502055615187\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:48 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:48 INFO 139787867731776] Epoch[314] Batch[0] avg_epoch_loss=0.560483\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:48 INFO 139787867731776] #quality_metric: host=algo-1, epoch=314, batch=0 train loss <loss>=0.560482859612\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:48 INFO 139787867731776] Epoch[314] Batch[5] avg_epoch_loss=0.540151\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:48 INFO 139787867731776] #quality_metric: host=algo-1, epoch=314, batch=5 train loss <loss>=0.540151461959\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:48 INFO 139787867731776] Epoch[314] Batch [5]#011Speed: 4243.65 samples/sec#011loss=0.540151\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:49 INFO 139787867731776] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 528.4671783447266, \"sum\": 528.4671783447266, \"min\": 528.4671783447266}}, \"EndTime\": 1601747509.004117, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747508.475371}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:49 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1144.5647837 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:49 INFO 139787867731776] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:49 INFO 139787867731776] #quality_metric: host=algo-1, epoch=314, train loss <loss>=0.483690920472\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:49 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:49 INFO 139787867731776] Epoch[315] Batch[0] avg_epoch_loss=0.547252\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:49 INFO 139787867731776] #quality_metric: host=algo-1, epoch=315, batch=0 train loss <loss>=0.547251522541\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:49 INFO 139787867731776] Epoch[315] Batch[5] avg_epoch_loss=0.524467\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:49 INFO 139787867731776] #quality_metric: host=algo-1, epoch=315, batch=5 train loss <loss>=0.524466733138\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:49 INFO 139787867731776] Epoch[315] Batch [5]#011Speed: 4340.51 samples/sec#011loss=0.524467\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:49 INFO 139787867731776] Epoch[315] Batch[10] avg_epoch_loss=0.528012\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:49 INFO 139787867731776] #quality_metric: host=algo-1, epoch=315, batch=10 train loss <loss>=0.5322671175\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:49 INFO 139787867731776] Epoch[315] Batch [10]#011Speed: 4282.14 samples/sec#011loss=0.532267\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:49 INFO 139787867731776] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 587.4001979827881, \"sum\": 587.4001979827881, \"min\": 587.4001979827881}}, \"EndTime\": 1601747509.591986, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747509.004203}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:49 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1135.33831579 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:49 INFO 139787867731776] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:49 INFO 139787867731776] #quality_metric: host=algo-1, epoch=315, train loss <loss>=0.528012362393\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:49 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:49 INFO 139787867731776] Epoch[316] Batch[0] avg_epoch_loss=0.581328\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:49 INFO 139787867731776] #quality_metric: host=algo-1, epoch=316, batch=0 train loss <loss>=0.581327915192\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:50 INFO 139787867731776] Epoch[316] Batch[5] avg_epoch_loss=0.507858\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:50 INFO 139787867731776] #quality_metric: host=algo-1, epoch=316, batch=5 train loss <loss>=0.507857834299\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:50 INFO 139787867731776] Epoch[316] Batch [5]#011Speed: 3689.66 samples/sec#011loss=0.507858\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:50 INFO 139787867731776] processed a total of 600 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 521.3041305541992, \"sum\": 521.3041305541992, \"min\": 521.3041305541992}}, \"EndTime\": 1601747510.113703, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747509.592048}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:50 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1150.71489223 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:50 INFO 139787867731776] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:50 INFO 139787867731776] #quality_metric: host=algo-1, epoch=316, train loss <loss>=0.551324814558\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:50 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:50 INFO 139787867731776] Epoch[317] Batch[0] avg_epoch_loss=0.636404\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:50 INFO 139787867731776] #quality_metric: host=algo-1, epoch=317, batch=0 train loss <loss>=0.63640421629\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:50 INFO 139787867731776] Epoch[317] Batch[5] avg_epoch_loss=0.529537\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:50 INFO 139787867731776] #quality_metric: host=algo-1, epoch=317, batch=5 train loss <loss>=0.529537086685\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:50 INFO 139787867731776] Epoch[317] Batch [5]#011Speed: 3598.18 samples/sec#011loss=0.529537\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:50 INFO 139787867731776] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 544.0270900726318, \"sum\": 544.0270900726318, \"min\": 544.0270900726318}}, \"EndTime\": 1601747510.658152, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747510.11378}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:50 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1132.10950875 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:50 INFO 139787867731776] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:50 INFO 139787867731776] #quality_metric: host=algo-1, epoch=317, train loss <loss>=0.534477302432\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:50 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] Epoch[318] Batch[0] avg_epoch_loss=0.560386\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] #quality_metric: host=algo-1, epoch=318, batch=0 train loss <loss>=0.560386180878\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] Epoch[318] Batch[5] avg_epoch_loss=0.518914\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] #quality_metric: host=algo-1, epoch=318, batch=5 train loss <loss>=0.518913572033\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] Epoch[318] Batch [5]#011Speed: 4092.86 samples/sec#011loss=0.518914\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] Epoch[318] Batch[10] avg_epoch_loss=0.530338\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] #quality_metric: host=algo-1, epoch=318, batch=10 train loss <loss>=0.544047033787\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] Epoch[318] Batch [10]#011Speed: 4210.03 samples/sec#011loss=0.544047\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 535.452127456665, \"sum\": 535.452127456665, \"min\": 535.452127456665}}, \"EndTime\": 1601747511.194101, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747510.658214}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1279.06375556 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] #quality_metric: host=algo-1, epoch=318, train loss <loss>=0.53033787283\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] Epoch[319] Batch[0] avg_epoch_loss=0.489845\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] #quality_metric: host=algo-1, epoch=319, batch=0 train loss <loss>=0.489845335484\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] Epoch[319] Batch[5] avg_epoch_loss=0.439447\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] #quality_metric: host=algo-1, epoch=319, batch=5 train loss <loss>=0.439447263877\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] Epoch[319] Batch [5]#011Speed: 3772.61 samples/sec#011loss=0.439447\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] Epoch[319] Batch[10] avg_epoch_loss=0.491466\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] #quality_metric: host=algo-1, epoch=319, batch=10 train loss <loss>=0.553888618946\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] Epoch[319] Batch [10]#011Speed: 3291.09 samples/sec#011loss=0.553889\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 565.1528835296631, \"sum\": 565.1528835296631, \"min\": 565.1528835296631}}, \"EndTime\": 1601747511.759758, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747511.194166}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1203.01783779 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] #quality_metric: host=algo-1, epoch=319, train loss <loss>=0.491466061635\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:51 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:52 INFO 139787867731776] Epoch[320] Batch[0] avg_epoch_loss=0.444896\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:52 INFO 139787867731776] #quality_metric: host=algo-1, epoch=320, batch=0 train loss <loss>=0.444896042347\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:52 INFO 139787867731776] Epoch[320] Batch[5] avg_epoch_loss=0.495333\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:52 INFO 139787867731776] #quality_metric: host=algo-1, epoch=320, batch=5 train loss <loss>=0.495333110293\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:52 INFO 139787867731776] Epoch[320] Batch [5]#011Speed: 4156.93 samples/sec#011loss=0.495333\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:52 INFO 139787867731776] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 537.5118255615234, \"sum\": 537.5118255615234, \"min\": 537.5118255615234}}, \"EndTime\": 1601747512.297635, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747511.75982}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:52 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1168.13550609 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:52 INFO 139787867731776] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:52 INFO 139787867731776] #quality_metric: host=algo-1, epoch=320, train loss <loss>=0.474300324917\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:52 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:52 INFO 139787867731776] Epoch[321] Batch[0] avg_epoch_loss=0.421876\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:52 INFO 139787867731776] #quality_metric: host=algo-1, epoch=321, batch=0 train loss <loss>=0.421875864267\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:52 INFO 139787867731776] Epoch[321] Batch[5] avg_epoch_loss=0.465603\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:52 INFO 139787867731776] #quality_metric: host=algo-1, epoch=321, batch=5 train loss <loss>=0.465603192647\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:52 INFO 139787867731776] Epoch[321] Batch [5]#011Speed: 4288.22 samples/sec#011loss=0.465603\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:52 INFO 139787867731776] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 557.9848289489746, \"sum\": 557.9848289489746, \"min\": 557.9848289489746}}, \"EndTime\": 1601747512.856021, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747512.297701}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:52 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1121.70498297 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:52 INFO 139787867731776] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:52 INFO 139787867731776] #quality_metric: host=algo-1, epoch=321, train loss <loss>=0.473930031061\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:52 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:53 INFO 139787867731776] Epoch[322] Batch[0] avg_epoch_loss=0.406791\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:53 INFO 139787867731776] #quality_metric: host=algo-1, epoch=322, batch=0 train loss <loss>=0.406790882349\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:53 INFO 139787867731776] Epoch[322] Batch[5] avg_epoch_loss=0.524546\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:53 INFO 139787867731776] #quality_metric: host=algo-1, epoch=322, batch=5 train loss <loss>=0.524545853337\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:53 INFO 139787867731776] Epoch[322] Batch [5]#011Speed: 4337.99 samples/sec#011loss=0.524546\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:53 INFO 139787867731776] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 567.6400661468506, \"sum\": 567.6400661468506, \"min\": 567.6400661468506}}, \"EndTime\": 1601747513.424057, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747512.856087}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:53 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1109.67459514 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:53 INFO 139787867731776] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:53 INFO 139787867731776] #quality_metric: host=algo-1, epoch=322, train loss <loss>=0.493042072654\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:53 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:53 INFO 139787867731776] Epoch[323] Batch[0] avg_epoch_loss=0.598659\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:53 INFO 139787867731776] #quality_metric: host=algo-1, epoch=323, batch=0 train loss <loss>=0.598658978939\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:53 INFO 139787867731776] Epoch[323] Batch[5] avg_epoch_loss=0.463997\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:53 INFO 139787867731776] #quality_metric: host=algo-1, epoch=323, batch=5 train loss <loss>=0.463997229934\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:53 INFO 139787867731776] Epoch[323] Batch [5]#011Speed: 3917.05 samples/sec#011loss=0.463997\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:53 INFO 139787867731776] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 574.23996925354, \"sum\": 574.23996925354, \"min\": 574.23996925354}}, \"EndTime\": 1601747513.998694, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747513.424121}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:53 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1102.14997823 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:53 INFO 139787867731776] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:53 INFO 139787867731776] #quality_metric: host=algo-1, epoch=323, train loss <loss>=0.453210699558\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:53 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:54 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_32cd7447-f3b8-43df-af4f-f3a63bdda936-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.885932922363281, \"sum\": 7.885932922363281, \"min\": 7.885932922363281}}, \"EndTime\": 1601747514.007032, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747513.998757}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:54 INFO 139787867731776] Epoch[324] Batch[0] avg_epoch_loss=0.516151\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:54 INFO 139787867731776] #quality_metric: host=algo-1, epoch=324, batch=0 train loss <loss>=0.516150832176\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:54 INFO 139787867731776] Epoch[324] Batch[5] avg_epoch_loss=0.473038\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:54 INFO 139787867731776] #quality_metric: host=algo-1, epoch=324, batch=5 train loss <loss>=0.473037953178\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:54 INFO 139787867731776] Epoch[324] Batch [5]#011Speed: 3606.25 samples/sec#011loss=0.473038\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:54 INFO 139787867731776] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 555.8569431304932, \"sum\": 555.8569431304932, \"min\": 555.8569431304932}}, \"EndTime\": 1601747514.563005, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747514.007095}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:54 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1142.19115635 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:54 INFO 139787867731776] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:54 INFO 139787867731776] #quality_metric: host=algo-1, epoch=324, train loss <loss>=0.457469448447\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:54 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:54 INFO 139787867731776] Epoch[325] Batch[0] avg_epoch_loss=0.389321\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:54 INFO 139787867731776] #quality_metric: host=algo-1, epoch=325, batch=0 train loss <loss>=0.389320701361\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] Epoch[325] Batch[5] avg_epoch_loss=0.450658\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] #quality_metric: host=algo-1, epoch=325, batch=5 train loss <loss>=0.450657511751\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] Epoch[325] Batch [5]#011Speed: 3660.57 samples/sec#011loss=0.450658\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 531.8160057067871, \"sum\": 531.8160057067871, \"min\": 531.8160057067871}}, \"EndTime\": 1601747515.09532, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747514.563066}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1152.44659435 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] #quality_metric: host=algo-1, epoch=325, train loss <loss>=0.514399689436\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] Epoch[326] Batch[0] avg_epoch_loss=0.354795\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] #quality_metric: host=algo-1, epoch=326, batch=0 train loss <loss>=0.354794800282\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] Epoch[326] Batch[5] avg_epoch_loss=0.478012\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] #quality_metric: host=algo-1, epoch=326, batch=5 train loss <loss>=0.478011995554\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] Epoch[326] Batch [5]#011Speed: 4356.77 samples/sec#011loss=0.478012\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] Epoch[326] Batch[10] avg_epoch_loss=0.459083\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] #quality_metric: host=algo-1, epoch=326, batch=10 train loss <loss>=0.436367839575\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] Epoch[326] Batch [10]#011Speed: 3643.94 samples/sec#011loss=0.436368\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 546.6079711914062, \"sum\": 546.6079711914062, \"min\": 546.6079711914062}}, \"EndTime\": 1601747515.642408, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747515.095383}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1179.77369391 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] #quality_metric: host=algo-1, epoch=326, train loss <loss>=0.459082833745\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:55 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] Epoch[327] Batch[0] avg_epoch_loss=0.507030\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=327, batch=0 train loss <loss>=0.507029891014\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] Epoch[327] Batch[5] avg_epoch_loss=0.472866\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=327, batch=5 train loss <loss>=0.472866147757\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] Epoch[327] Batch [5]#011Speed: 4096.05 samples/sec#011loss=0.472866\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] Epoch[327] Batch[10] avg_epoch_loss=0.445143\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=327, batch=10 train loss <loss>=0.41187415719\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] Epoch[327] Batch [10]#011Speed: 3943.71 samples/sec#011loss=0.411874\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 564.1930103302002, \"sum\": 564.1930103302002, \"min\": 564.1930103302002}}, \"EndTime\": 1601747516.207056, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747515.642481}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1139.49743998 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=327, train loss <loss>=0.445142515681\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_fbf06279-ba5a-43c7-af56-040969c5f95e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.410121917724609, \"sum\": 6.410121917724609, \"min\": 6.410121917724609}}, \"EndTime\": 1601747516.21392, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747516.207115}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] Epoch[328] Batch[0] avg_epoch_loss=0.549335\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=328, batch=0 train loss <loss>=0.549335479736\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] Epoch[328] Batch[5] avg_epoch_loss=0.538483\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=328, batch=5 train loss <loss>=0.538482790192\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] Epoch[328] Batch [5]#011Speed: 3959.66 samples/sec#011loss=0.538483\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] Epoch[328] Batch[10] avg_epoch_loss=0.617068\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=328, batch=10 train loss <loss>=0.711371064186\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] Epoch[328] Batch [10]#011Speed: 2913.09 samples/sec#011loss=0.711371\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 571.0809230804443, \"sum\": 571.0809230804443, \"min\": 571.0809230804443}}, \"EndTime\": 1601747516.785108, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747516.213971}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1130.98421977 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] #quality_metric: host=algo-1, epoch=328, train loss <loss>=0.61706836928\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:56 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] Epoch[329] Batch[0] avg_epoch_loss=0.584002\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=329, batch=0 train loss <loss>=0.584001719952\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] Epoch[329] Batch[5] avg_epoch_loss=0.603663\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=329, batch=5 train loss <loss>=0.603662977616\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] Epoch[329] Batch [5]#011Speed: 4013.50 samples/sec#011loss=0.603663\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 528.850793838501, \"sum\": 528.850793838501, \"min\": 528.850793838501}}, \"EndTime\": 1601747517.314446, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747516.785179}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1181.55692337 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=329, train loss <loss>=0.593307220936\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] Epoch[330] Batch[0] avg_epoch_loss=0.593816\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=330, batch=0 train loss <loss>=0.593816041946\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] Epoch[330] Batch[5] avg_epoch_loss=0.543834\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=330, batch=5 train loss <loss>=0.54383434852\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] Epoch[330] Batch [5]#011Speed: 4158.37 samples/sec#011loss=0.543834\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] Epoch[330] Batch[10] avg_epoch_loss=0.507578\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=330, batch=10 train loss <loss>=0.464071017504\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] Epoch[330] Batch [10]#011Speed: 4064.16 samples/sec#011loss=0.464071\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 533.282995223999, \"sum\": 533.282995223999, \"min\": 533.282995223999}}, \"EndTime\": 1601747517.848233, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747517.314523}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1216.77118016 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] #quality_metric: host=algo-1, epoch=330, train loss <loss>=0.507578288967\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:57 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] Epoch[331] Batch[0] avg_epoch_loss=0.407929\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=331, batch=0 train loss <loss>=0.40792927146\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] Epoch[331] Batch[5] avg_epoch_loss=0.505995\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=331, batch=5 train loss <loss>=0.505995000402\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] Epoch[331] Batch [5]#011Speed: 4285.41 samples/sec#011loss=0.505995\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] processed a total of 599 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 521.1789608001709, \"sum\": 521.1789608001709, \"min\": 521.1789608001709}}, \"EndTime\": 1601747518.369902, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747517.848296}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1149.10809049 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=331, train loss <loss>=0.549177446961\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] Epoch[332] Batch[0] avg_epoch_loss=0.662015\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=332, batch=0 train loss <loss>=0.662015497684\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] Epoch[332] Batch[5] avg_epoch_loss=0.551983\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=332, batch=5 train loss <loss>=0.551983108123\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] Epoch[332] Batch [5]#011Speed: 4081.12 samples/sec#011loss=0.551983\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] Epoch[332] Batch[10] avg_epoch_loss=0.533108\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=332, batch=10 train loss <loss>=0.510458016396\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] Epoch[332] Batch [10]#011Speed: 4084.13 samples/sec#011loss=0.510458\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 536.0119342803955, \"sum\": 536.0119342803955, \"min\": 536.0119342803955}}, \"EndTime\": 1601747518.906385, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747518.369962}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1259.05838624 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] #quality_metric: host=algo-1, epoch=332, train loss <loss>=0.533108066429\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:58 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:59 INFO 139787867731776] Epoch[333] Batch[0] avg_epoch_loss=0.497880\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=333, batch=0 train loss <loss>=0.497879862785\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:59 INFO 139787867731776] Epoch[333] Batch[5] avg_epoch_loss=0.450988\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=333, batch=5 train loss <loss>=0.4509875079\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:59 INFO 139787867731776] Epoch[333] Batch [5]#011Speed: 3733.74 samples/sec#011loss=0.450988\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:59 INFO 139787867731776] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 531.6250324249268, \"sum\": 531.6250324249268, \"min\": 531.6250324249268}}, \"EndTime\": 1601747519.438493, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747518.906454}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:59 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1164.14618984 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:59 INFO 139787867731776] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=333, train loss <loss>=0.484901002049\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:59 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:59 INFO 139787867731776] Epoch[334] Batch[0] avg_epoch_loss=0.380050\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=334, batch=0 train loss <loss>=0.38004976511\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:59 INFO 139787867731776] Epoch[334] Batch[5] avg_epoch_loss=0.469951\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=334, batch=5 train loss <loss>=0.469951286912\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:59 INFO 139787867731776] Epoch[334] Batch [5]#011Speed: 4271.40 samples/sec#011loss=0.469951\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:59 INFO 139787867731776] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 517.8148746490479, \"sum\": 517.8148746490479, \"min\": 517.8148746490479}}, \"EndTime\": 1601747519.956828, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747519.438553}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:59 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1224.13907148 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:59 INFO 139787867731776] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:59 INFO 139787867731776] #quality_metric: host=algo-1, epoch=334, train loss <loss>=0.458529689908\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:51:59 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:00 INFO 139787867731776] Epoch[335] Batch[0] avg_epoch_loss=0.345398\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=335, batch=0 train loss <loss>=0.345397800207\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:00 INFO 139787867731776] Epoch[335] Batch[5] avg_epoch_loss=0.430244\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=335, batch=5 train loss <loss>=0.430244465669\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:00 INFO 139787867731776] Epoch[335] Batch [5]#011Speed: 3972.49 samples/sec#011loss=0.430244\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:00 INFO 139787867731776] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 536.3950729370117, \"sum\": 536.3950729370117, \"min\": 536.3950729370117}}, \"EndTime\": 1601747520.493667, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747519.956894}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:00 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1155.67080159 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:00 INFO 139787867731776] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=335, train loss <loss>=0.437033060193\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:00 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:00 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_6663436a-a92e-421a-9d5d-85c6b4cc0b97-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.372928619384766, \"sum\": 6.372928619384766, \"min\": 6.372928619384766}}, \"EndTime\": 1601747520.500539, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747520.493729}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:00 INFO 139787867731776] Epoch[336] Batch[0] avg_epoch_loss=0.422487\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=336, batch=0 train loss <loss>=0.422486960888\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:00 INFO 139787867731776] Epoch[336] Batch[5] avg_epoch_loss=0.441041\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:00 INFO 139787867731776] #quality_metric: host=algo-1, epoch=336, batch=5 train loss <loss>=0.441041196386\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:00 INFO 139787867731776] Epoch[336] Batch [5]#011Speed: 4311.86 samples/sec#011loss=0.441041\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] Epoch[336] Batch[10] avg_epoch_loss=0.406912\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=336, batch=10 train loss <loss>=0.365957325697\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] Epoch[336] Batch [10]#011Speed: 4290.01 samples/sec#011loss=0.365957\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 528.5460948944092, \"sum\": 528.5460948944092, \"min\": 528.5460948944092}}, \"EndTime\": 1601747521.029198, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747520.500598}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1227.67129965 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=336, train loss <loss>=0.406912164255\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_c9d6523e-326d-42d5-8f39-5875b7f9160f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.466865539550781, \"sum\": 6.466865539550781, \"min\": 6.466865539550781}}, \"EndTime\": 1601747521.036146, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747521.029262}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] Epoch[337] Batch[0] avg_epoch_loss=0.368563\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=337, batch=0 train loss <loss>=0.368562757969\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] Epoch[337] Batch[5] avg_epoch_loss=0.453615\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=337, batch=5 train loss <loss>=0.4536152035\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] Epoch[337] Batch [5]#011Speed: 4287.42 samples/sec#011loss=0.453615\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] Epoch[337] Batch[10] avg_epoch_loss=0.442900\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=337, batch=10 train loss <loss>=0.430041754246\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] Epoch[337] Batch [10]#011Speed: 4303.80 samples/sec#011loss=0.430042\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 535.5169773101807, \"sum\": 535.5169773101807, \"min\": 535.5169773101807}}, \"EndTime\": 1601747521.571762, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747521.036196}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1226.64597431 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=337, train loss <loss>=0.442899999293\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] Epoch[338] Batch[0] avg_epoch_loss=0.411664\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:01 INFO 139787867731776] #quality_metric: host=algo-1, epoch=338, batch=0 train loss <loss>=0.41166383028\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] Epoch[338] Batch[5] avg_epoch_loss=0.501039\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=338, batch=5 train loss <loss>=0.501039216916\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] Epoch[338] Batch [5]#011Speed: 4295.71 samples/sec#011loss=0.501039\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] Epoch[338] Batch[10] avg_epoch_loss=0.472087\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=338, batch=10 train loss <loss>=0.437344861031\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] Epoch[338] Batch [10]#011Speed: 4196.06 samples/sec#011loss=0.437345\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 557.7530860900879, \"sum\": 557.7530860900879, \"min\": 557.7530860900879}}, \"EndTime\": 1601747522.129861, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747521.571823}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1165.19598457 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=338, train loss <loss>=0.472087236968\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] Epoch[339] Batch[0] avg_epoch_loss=0.514644\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=339, batch=0 train loss <loss>=0.514643847942\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] Epoch[339] Batch[5] avg_epoch_loss=0.532237\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=339, batch=5 train loss <loss>=0.532237460216\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] Epoch[339] Batch [5]#011Speed: 3963.15 samples/sec#011loss=0.532237\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 521.697998046875, \"sum\": 521.697998046875, \"min\": 521.697998046875}}, \"EndTime\": 1601747522.651915, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747522.129925}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1190.12519819 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] #quality_metric: host=algo-1, epoch=339, train loss <loss>=0.550676256418\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:02 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] Epoch[340] Batch[0] avg_epoch_loss=0.472695\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=340, batch=0 train loss <loss>=0.472695380449\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] Epoch[340] Batch[5] avg_epoch_loss=0.482290\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=340, batch=5 train loss <loss>=0.482290302714\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] Epoch[340] Batch [5]#011Speed: 4326.31 samples/sec#011loss=0.482290\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] Epoch[340] Batch[10] avg_epoch_loss=0.470363\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=340, batch=10 train loss <loss>=0.456050232053\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] Epoch[340] Batch [10]#011Speed: 4153.42 samples/sec#011loss=0.456050\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 587.9929065704346, \"sum\": 587.9929065704346, \"min\": 587.9929065704346}}, \"EndTime\": 1601747523.240311, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747522.651982}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1121.74027937 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=340, train loss <loss>=0.470362997868\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] Epoch[341] Batch[0] avg_epoch_loss=0.536516\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=341, batch=0 train loss <loss>=0.536516189575\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] Epoch[341] Batch[5] avg_epoch_loss=0.433375\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=341, batch=5 train loss <loss>=0.433374876777\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] Epoch[341] Batch [5]#011Speed: 4300.50 samples/sec#011loss=0.433375\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] Epoch[341] Batch[10] avg_epoch_loss=0.431521\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=341, batch=10 train loss <loss>=0.429295688868\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] Epoch[341] Batch [10]#011Speed: 4305.36 samples/sec#011loss=0.429296\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 533.715009689331, \"sum\": 533.715009689331, \"min\": 533.715009689331}}, \"EndTime\": 1601747523.776075, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747523.241545}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1228.88543277 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] #quality_metric: host=algo-1, epoch=341, train loss <loss>=0.431520700455\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:03 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] Epoch[342] Batch[0] avg_epoch_loss=1.088009\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=342, batch=0 train loss <loss>=1.08800947666\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] Epoch[342] Batch[5] avg_epoch_loss=0.881009\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=342, batch=5 train loss <loss>=0.881009489298\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] Epoch[342] Batch [5]#011Speed: 4163.74 samples/sec#011loss=0.881009\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] Epoch[342] Batch[10] avg_epoch_loss=0.787821\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=342, batch=10 train loss <loss>=0.675994801521\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] Epoch[342] Batch [10]#011Speed: 3527.61 samples/sec#011loss=0.675995\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 554.3618202209473, \"sum\": 554.3618202209473, \"min\": 554.3618202209473}}, \"EndTime\": 1601747524.330866, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747523.776146}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1197.54861264 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=342, train loss <loss>=0.787820994854\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] Epoch[343] Batch[0] avg_epoch_loss=0.604293\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=343, batch=0 train loss <loss>=0.604292631149\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] Epoch[343] Batch[5] avg_epoch_loss=0.644923\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=343, batch=5 train loss <loss>=0.644922544559\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] Epoch[343] Batch [5]#011Speed: 3615.91 samples/sec#011loss=0.644923\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] Epoch[343] Batch[10] avg_epoch_loss=0.654995\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=343, batch=10 train loss <loss>=0.667082440853\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] Epoch[343] Batch [10]#011Speed: 3707.69 samples/sec#011loss=0.667082\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 562.8712177276611, \"sum\": 562.8712177276611, \"min\": 562.8712177276611}}, \"EndTime\": 1601747524.894171, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747524.33093}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1167.05420615 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] #quality_metric: host=algo-1, epoch=343, train loss <loss>=0.654995224693\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:04 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] Epoch[344] Batch[0] avg_epoch_loss=0.542301\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=344, batch=0 train loss <loss>=0.542300820351\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] Epoch[344] Batch[5] avg_epoch_loss=0.525461\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=344, batch=5 train loss <loss>=0.525460869074\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] Epoch[344] Batch [5]#011Speed: 4083.97 samples/sec#011loss=0.525461\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] Epoch[344] Batch[10] avg_epoch_loss=0.463560\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=344, batch=10 train loss <loss>=0.389278699458\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] Epoch[344] Batch [10]#011Speed: 4178.58 samples/sec#011loss=0.389279\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 547.8150844573975, \"sum\": 547.8150844573975, \"min\": 547.8150844573975}}, \"EndTime\": 1601747525.442393, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747524.894224}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1246.5616571 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=344, train loss <loss>=0.463559882885\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] Epoch[345] Batch[0] avg_epoch_loss=0.493287\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=345, batch=0 train loss <loss>=0.493286550045\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] Epoch[345] Batch[5] avg_epoch_loss=0.447147\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=345, batch=5 train loss <loss>=0.447146852811\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] Epoch[345] Batch [5]#011Speed: 3827.15 samples/sec#011loss=0.447147\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] Epoch[345] Batch[10] avg_epoch_loss=0.443225\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] #quality_metric: host=algo-1, epoch=345, batch=10 train loss <loss>=0.438518494368\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] Epoch[345] Batch [10]#011Speed: 3810.98 samples/sec#011loss=0.438518\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 557.1069717407227, \"sum\": 557.1069717407227, \"min\": 557.1069717407227}}, \"EndTime\": 1601747525.999874, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747525.442455}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:05 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1189.85260124 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:06 INFO 139787867731776] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=345, train loss <loss>=0.4432248717\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:06 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:06 INFO 139787867731776] Epoch[346] Batch[0] avg_epoch_loss=0.482853\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=346, batch=0 train loss <loss>=0.482853382826\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:06 INFO 139787867731776] Epoch[346] Batch[5] avg_epoch_loss=0.459634\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=346, batch=5 train loss <loss>=0.459633737803\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:06 INFO 139787867731776] Epoch[346] Batch [5]#011Speed: 4141.37 samples/sec#011loss=0.459634\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:06 INFO 139787867731776] Epoch[346] Batch[10] avg_epoch_loss=0.501889\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=346, batch=10 train loss <loss>=0.552594345808\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:06 INFO 139787867731776] Epoch[346] Batch [10]#011Speed: 4284.10 samples/sec#011loss=0.552594\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:06 INFO 139787867731776] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 538.154125213623, \"sum\": 538.154125213623, \"min\": 538.154125213623}}, \"EndTime\": 1601747526.53849, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747525.999945}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:06 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1224.34254085 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:06 INFO 139787867731776] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=346, train loss <loss>=0.501888559623\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:06 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:06 INFO 139787867731776] Epoch[347] Batch[0] avg_epoch_loss=0.449765\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:06 INFO 139787867731776] #quality_metric: host=algo-1, epoch=347, batch=0 train loss <loss>=0.449764549732\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:07 INFO 139787867731776] Epoch[347] Batch[5] avg_epoch_loss=0.442563\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=347, batch=5 train loss <loss>=0.442562515537\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:07 INFO 139787867731776] Epoch[347] Batch [5]#011Speed: 4314.18 samples/sec#011loss=0.442563\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:07 INFO 139787867731776] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 560.6999397277832, \"sum\": 560.6999397277832, \"min\": 560.6999397277832}}, \"EndTime\": 1601747527.099555, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747526.538553}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:07 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1109.13768133 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:07 INFO 139787867731776] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=347, train loss <loss>=0.455567675829\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:07 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:07 INFO 139787867731776] Epoch[348] Batch[0] avg_epoch_loss=0.365937\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=348, batch=0 train loss <loss>=0.365937173367\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:07 INFO 139787867731776] Epoch[348] Batch[5] avg_epoch_loss=0.440042\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=348, batch=5 train loss <loss>=0.440041571856\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:07 INFO 139787867731776] Epoch[348] Batch [5]#011Speed: 4024.86 samples/sec#011loss=0.440042\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:07 INFO 139787867731776] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 558.5520267486572, \"sum\": 558.5520267486572, \"min\": 558.5520267486572}}, \"EndTime\": 1601747527.65851, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747527.099621}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:07 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1118.78036279 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:07 INFO 139787867731776] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:07 INFO 139787867731776] #quality_metric: host=algo-1, epoch=348, train loss <loss>=0.458354711533\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:07 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] Epoch[349] Batch[0] avg_epoch_loss=0.313449\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=349, batch=0 train loss <loss>=0.313449472189\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] Epoch[349] Batch[5] avg_epoch_loss=0.458058\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=349, batch=5 train loss <loss>=0.458058436712\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] Epoch[349] Batch [5]#011Speed: 4288.16 samples/sec#011loss=0.458058\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 534.9371433258057, \"sum\": 534.9371433258057, \"min\": 534.9371433258057}}, \"EndTime\": 1601747528.193846, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747527.658572}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1186.84905478 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=349, train loss <loss>=0.483696991205\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] Epoch[350] Batch[0] avg_epoch_loss=0.486134\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=350, batch=0 train loss <loss>=0.486134499311\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] Epoch[350] Batch[5] avg_epoch_loss=0.439119\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=350, batch=5 train loss <loss>=0.439119065801\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] Epoch[350] Batch [5]#011Speed: 4326.55 samples/sec#011loss=0.439119\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] Epoch[350] Batch[10] avg_epoch_loss=0.488418\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=350, batch=10 train loss <loss>=0.547577732801\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] Epoch[350] Batch [10]#011Speed: 3832.79 samples/sec#011loss=0.547578\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 544.0750122070312, \"sum\": 544.0750122070312, \"min\": 544.0750122070312}}, \"EndTime\": 1601747528.738427, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747528.193908}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1216.52488689 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] #quality_metric: host=algo-1, epoch=350, train loss <loss>=0.488418459892\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:08 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:09 INFO 139787867731776] Epoch[351] Batch[0] avg_epoch_loss=0.466276\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=351, batch=0 train loss <loss>=0.466275572777\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:09 INFO 139787867731776] Epoch[351] Batch[5] avg_epoch_loss=0.467642\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=351, batch=5 train loss <loss>=0.467641755939\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:09 INFO 139787867731776] Epoch[351] Batch [5]#011Speed: 3561.13 samples/sec#011loss=0.467642\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:09 INFO 139787867731776] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 546.9779968261719, \"sum\": 546.9779968261719, \"min\": 546.9779968261719}}, \"EndTime\": 1601747529.285857, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747528.738493}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:09 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1151.59584496 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:09 INFO 139787867731776] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=351, train loss <loss>=0.453551989794\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:09 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:09 INFO 139787867731776] Epoch[352] Batch[0] avg_epoch_loss=0.368567\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=352, batch=0 train loss <loss>=0.368567198515\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:09 INFO 139787867731776] Epoch[352] Batch[5] avg_epoch_loss=0.444994\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=352, batch=5 train loss <loss>=0.444993600249\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:09 INFO 139787867731776] Epoch[352] Batch [5]#011Speed: 4025.16 samples/sec#011loss=0.444994\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:09 INFO 139787867731776] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 522.0611095428467, \"sum\": 522.0611095428467, \"min\": 522.0611095428467}}, \"EndTime\": 1601747529.808363, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747529.285913}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:09 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1210.38006699 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:09 INFO 139787867731776] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:09 INFO 139787867731776] #quality_metric: host=algo-1, epoch=352, train loss <loss>=0.445207354426\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:09 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:10 INFO 139787867731776] Epoch[353] Batch[0] avg_epoch_loss=0.494045\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=353, batch=0 train loss <loss>=0.49404451251\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:10 INFO 139787867731776] Epoch[353] Batch[5] avg_epoch_loss=0.442270\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=353, batch=5 train loss <loss>=0.44227009515\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:10 INFO 139787867731776] Epoch[353] Batch [5]#011Speed: 3834.50 samples/sec#011loss=0.442270\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:10 INFO 139787867731776] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 537.4329090118408, \"sum\": 537.4329090118408, \"min\": 537.4329090118408}}, \"EndTime\": 1601747530.346243, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747529.808424}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:10 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1162.69136723 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:10 INFO 139787867731776] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=353, train loss <loss>=0.450141885877\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:10 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:10 INFO 139787867731776] Epoch[354] Batch[0] avg_epoch_loss=0.449108\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=354, batch=0 train loss <loss>=0.449108123779\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:10 INFO 139787867731776] Epoch[354] Batch[5] avg_epoch_loss=0.457609\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=354, batch=5 train loss <loss>=0.457608977954\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:10 INFO 139787867731776] Epoch[354] Batch [5]#011Speed: 3872.50 samples/sec#011loss=0.457609\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:10 INFO 139787867731776] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 542.5610542297363, \"sum\": 542.5610542297363, \"min\": 542.5610542297363}}, \"EndTime\": 1601747530.889426, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747530.346307}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:10 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1171.99376126 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:10 INFO 139787867731776] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:10 INFO 139787867731776] #quality_metric: host=algo-1, epoch=354, train loss <loss>=0.439204898477\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:10 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:11 INFO 139787867731776] Epoch[355] Batch[0] avg_epoch_loss=0.435480\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=355, batch=0 train loss <loss>=0.435480296612\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:11 INFO 139787867731776] Epoch[355] Batch[5] avg_epoch_loss=0.405909\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=355, batch=5 train loss <loss>=0.405909195542\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:11 INFO 139787867731776] Epoch[355] Batch [5]#011Speed: 3519.15 samples/sec#011loss=0.405909\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:11 INFO 139787867731776] Epoch[355] Batch[10] avg_epoch_loss=0.421135\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=355, batch=10 train loss <loss>=0.439405184984\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:11 INFO 139787867731776] Epoch[355] Batch [10]#011Speed: 4067.27 samples/sec#011loss=0.439405\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:11 INFO 139787867731776] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 553.8291931152344, \"sum\": 553.8291931152344, \"min\": 553.8291931152344}}, \"EndTime\": 1601747531.443747, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747530.889489}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:11 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1211.36880236 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:11 INFO 139787867731776] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=355, train loss <loss>=0.421134645289\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:11 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:11 INFO 139787867731776] Epoch[356] Batch[0] avg_epoch_loss=0.400287\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=356, batch=0 train loss <loss>=0.400286763906\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:11 INFO 139787867731776] Epoch[356] Batch[5] avg_epoch_loss=0.415028\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:11 INFO 139787867731776] #quality_metric: host=algo-1, epoch=356, batch=5 train loss <loss>=0.415027648211\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:11 INFO 139787867731776] Epoch[356] Batch [5]#011Speed: 3314.85 samples/sec#011loss=0.415028\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] Epoch[356] Batch[10] avg_epoch_loss=0.404660\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=356, batch=10 train loss <loss>=0.392218840122\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] Epoch[356] Batch [10]#011Speed: 4228.44 samples/sec#011loss=0.392219\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 559.0329170227051, \"sum\": 559.0329170227051, \"min\": 559.0329170227051}}, \"EndTime\": 1601747532.003246, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747531.443808}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1183.96569665 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=356, train loss <loss>=0.40466000817\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_cdc805b0-7380-4efb-8ef9-0398d79db913-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.247043609619141, \"sum\": 6.247043609619141, \"min\": 6.247043609619141}}, \"EndTime\": 1601747532.009992, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747532.003315}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] Epoch[357] Batch[0] avg_epoch_loss=0.527399\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=357, batch=0 train loss <loss>=0.527398765087\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] Epoch[357] Batch[5] avg_epoch_loss=0.565696\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=357, batch=5 train loss <loss>=0.565695653359\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] Epoch[357] Batch [5]#011Speed: 3995.30 samples/sec#011loss=0.565696\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 522.324800491333, \"sum\": 522.324800491333, \"min\": 522.324800491333}}, \"EndTime\": 1601747532.532432, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747532.010051}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1156.14642869 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=357, train loss <loss>=0.539247632027\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] Epoch[358] Batch[0] avg_epoch_loss=0.425554\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:12 INFO 139787867731776] #quality_metric: host=algo-1, epoch=358, batch=0 train loss <loss>=0.42555424571\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] Epoch[358] Batch[5] avg_epoch_loss=0.520779\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=358, batch=5 train loss <loss>=0.520778973897\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] Epoch[358] Batch [5]#011Speed: 4062.30 samples/sec#011loss=0.520779\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] Epoch[358] Batch[10] avg_epoch_loss=0.488198\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=358, batch=10 train loss <loss>=0.449100717902\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] Epoch[358] Batch [10]#011Speed: 4221.35 samples/sec#011loss=0.449101\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 585.798978805542, \"sum\": 585.798978805542, \"min\": 585.798978805542}}, \"EndTime\": 1601747533.11866, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747532.5325}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1095.76293128 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=358, train loss <loss>=0.488197948445\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] Epoch[359] Batch[0] avg_epoch_loss=0.626238\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=359, batch=0 train loss <loss>=0.626237750053\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] Epoch[359] Batch[5] avg_epoch_loss=0.525020\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=359, batch=5 train loss <loss>=0.525019879142\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] Epoch[359] Batch [5]#011Speed: 4207.48 samples/sec#011loss=0.525020\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] Epoch[359] Batch[10] avg_epoch_loss=0.491444\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=359, batch=10 train loss <loss>=0.45115249753\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] Epoch[359] Batch [10]#011Speed: 4156.87 samples/sec#011loss=0.451152\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 595.224142074585, \"sum\": 595.224142074585, \"min\": 595.224142074585}}, \"EndTime\": 1601747533.714244, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747533.118725}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1091.85503382 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] #quality_metric: host=algo-1, epoch=359, train loss <loss>=0.491443796591\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:13 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] Epoch[360] Batch[0] avg_epoch_loss=0.490696\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=360, batch=0 train loss <loss>=0.490696191788\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] Epoch[360] Batch[5] avg_epoch_loss=0.480796\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=360, batch=5 train loss <loss>=0.480796242754\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] Epoch[360] Batch [5]#011Speed: 4298.24 samples/sec#011loss=0.480796\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] Epoch[360] Batch[10] avg_epoch_loss=0.430795\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=360, batch=10 train loss <loss>=0.370793840289\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] Epoch[360] Batch [10]#011Speed: 4256.89 samples/sec#011loss=0.370794\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 545.9320545196533, \"sum\": 545.9320545196533, \"min\": 545.9320545196533}}, \"EndTime\": 1601747534.260534, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747533.714308}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1181.26451241 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=360, train loss <loss>=0.430795150724\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] Epoch[361] Batch[0] avg_epoch_loss=0.436014\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=361, batch=0 train loss <loss>=0.436013877392\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] Epoch[361] Batch[5] avg_epoch_loss=0.463889\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=361, batch=5 train loss <loss>=0.463889395197\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] Epoch[361] Batch [5]#011Speed: 3875.78 samples/sec#011loss=0.463889\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 541.9559478759766, \"sum\": 541.9559478759766, \"min\": 541.9559478759766}}, \"EndTime\": 1601747534.802927, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747534.260598}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1165.93973339 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] #quality_metric: host=algo-1, epoch=361, train loss <loss>=0.443456986547\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:14 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:15 INFO 139787867731776] Epoch[362] Batch[0] avg_epoch_loss=0.401287\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=362, batch=0 train loss <loss>=0.401287049055\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:15 INFO 139787867731776] Epoch[362] Batch[5] avg_epoch_loss=0.462918\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=362, batch=5 train loss <loss>=0.462917993466\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:15 INFO 139787867731776] Epoch[362] Batch [5]#011Speed: 3997.60 samples/sec#011loss=0.462918\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:15 INFO 139787867731776] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 526.9980430603027, \"sum\": 526.9980430603027, \"min\": 526.9980430603027}}, \"EndTime\": 1601747535.330319, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747534.802992}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:15 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1140.22515403 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:15 INFO 139787867731776] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=362, train loss <loss>=0.425671264529\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:15 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:15 INFO 139787867731776] Epoch[363] Batch[0] avg_epoch_loss=0.489885\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=363, batch=0 train loss <loss>=0.489885210991\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:15 INFO 139787867731776] Epoch[363] Batch[5] avg_epoch_loss=0.491758\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=363, batch=5 train loss <loss>=0.4917576015\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:15 INFO 139787867731776] Epoch[363] Batch [5]#011Speed: 3958.68 samples/sec#011loss=0.491758\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:15 INFO 139787867731776] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 581.8939208984375, \"sum\": 581.8939208984375, \"min\": 581.8939208984375}}, \"EndTime\": 1601747535.912619, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747535.330381}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:15 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1039.44903613 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:15 INFO 139787867731776] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:15 INFO 139787867731776] #quality_metric: host=algo-1, epoch=363, train loss <loss>=0.498306238651\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:15 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:16 INFO 139787867731776] Epoch[364] Batch[0] avg_epoch_loss=0.408730\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=364, batch=0 train loss <loss>=0.408730268478\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:16 INFO 139787867731776] Epoch[364] Batch[5] avg_epoch_loss=0.450432\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=364, batch=5 train loss <loss>=0.45043216149\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:16 INFO 139787867731776] Epoch[364] Batch [5]#011Speed: 4287.89 samples/sec#011loss=0.450432\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:16 INFO 139787867731776] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 519.2310810089111, \"sum\": 519.2310810089111, \"min\": 519.2310810089111}}, \"EndTime\": 1601747536.432414, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747535.912731}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:16 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1232.3570168 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:16 INFO 139787867731776] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=364, train loss <loss>=0.446560198069\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:16 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:16 INFO 139787867731776] Epoch[365] Batch[0] avg_epoch_loss=0.445237\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=365, batch=0 train loss <loss>=0.445237219334\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:16 INFO 139787867731776] Epoch[365] Batch[5] avg_epoch_loss=0.460647\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:16 INFO 139787867731776] #quality_metric: host=algo-1, epoch=365, batch=5 train loss <loss>=0.460646927357\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:16 INFO 139787867731776] Epoch[365] Batch [5]#011Speed: 3200.87 samples/sec#011loss=0.460647\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:17 INFO 139787867731776] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 590.7490253448486, \"sum\": 590.7490253448486, \"min\": 590.7490253448486}}, \"EndTime\": 1601747537.02359, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747536.432482}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:17 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1067.9708413 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:17 INFO 139787867731776] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=365, train loss <loss>=0.445935171843\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:17 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:17 INFO 139787867731776] Epoch[366] Batch[0] avg_epoch_loss=0.354695\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=366, batch=0 train loss <loss>=0.354695498943\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:17 INFO 139787867731776] Epoch[366] Batch[5] avg_epoch_loss=0.413119\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=366, batch=5 train loss <loss>=0.413119067748\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:17 INFO 139787867731776] Epoch[366] Batch [5]#011Speed: 3937.20 samples/sec#011loss=0.413119\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:17 INFO 139787867731776] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 521.1780071258545, \"sum\": 521.1780071258545, \"min\": 521.1780071258545}}, \"EndTime\": 1601747537.545229, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747537.023654}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:17 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1210.48675676 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:17 INFO 139787867731776] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=366, train loss <loss>=0.407896202803\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:17 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:17 INFO 139787867731776] Epoch[367] Batch[0] avg_epoch_loss=0.412759\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:17 INFO 139787867731776] #quality_metric: host=algo-1, epoch=367, batch=0 train loss <loss>=0.41275921464\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] Epoch[367] Batch[5] avg_epoch_loss=0.433730\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=367, batch=5 train loss <loss>=0.433730194966\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] Epoch[367] Batch [5]#011Speed: 3653.84 samples/sec#011loss=0.433730\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] Epoch[367] Batch[10] avg_epoch_loss=0.416403\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=367, batch=10 train loss <loss>=0.395610630512\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] Epoch[367] Batch [10]#011Speed: 4102.08 samples/sec#011loss=0.395611\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 588.4249210357666, \"sum\": 588.4249210357666, \"min\": 588.4249210357666}}, \"EndTime\": 1601747538.134059, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747537.545296}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1109.56692269 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=367, train loss <loss>=0.416403120214\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] Epoch[368] Batch[0] avg_epoch_loss=0.638776\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=368, batch=0 train loss <loss>=0.638775765896\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] Epoch[368] Batch[5] avg_epoch_loss=0.541325\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=368, batch=5 train loss <loss>=0.541324665149\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] Epoch[368] Batch [5]#011Speed: 4057.00 samples/sec#011loss=0.541325\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 522.1691131591797, \"sum\": 522.1691131591797, \"min\": 522.1691131591797}}, \"EndTime\": 1601747538.65659, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747538.134123}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1152.62743093 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] #quality_metric: host=algo-1, epoch=368, train loss <loss>=0.500021344423\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:18 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:19 INFO 139787867731776] Epoch[369] Batch[0] avg_epoch_loss=0.469822\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=369, batch=0 train loss <loss>=0.469822019339\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:19 INFO 139787867731776] Epoch[369] Batch[5] avg_epoch_loss=0.503156\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=369, batch=5 train loss <loss>=0.503155559301\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:19 INFO 139787867731776] Epoch[369] Batch [5]#011Speed: 4150.77 samples/sec#011loss=0.503156\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:19 INFO 139787867731776] processed a total of 585 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 514.8417949676514, \"sum\": 514.8417949676514, \"min\": 514.8417949676514}}, \"EndTime\": 1601747539.171865, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747538.656674}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:19 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1136.0772526 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:19 INFO 139787867731776] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=369, train loss <loss>=0.559441593289\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:19 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:19 INFO 139787867731776] Epoch[370] Batch[0] avg_epoch_loss=0.520400\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=370, batch=0 train loss <loss>=0.520399749279\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:19 INFO 139787867731776] Epoch[370] Batch[5] avg_epoch_loss=0.472020\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=370, batch=5 train loss <loss>=0.472019573053\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:19 INFO 139787867731776] Epoch[370] Batch [5]#011Speed: 4284.03 samples/sec#011loss=0.472020\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:19 INFO 139787867731776] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 529.2859077453613, \"sum\": 529.2859077453613, \"min\": 529.2859077453613}}, \"EndTime\": 1601747539.701537, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747539.171925}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:19 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1174.96872719 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:19 INFO 139787867731776] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:19 INFO 139787867731776] #quality_metric: host=algo-1, epoch=370, train loss <loss>=0.456026917696\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:19 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] Epoch[371] Batch[0] avg_epoch_loss=0.507942\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=371, batch=0 train loss <loss>=0.507941961288\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] Epoch[371] Batch[5] avg_epoch_loss=0.495164\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=371, batch=5 train loss <loss>=0.495163882772\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] Epoch[371] Batch [5]#011Speed: 4287.82 samples/sec#011loss=0.495164\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] Epoch[371] Batch[10] avg_epoch_loss=0.416512\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=371, batch=10 train loss <loss>=0.322129614651\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] Epoch[371] Batch [10]#011Speed: 4253.51 samples/sec#011loss=0.322130\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 542.5770282745361, \"sum\": 542.5770282745361, \"min\": 542.5770282745361}}, \"EndTime\": 1601747540.244614, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747539.7016}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1243.8082962 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=371, train loss <loss>=0.416511942717\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] Epoch[372] Batch[0] avg_epoch_loss=0.490550\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=372, batch=0 train loss <loss>=0.490550249815\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] Epoch[372] Batch[5] avg_epoch_loss=0.426970\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=372, batch=5 train loss <loss>=0.426970203718\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] Epoch[372] Batch [5]#011Speed: 4276.53 samples/sec#011loss=0.426970\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] Epoch[372] Batch[10] avg_epoch_loss=0.393350\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=372, batch=10 train loss <loss>=0.353005483374\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] Epoch[372] Batch [10]#011Speed: 4137.95 samples/sec#011loss=0.353005\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 533.5378646850586, \"sum\": 533.5378646850586, \"min\": 533.5378646850586}}, \"EndTime\": 1601747540.778623, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747540.244694}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1234.90079231 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] #quality_metric: host=algo-1, epoch=372, train loss <loss>=0.393349876289\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:20 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_7e893b77-b403-4ab8-873d-af75225411ad-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.170988082885742, \"sum\": 6.170988082885742, \"min\": 6.170988082885742}}, \"EndTime\": 1601747540.785324, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747540.778696}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] Epoch[373] Batch[0] avg_epoch_loss=0.435594\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=373, batch=0 train loss <loss>=0.435593545437\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] Epoch[373] Batch[5] avg_epoch_loss=0.387420\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=373, batch=5 train loss <loss>=0.387419829766\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] Epoch[373] Batch [5]#011Speed: 4180.27 samples/sec#011loss=0.387420\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] Epoch[373] Batch[10] avg_epoch_loss=0.333553\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=373, batch=10 train loss <loss>=0.268912807107\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] Epoch[373] Batch [10]#011Speed: 3939.48 samples/sec#011loss=0.268913\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 533.9748859405518, \"sum\": 533.9748859405518, \"min\": 533.9748859405518}}, \"EndTime\": 1601747541.319401, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747540.785373}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1200.22145279 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=373, train loss <loss>=0.333553001285\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/state_fc762e67-dae1-42b2-b83c-0e189346f372-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.01506233215332, \"sum\": 6.01506233215332, \"min\": 6.01506233215332}}, \"EndTime\": 1601747541.32592, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747541.319462}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] Epoch[374] Batch[0] avg_epoch_loss=0.552858\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=374, batch=0 train loss <loss>=0.552857816219\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] Epoch[374] Batch[5] avg_epoch_loss=0.540375\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=374, batch=5 train loss <loss>=0.540375287334\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] Epoch[374] Batch [5]#011Speed: 4283.34 samples/sec#011loss=0.540375\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 521.7821598052979, \"sum\": 521.7821598052979, \"min\": 521.7821598052979}}, \"EndTime\": 1601747541.8478, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747541.325968}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1191.86086124 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] #quality_metric: host=algo-1, epoch=374, train loss <loss>=0.527592387795\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:21 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] Epoch[375] Batch[0] avg_epoch_loss=0.521275\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=375, batch=0 train loss <loss>=0.52127468586\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] Epoch[375] Batch[5] avg_epoch_loss=0.542112\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=375, batch=5 train loss <loss>=0.542111660043\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] Epoch[375] Batch [5]#011Speed: 3617.33 samples/sec#011loss=0.542112\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] Epoch[375] Batch[10] avg_epoch_loss=0.482996\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=375, batch=10 train loss <loss>=0.412056982517\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] Epoch[375] Batch [10]#011Speed: 4025.26 samples/sec#011loss=0.412057\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 570.3649520874023, \"sum\": 570.3649520874023, \"min\": 570.3649520874023}}, \"EndTime\": 1601747542.418575, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747541.847864}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1123.6605399 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=375, train loss <loss>=0.482995897532\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] Epoch[376] Batch[0] avg_epoch_loss=0.639414\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=376, batch=0 train loss <loss>=0.639413774014\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] Epoch[376] Batch[5] avg_epoch_loss=0.488786\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=376, batch=5 train loss <loss>=0.488785644372\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] Epoch[376] Batch [5]#011Speed: 3914.66 samples/sec#011loss=0.488786\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] Epoch[376] Batch[10] avg_epoch_loss=0.459367\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=376, batch=10 train loss <loss>=0.42406359911\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] Epoch[376] Batch [10]#011Speed: 3654.18 samples/sec#011loss=0.424064\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 560.3170394897461, \"sum\": 560.3170394897461, \"min\": 560.3170394897461}}, \"EndTime\": 1601747542.979261, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747542.418637}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1183.0663793 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] #quality_metric: host=algo-1, epoch=376, train loss <loss>=0.459366532889\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:22 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:23 INFO 139787867731776] Epoch[377] Batch[0] avg_epoch_loss=0.473500\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=377, batch=0 train loss <loss>=0.473500370979\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:23 INFO 139787867731776] Epoch[377] Batch[5] avg_epoch_loss=0.423088\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=377, batch=5 train loss <loss>=0.423088262479\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:23 INFO 139787867731776] Epoch[377] Batch [5]#011Speed: 4125.89 samples/sec#011loss=0.423088\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:23 INFO 139787867731776] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 522.1548080444336, \"sum\": 522.1548080444336, \"min\": 522.1548080444336}}, \"EndTime\": 1601747543.501913, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747542.979322}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:23 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1225.46016479 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:23 INFO 139787867731776] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=377, train loss <loss>=0.432158377767\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:23 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:23 INFO 139787867731776] Epoch[378] Batch[0] avg_epoch_loss=0.472825\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:23 INFO 139787867731776] #quality_metric: host=algo-1, epoch=378, batch=0 train loss <loss>=0.47282537818\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] Epoch[378] Batch[5] avg_epoch_loss=0.437653\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=378, batch=5 train loss <loss>=0.437652692199\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] Epoch[378] Batch [5]#011Speed: 4305.82 samples/sec#011loss=0.437653\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 567.2659873962402, \"sum\": 567.2659873962402, \"min\": 567.2659873962402}}, \"EndTime\": 1601747544.069598, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747543.501979}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1103.24152968 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=378, train loss <loss>=0.438194710016\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] Epoch[379] Batch[0] avg_epoch_loss=0.328822\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=379, batch=0 train loss <loss>=0.328821837902\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] Epoch[379] Batch[5] avg_epoch_loss=0.409305\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=379, batch=5 train loss <loss>=0.40930510064\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] Epoch[379] Batch [5]#011Speed: 3951.40 samples/sec#011loss=0.409305\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] Epoch[379] Batch[10] avg_epoch_loss=0.414707\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=379, batch=10 train loss <loss>=0.421188628674\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] Epoch[379] Batch [10]#011Speed: 4215.35 samples/sec#011loss=0.421189\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 555.6309223175049, \"sum\": 555.6309223175049, \"min\": 555.6309223175049}}, \"EndTime\": 1601747544.625696, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747544.069718}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1212.83663804 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] #quality_metric: host=algo-1, epoch=379, train loss <loss>=0.414706704291\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:24 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] Epoch[380] Batch[0] avg_epoch_loss=0.375046\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=380, batch=0 train loss <loss>=0.375046372414\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] Epoch[380] Batch[5] avg_epoch_loss=0.469323\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=380, batch=5 train loss <loss>=0.469323113561\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] Epoch[380] Batch [5]#011Speed: 4313.48 samples/sec#011loss=0.469323\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 568.6240196228027, \"sum\": 568.6240196228027, \"min\": 568.6240196228027}}, \"EndTime\": 1601747545.194686, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747544.62576}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1097.19869422 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=380, train loss <loss>=0.436167883873\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] Epoch[381] Batch[0] avg_epoch_loss=0.495287\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=381, batch=0 train loss <loss>=0.49528670311\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] Epoch[381] Batch[5] avg_epoch_loss=0.420334\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=381, batch=5 train loss <loss>=0.42033427457\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] Epoch[381] Batch [5]#011Speed: 4304.72 samples/sec#011loss=0.420334\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] Epoch[381] Batch[10] avg_epoch_loss=0.399171\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=381, batch=10 train loss <loss>=0.373775458336\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] Epoch[381] Batch [10]#011Speed: 4269.80 samples/sec#011loss=0.373775\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 585.0458145141602, \"sum\": 585.0458145141602, \"min\": 585.0458145141602}}, \"EndTime\": 1601747545.780159, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747545.194751}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1110.85479217 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] #quality_metric: host=algo-1, epoch=381, train loss <loss>=0.399171176282\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:25 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] Epoch[382] Batch[0] avg_epoch_loss=0.489455\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=382, batch=0 train loss <loss>=0.489455401897\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] Epoch[382] Batch[5] avg_epoch_loss=0.423201\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=382, batch=5 train loss <loss>=0.423200726509\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] Epoch[382] Batch [5]#011Speed: 4140.13 samples/sec#011loss=0.423201\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] Epoch[382] Batch[10] avg_epoch_loss=0.458774\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=382, batch=10 train loss <loss>=0.501461946964\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] Epoch[382] Batch [10]#011Speed: 4279.97 samples/sec#011loss=0.501462\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 546.2620258331299, \"sum\": 546.2620258331299, \"min\": 546.2620258331299}}, \"EndTime\": 1601747546.326765, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747545.78022}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1193.36326961 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=382, train loss <loss>=0.458774008534\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] Epoch[383] Batch[0] avg_epoch_loss=0.377942\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=383, batch=0 train loss <loss>=0.37794226408\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] Epoch[383] Batch[5] avg_epoch_loss=0.413552\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=383, batch=5 train loss <loss>=0.413552050789\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] Epoch[383] Batch [5]#011Speed: 3877.33 samples/sec#011loss=0.413552\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 583.5270881652832, \"sum\": 583.5270881652832, \"min\": 583.5270881652832}}, \"EndTime\": 1601747546.910655, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747546.326829}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1053.75869367 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] #quality_metric: host=algo-1, epoch=383, train loss <loss>=0.422347891331\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:26 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:27 INFO 139787867731776] Epoch[384] Batch[0] avg_epoch_loss=0.358059\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=384, batch=0 train loss <loss>=0.358058780432\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:27 INFO 139787867731776] Epoch[384] Batch[5] avg_epoch_loss=0.441139\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=384, batch=5 train loss <loss>=0.4411393255\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:27 INFO 139787867731776] Epoch[384] Batch [5]#011Speed: 4225.71 samples/sec#011loss=0.441139\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:27 INFO 139787867731776] Epoch[384] Batch[10] avg_epoch_loss=0.409944\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=384, batch=10 train loss <loss>=0.37251060158\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:27 INFO 139787867731776] Epoch[384] Batch [10]#011Speed: 4118.99 samples/sec#011loss=0.372511\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:27 INFO 139787867731776] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 583.3079814910889, \"sum\": 583.3079814910889, \"min\": 583.3079814910889}}, \"EndTime\": 1601747547.494403, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747546.91072}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:27 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1115.78991622 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:27 INFO 139787867731776] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=384, train loss <loss>=0.409944450991\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:27 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:27 INFO 139787867731776] Epoch[385] Batch[0] avg_epoch_loss=0.444840\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=385, batch=0 train loss <loss>=0.444840073586\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:27 INFO 139787867731776] Epoch[385] Batch[5] avg_epoch_loss=0.424177\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:27 INFO 139787867731776] #quality_metric: host=algo-1, epoch=385, batch=5 train loss <loss>=0.424177184701\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:27 INFO 139787867731776] Epoch[385] Batch [5]#011Speed: 4155.73 samples/sec#011loss=0.424177\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:28 INFO 139787867731776] Epoch[385] Batch[10] avg_epoch_loss=0.410879\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=385, batch=10 train loss <loss>=0.394921065867\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:28 INFO 139787867731776] Epoch[385] Batch [10]#011Speed: 4121.53 samples/sec#011loss=0.394921\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:28 INFO 139787867731776] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 543.2839393615723, \"sum\": 543.2839393615723, \"min\": 543.2839393615723}}, \"EndTime\": 1601747548.03821, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747547.49447}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:28 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1199.58582475 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:28 INFO 139787867731776] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=385, train loss <loss>=0.410878948867\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:28 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:28 INFO 139787867731776] Epoch[386] Batch[0] avg_epoch_loss=0.664253\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=386, batch=0 train loss <loss>=0.664253413677\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:28 INFO 139787867731776] Epoch[386] Batch[5] avg_epoch_loss=0.570861\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=386, batch=5 train loss <loss>=0.570861230294\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:28 INFO 139787867731776] Epoch[386] Batch [5]#011Speed: 3972.20 samples/sec#011loss=0.570861\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:28 INFO 139787867731776] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 524.7738361358643, \"sum\": 524.7738361358643, \"min\": 524.7738361358643}}, \"EndTime\": 1601747548.563696, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747548.038412}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:28 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1213.65278387 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:28 INFO 139787867731776] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=386, train loss <loss>=0.558503234386\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:28 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:28 INFO 139787867731776] Epoch[387] Batch[0] avg_epoch_loss=0.404133\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:28 INFO 139787867731776] #quality_metric: host=algo-1, epoch=387, batch=0 train loss <loss>=0.404132694006\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:29 INFO 139787867731776] Epoch[387] Batch[5] avg_epoch_loss=0.503015\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=387, batch=5 train loss <loss>=0.503015086055\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:29 INFO 139787867731776] Epoch[387] Batch [5]#011Speed: 3663.30 samples/sec#011loss=0.503015\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:29 INFO 139787867731776] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 541.0459041595459, \"sum\": 541.0459041595459, \"min\": 541.0459041595459}}, \"EndTime\": 1601747549.105164, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747548.563759}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:29 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1182.66894534 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:29 INFO 139787867731776] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=387, train loss <loss>=0.469765231013\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:29 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:29 INFO 139787867731776] Epoch[388] Batch[0] avg_epoch_loss=0.525357\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=388, batch=0 train loss <loss>=0.525357306004\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:29 INFO 139787867731776] Epoch[388] Batch[5] avg_epoch_loss=0.405867\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=388, batch=5 train loss <loss>=0.405866980553\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:29 INFO 139787867731776] Epoch[388] Batch [5]#011Speed: 4142.88 samples/sec#011loss=0.405867\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:29 INFO 139787867731776] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 564.3429756164551, \"sum\": 564.3429756164551, \"min\": 564.3429756164551}}, \"EndTime\": 1601747549.669928, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747549.105234}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:29 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1098.41433595 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:29 INFO 139787867731776] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:29 INFO 139787867731776] #quality_metric: host=algo-1, epoch=388, train loss <loss>=0.405589407682\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:29 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] Epoch[389] Batch[0] avg_epoch_loss=0.396599\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=389, batch=0 train loss <loss>=0.39659884572\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] Epoch[389] Batch[5] avg_epoch_loss=0.431710\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=389, batch=5 train loss <loss>=0.43170983096\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] Epoch[389] Batch [5]#011Speed: 4302.98 samples/sec#011loss=0.431710\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 527.2469520568848, \"sum\": 527.2469520568848, \"min\": 527.2469520568848}}, \"EndTime\": 1601747550.197621, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747549.67}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1171.88620938 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=389, train loss <loss>=0.468176305294\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] Epoch[390] Batch[0] avg_epoch_loss=0.428056\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=390, batch=0 train loss <loss>=0.428055912256\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] Epoch[390] Batch[5] avg_epoch_loss=0.358858\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=390, batch=5 train loss <loss>=0.358858371774\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] Epoch[390] Batch [5]#011Speed: 3936.23 samples/sec#011loss=0.358858\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] Epoch[390] Batch[10] avg_epoch_loss=0.423654\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=390, batch=10 train loss <loss>=0.501408934593\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] Epoch[390] Batch [10]#011Speed: 4064.99 samples/sec#011loss=0.501409\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 540.0271415710449, \"sum\": 540.0271415710449, \"min\": 540.0271415710449}}, \"EndTime\": 1601747550.738132, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747550.197694}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1220.09680096 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] #quality_metric: host=algo-1, epoch=390, train loss <loss>=0.423654082147\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:30 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] Epoch[391] Batch[0] avg_epoch_loss=0.370222\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=391, batch=0 train loss <loss>=0.370221942663\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] Epoch[391] Batch[5] avg_epoch_loss=0.390428\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=391, batch=5 train loss <loss>=0.390427564581\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] Epoch[391] Batch [5]#011Speed: 4334.64 samples/sec#011loss=0.390428\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 516.3731575012207, \"sum\": 516.3731575012207, \"min\": 516.3731575012207}}, \"EndTime\": 1601747551.254935, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747550.738193}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1214.01508247 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=391, train loss <loss>=0.429075315595\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] Epoch[392] Batch[0] avg_epoch_loss=0.464892\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=392, batch=0 train loss <loss>=0.464892089367\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] Epoch[392] Batch[5] avg_epoch_loss=0.387509\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=392, batch=5 train loss <loss>=0.387509102623\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] Epoch[392] Batch [5]#011Speed: 3844.35 samples/sec#011loss=0.387509\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] Epoch[392] Batch[10] avg_epoch_loss=0.358865\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=392, batch=10 train loss <loss>=0.324493101239\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] Epoch[392] Batch [10]#011Speed: 3695.45 samples/sec#011loss=0.324493\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 576.2829780578613, \"sum\": 576.2829780578613, \"min\": 576.2829780578613}}, \"EndTime\": 1601747551.831613, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747551.255}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1125.98951405 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] #quality_metric: host=algo-1, epoch=392, train loss <loss>=0.35886546563\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:31 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] Epoch[393] Batch[0] avg_epoch_loss=0.464998\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=393, batch=0 train loss <loss>=0.464997887611\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] Epoch[393] Batch[5] avg_epoch_loss=0.397169\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=393, batch=5 train loss <loss>=0.397169073423\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] Epoch[393] Batch [5]#011Speed: 3966.39 samples/sec#011loss=0.397169\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] Epoch[393] Batch[10] avg_epoch_loss=0.422541\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=393, batch=10 train loss <loss>=0.452988296747\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] Epoch[393] Batch [10]#011Speed: 4227.05 samples/sec#011loss=0.452988\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 562.2251033782959, \"sum\": 562.2251033782959, \"min\": 562.2251033782959}}, \"EndTime\": 1601747552.394289, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747551.831678}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1195.05314872 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=393, train loss <loss>=0.422541447661\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] Epoch[394] Batch[0] avg_epoch_loss=0.432817\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=394, batch=0 train loss <loss>=0.43281725049\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] Epoch[394] Batch[5] avg_epoch_loss=0.474716\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=394, batch=5 train loss <loss>=0.47471597294\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] Epoch[394] Batch [5]#011Speed: 3709.12 samples/sec#011loss=0.474716\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] Epoch[394] Batch[10] avg_epoch_loss=0.463632\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=394, batch=10 train loss <loss>=0.450330370665\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] Epoch[394] Batch [10]#011Speed: 4000.19 samples/sec#011loss=0.450330\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 545.8838939666748, \"sum\": 545.8838939666748, \"min\": 545.8838939666748}}, \"EndTime\": 1601747552.940632, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747552.39435}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1181.29700821 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] #quality_metric: host=algo-1, epoch=394, train loss <loss>=0.463631608269\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:32 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:33 INFO 139787867731776] Epoch[395] Batch[0] avg_epoch_loss=0.291966\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=395, batch=0 train loss <loss>=0.291966021061\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:33 INFO 139787867731776] Epoch[395] Batch[5] avg_epoch_loss=0.409304\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=395, batch=5 train loss <loss>=0.409304330746\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:33 INFO 139787867731776] Epoch[395] Batch [5]#011Speed: 4259.11 samples/sec#011loss=0.409304\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:33 INFO 139787867731776] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 567.5039291381836, \"sum\": 567.5039291381836, \"min\": 567.5039291381836}}, \"EndTime\": 1601747553.508542, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747552.940715}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:33 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1116.99549789 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:33 INFO 139787867731776] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=395, train loss <loss>=0.436476475\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:33 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:33 INFO 139787867731776] Epoch[396] Batch[0] avg_epoch_loss=0.450132\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=396, batch=0 train loss <loss>=0.450132220984\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:33 INFO 139787867731776] Epoch[396] Batch[5] avg_epoch_loss=0.402547\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:33 INFO 139787867731776] #quality_metric: host=algo-1, epoch=396, batch=5 train loss <loss>=0.402546880146\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:33 INFO 139787867731776] Epoch[396] Batch [5]#011Speed: 4233.65 samples/sec#011loss=0.402547\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:34 INFO 139787867731776] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 515.3350830078125, \"sum\": 515.3350830078125, \"min\": 515.3350830078125}}, \"EndTime\": 1601747554.02437, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747553.508604}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:34 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1206.74997722 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:34 INFO 139787867731776] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=396, train loss <loss>=0.381020352244\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:34 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:34 INFO 139787867731776] Epoch[397] Batch[0] avg_epoch_loss=0.323543\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=397, batch=0 train loss <loss>=0.323543101549\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:34 INFO 139787867731776] Epoch[397] Batch[5] avg_epoch_loss=0.385579\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=397, batch=5 train loss <loss>=0.385579158862\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:34 INFO 139787867731776] Epoch[397] Batch [5]#011Speed: 4314.77 samples/sec#011loss=0.385579\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:34 INFO 139787867731776] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 540.4798984527588, \"sum\": 540.4798984527588, \"min\": 540.4798984527588}}, \"EndTime\": 1601747554.565282, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747554.024439}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:34 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1126.58657031 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:34 INFO 139787867731776] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=397, train loss <loss>=0.420228797197\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:34 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:34 INFO 139787867731776] Epoch[398] Batch[0] avg_epoch_loss=0.402415\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:34 INFO 139787867731776] #quality_metric: host=algo-1, epoch=398, batch=0 train loss <loss>=0.402415186167\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] Epoch[398] Batch[5] avg_epoch_loss=0.371844\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=398, batch=5 train loss <loss>=0.371843541662\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] Epoch[398] Batch [5]#011Speed: 4071.30 samples/sec#011loss=0.371844\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 525.5780220031738, \"sum\": 525.5780220031738, \"min\": 525.5780220031738}}, \"EndTime\": 1601747555.09131, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747554.565342}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1171.82710038 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=398, train loss <loss>=0.372861135006\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] Epoch[399] Batch[0] avg_epoch_loss=0.380187\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=399, batch=0 train loss <loss>=0.380186647177\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] Epoch[399] Batch[5] avg_epoch_loss=0.360928\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=399, batch=5 train loss <loss>=0.36092787981\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] Epoch[399] Batch [5]#011Speed: 4274.53 samples/sec#011loss=0.360928\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 566.5237903594971, \"sum\": 566.5237903594971, \"min\": 566.5237903594971}}, \"EndTime\": 1601747555.658227, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747555.091374}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] #throughput_metric: host=algo-1, train throughput=1083.63029281 records/second\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] #quality_metric: host=algo-1, epoch=399, train loss <loss>=0.360533440113\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] Final loss: 0.333553001285 (occurred at epoch 373)\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] #quality_metric: host=algo-1, train final_loss <loss>=0.333553001285\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 WARNING 139787867731776] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 20.216941833496094, \"sum\": 20.216941833496094, \"min\": 20.216941833496094}}, \"EndTime\": 1601747555.679153, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747555.658289}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 34.61909294128418, \"sum\": 34.61909294128418, \"min\": 34.61909294128418}}, \"EndTime\": 1601747555.693512, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747555.679213}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 2.79998779296875, \"sum\": 2.79998779296875, \"min\": 2.79998779296875}}, \"EndTime\": 1601747555.696409, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747555.693567}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[10/03/2020 17:52:35 INFO 139787867731776] No test data passed, skipping evaluation.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 219703.37796211243, \"sum\": 219703.37796211243, \"min\": 219703.37796211243}, \"setuptime\": {\"count\": 1, \"max\": 8.849859237670898, \"sum\": 8.849859237670898, \"min\": 8.849859237670898}}, \"EndTime\": 1601747555.699635, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1601747555.696466}\n",
      "\u001b[0m\n",
      "\n",
      "2020-10-03 17:52:44 Uploading - Uploading generated training model\n",
      "2020-10-03 17:52:44 Completed - Training job completed\n",
      "Training seconds: 278\n",
      "Billable seconds: 278\n",
      "CPU times: user 1.5 s, sys: 81.1 ms, total: 1.58 s\n",
      "Wall time: 6min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": \"{}/train/\".format(s3_data_path),\n",
    "    #\"test\": \"{}/test/\".format(s3_data_path)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, content_type=sagemaker.content_types.CONTENT_TYPE_JSON, **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + timedelta(1)\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "        prediction_index = pd.DatetimeIndex(start=prediction_time, freq=freq, periods=prediction_length)        \n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    predictor_cls=DeepARPredictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_feat_pred=[]\n",
    "for col in overalldf.columns[1:]:\n",
    "    dynamic_feat_pred.append(list(overalldf[start_date-timedelta(1):end_dataset][col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2020-09-30    4.781867\n",
       "Freq: D, Name: 0.5, dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(ts=overalldf[start_date:end_dataset]['MINAVGMED'].astype('float64').resample('D').mean(),dynamic_feat=dynamic_feat_pred, quantiles=[0.10, 0.5, 0.90])[\"0.5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(\n",
    "    predictor, \n",
    "    target_ts, \n",
    "    cat=None, \n",
    "    dynamic_feat=None, \n",
    "    forecast_date=end_training, \n",
    "    show_samples=False, \n",
    "    plot_history=7,\n",
    "    confidence=80\n",
    "):\n",
    "    print(\"calling served model to generate predictions starting from {}\".format(str(forecast_date)))\n",
    "    assert(confidence > 50 and confidence < 100)\n",
    "    low_quantile = 0.5 - confidence * 0.005\n",
    "    up_quantile = confidence * 0.005 + 0.5\n",
    "        \n",
    "    # we first construct the argument to call our model\n",
    "    args = {\n",
    "        \"ts\": target_ts[:forecast_date],\n",
    "        \"return_samples\": show_samples,\n",
    "        \"dynamic_feat\":dynamic_feat,\n",
    "        \"quantiles\": [low_quantile, 0.5, up_quantile],\n",
    "        \"num_samples\": 100\n",
    "    }\n",
    "\n",
    "\n",
    "    if dynamic_feat is not None:\n",
    "        args[\"dynamic_feat\"] = dynamic_feat\n",
    "        fig = plt.figure(figsize=(20, 6))\n",
    "        ax = plt.subplot(2, 1, 1)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(20, 3))\n",
    "        ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    if cat is not None:\n",
    "        args[\"cat\"] = cat\n",
    "        ax.text(0.9, 0.9, 'cat = {}'.format(cat), transform=ax.transAxes)\n",
    "\n",
    "    # call the end point to get the prediction\n",
    "    prediction = predictor.predict(**args)\n",
    "\n",
    "    # plot the samples\n",
    "    if show_samples: \n",
    "        for key in prediction.keys():\n",
    "            if \"sample\" in key:\n",
    "                #prediction[key].plot(color='lightskyblue', alpha=0.2, label='_nolegend_')\n",
    "                plt.plot(prediction[key])\n",
    "                \n",
    "                \n",
    "    # plot the target\n",
    "    target_section = target_ts[forecast_date-plot_history:forecast_date+prediction_length]\n",
    "    plt.plot(target_section)\n",
    "\n",
    "    \n",
    "    # plot the confidence interval and the median predicted\n",
    "    ax.fill_between(\n",
    "        prediction[str(low_quantile)].index, \n",
    "        prediction[str(low_quantile)].values, \n",
    "        prediction[str(up_quantile)].values, \n",
    "        color=\"b\", alpha=0.3, label='{}% confidence interval'.format(confidence)\n",
    "    )\n",
    "    print(prediction[\"0.5\"])\n",
    "    #prediction[\"0.5\"].plot(color=\"b\", label='P50').scatter()\n",
    "    plt.plot(prediction[\"0.5\"])\n",
    "    ax.legend(loc=2)    \n",
    "    \n",
    "    # fix the scale as the samples may change it\n",
    "    ax.set_ylim(target_section.min() * 0.5, target_section.max() * 1.5)\n",
    "    \n",
    "    if dynamic_feat is not None:\n",
    "        for i, f in enumerate(dynamic_feat, start=1):\n",
    "            ax = plt.subplot(len(dynamic_feat) * 2, 1, len(dynamic_feat) + i, sharex=ax)\n",
    "            feat_ts = pd.Series(\n",
    "                index=pd.DatetimeIndex(start=target_ts.index[0], freq=target_ts.index.freq, periods=len(f)),\n",
    "                data=f\n",
    "            )\n",
    "            #feat_ts[forecast_date-plot_history:forecast_date+prediction_length].plot(ax=ax, color='g')\n",
    "            plt.plot(feat_ts[forecast_date-plot_history:forecast_date+prediction_length])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = {'description_width': 'initial'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c780dfd327414f8b268f28d908e7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='forecast_day', max=365, style=SliderStyle(description_wi"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual(\n",
    "    forecast_day=IntSlider(min=0, max=365, value=1, style=style),\n",
    "    confidence=IntSlider(min=60, max=95, value=80, step=5, style=style),\n",
    "    history_weeks_plot=IntSlider(min=0, max=10, value=1, style=style),\n",
    "    show_samples=Checkbox(value=False),\n",
    "    continuous_update=False\n",
    ")\n",
    "def plot_interact( forecast_day, confidence, history_weeks_plot, show_samples):\n",
    "    plot(\n",
    "        predictor,\n",
    "        dynamic_feat=dynamic_feat_pred,\n",
    "        target_ts=overalldf[start_date:end_dataset]['MINAVGMED'].astype('float64').resample('D').mean(),\n",
    "        forecast_date=end_training + datetime.timedelta(days=forecast_day),\n",
    "        show_samples=show_samples,\n",
    "        plot_history=history_weeks_plot*7,\n",
    "        confidence=confidence\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "carddate\n",
       "2019-10-23    5.290000 \n",
       "2019-10-24    6.295000 \n",
       "2019-10-25    6.118000 \n",
       "2019-10-26    8.940001 \n",
       "2019-10-27    7.243333 \n",
       "2019-10-28    6.833333 \n",
       "2019-10-29   NaN       \n",
       "2019-10-30    6.880000 \n",
       "2019-10-31    5.485000 \n",
       "2019-11-01    7.175000 \n",
       "2019-11-02    7.620000 \n",
       "2019-11-03    7.820000 \n",
       "2019-11-04    7.475000 \n",
       "2019-11-05    8.495000 \n",
       "2019-11-06    9.520000 \n",
       "2019-11-07    7.923333 \n",
       "2019-11-08    7.755000 \n",
       "2019-11-09   NaN       \n",
       "2019-11-10   NaN       \n",
       "2019-11-11    10.680000\n",
       "2019-11-12    9.186250 \n",
       "2019-11-13    10.340000\n",
       "2019-11-14    10.790000\n",
       "2019-11-15    9.811666 \n",
       "2019-11-16    10.059999\n",
       "2019-11-17    10.495000\n",
       "2019-11-18    11.377778\n",
       "2019-11-19    11.677500\n",
       "2019-11-20   NaN       \n",
       "2019-11-21    11.790000\n",
       "2019-11-22    10.300000\n",
       "2019-11-23    10.000000\n",
       "2019-11-24    4.330000 \n",
       "2019-11-25    11.763333\n",
       "2019-11-26    9.990000 \n",
       "2019-11-27   NaN       \n",
       "2019-11-28    11.975000\n",
       "2019-11-29    11.000000\n",
       "2019-11-30    11.506000\n",
       "2019-12-01    14.480000\n",
       "2019-12-02    12.740000\n",
       "2019-12-03    11.000000\n",
       "2019-12-04    11.790000\n",
       "2019-12-05    11.490000\n",
       "2019-12-06   NaN       \n",
       "2019-12-07    11.590000\n",
       "2019-12-08   NaN       \n",
       "2019-12-09    11.576667\n",
       "2019-12-10    13.500000\n",
       "2019-12-11    10.882500\n",
       "2019-12-12    11.990000\n",
       "2019-12-13    12.485000\n",
       "2019-12-14   NaN       \n",
       "2019-12-15    11.420000\n",
       "2019-12-16   NaN       \n",
       "2019-12-17    12.236000\n",
       "2019-12-18    11.990000\n",
       "2019-12-19   NaN       \n",
       "2019-12-20    12.750000\n",
       "2019-12-21    11.355000\n",
       "2019-12-22    12.890000\n",
       "2019-12-23    13.745000\n",
       "2019-12-24   NaN       \n",
       "2019-12-25   NaN       \n",
       "2019-12-26    12.559999\n",
       "2019-12-27    12.750000\n",
       "2019-12-28    14.996667\n",
       "2019-12-29   NaN       \n",
       "2019-12-30    13.820000\n",
       "2019-12-31    14.337500\n",
       "2020-01-01   NaN       \n",
       "2020-01-02    14.990000\n",
       "2020-01-03    14.440001\n",
       "2020-01-04    13.000000\n",
       "2020-01-05    12.375000\n",
       "2020-01-06    14.500000\n",
       "2020-01-07    12.500000\n",
       "2020-01-08    12.750000\n",
       "2020-01-09    12.000000\n",
       "2020-01-10    14.385000\n",
       "2020-01-11    11.740000\n",
       "2020-01-12    12.646000\n",
       "2020-01-13    15.000000\n",
       "2020-01-14    12.333333\n",
       "2020-01-15    11.485000\n",
       "2020-01-16   NaN       \n",
       "2020-01-17    13.500000\n",
       "2020-01-18    12.500000\n",
       "2020-01-19    11.495000\n",
       "2020-01-20    13.250000\n",
       "2020-01-21    12.500000\n",
       "2020-01-22    11.738000\n",
       "2020-01-23    11.990000\n",
       "2020-01-24    10.500000\n",
       "2020-01-25    13.250000\n",
       "2020-01-26    12.000000\n",
       "2020-01-27    10.500000\n",
       "2020-01-28    12.000000\n",
       "2020-01-29    12.940000\n",
       "2020-01-30    13.000000\n",
       "2020-01-31   NaN       \n",
       "2020-02-01    9.500000 \n",
       "2020-02-02    11.657500\n",
       "2020-02-03    13.500000\n",
       "2020-02-04    12.960000\n",
       "2020-02-05    12.160000\n",
       "2020-02-06    9.655000 \n",
       "2020-02-07    12.600000\n",
       "2020-02-08   NaN       \n",
       "2020-02-09    11.000000\n",
       "2020-02-10   NaN       \n",
       "2020-02-11    12.990000\n",
       "2020-02-12    11.000000\n",
       "2020-02-13   NaN       \n",
       "2020-02-14    13.950000\n",
       "2020-02-15    13.350000\n",
       "2020-02-16    12.033333\n",
       "2020-02-17    14.110000\n",
       "2020-02-18   NaN       \n",
       "2020-02-19    14.990000\n",
       "2020-02-20   NaN       \n",
       "2020-02-21   NaN       \n",
       "2020-02-22    12.116667\n",
       "2020-02-23   NaN       \n",
       "2020-02-24    12.000000\n",
       "2020-02-25    10.500000\n",
       "2020-02-26   NaN       \n",
       "2020-02-27   NaN       \n",
       "2020-02-28   NaN       \n",
       "2020-02-29    11.500000\n",
       "2020-03-01    11.990000\n",
       "2020-03-02    12.000000\n",
       "2020-03-03   NaN       \n",
       "2020-03-04    10.547500\n",
       "2020-03-05    17.189999\n",
       "2020-03-06    10.012500\n",
       "2020-03-07    11.900000\n",
       "2020-03-08    11.490000\n",
       "2020-03-09    9.615000 \n",
       "2020-03-10    13.000000\n",
       "2020-03-11    10.615000\n",
       "2020-03-12   NaN       \n",
       "2020-03-13    8.285000 \n",
       "2020-03-14    8.330000 \n",
       "2020-03-15    9.190000 \n",
       "2020-03-16    6.500000 \n",
       "2020-03-17    8.400000 \n",
       "2020-03-18   NaN       \n",
       "2020-03-19   NaN       \n",
       "2020-03-20    6.700000 \n",
       "2020-03-21    9.720000 \n",
       "2020-03-22    7.950000 \n",
       "2020-03-23    7.280000 \n",
       "2020-03-24    5.150000 \n",
       "2020-03-25   NaN       \n",
       "2020-03-26   NaN       \n",
       "2020-03-27   NaN       \n",
       "2020-03-28   NaN       \n",
       "2020-03-29    6.980000 \n",
       "2020-03-30    7.372500 \n",
       "2020-03-31    5.500000 \n",
       "2020-04-01    7.000000 \n",
       "2020-04-02    6.750000 \n",
       "2020-04-03    6.860000 \n",
       "2020-04-04   NaN       \n",
       "2020-04-05    6.702500 \n",
       "2020-04-06    7.743333 \n",
       "2020-04-07   NaN       \n",
       "2020-04-08    6.000000 \n",
       "2020-04-09    9.226000 \n",
       "2020-04-10    6.750000 \n",
       "2020-04-11    6.250000 \n",
       "2020-04-12    5.990000 \n",
       "2020-04-13   NaN       \n",
       "2020-04-14   NaN       \n",
       "2020-04-15    6.745000 \n",
       "2020-04-16    6.800000 \n",
       "2020-04-17    5.670000 \n",
       "2020-04-18    7.750000 \n",
       "2020-04-19    8.225000 \n",
       "2020-04-20    7.005000 \n",
       "2020-04-21    7.000000 \n",
       "2020-04-22    6.990000 \n",
       "2020-04-23    8.750000 \n",
       "2020-04-24    6.695000 \n",
       "2020-04-25    6.225000 \n",
       "2020-04-26    6.990000 \n",
       "2020-04-27    6.262000 \n",
       "2020-04-28    8.100000 \n",
       "2020-04-29    5.990000 \n",
       "2020-04-30    6.390000 \n",
       "2020-05-01    6.830833 \n",
       "2020-05-02    9.135000 \n",
       "2020-05-03    8.515000 \n",
       "2020-05-04    5.605000 \n",
       "2020-05-05    7.740000 \n",
       "2020-05-06    7.615000 \n",
       "2020-05-07    7.000000 \n",
       "2020-05-08    7.830000 \n",
       "2020-05-09   NaN       \n",
       "2020-05-10    7.095000 \n",
       "2020-05-11   NaN       \n",
       "2020-05-12    8.290000 \n",
       "2020-05-13    8.370000 \n",
       "2020-05-14    11.990000\n",
       "2020-05-15    9.975000 \n",
       "2020-05-16    6.483333 \n",
       "2020-05-17    7.330000 \n",
       "2020-05-18    5.870000 \n",
       "2020-05-19    6.800000 \n",
       "2020-05-20    9.970000 \n",
       "2020-05-21    5.750000 \n",
       "2020-05-22    9.655000 \n",
       "2020-05-23    5.970000 \n",
       "2020-05-24    6.495000 \n",
       "2020-05-25    7.500000 \n",
       "2020-05-26   NaN       \n",
       "2020-05-27    5.990000 \n",
       "2020-05-28    7.000000 \n",
       "2020-05-29    5.915000 \n",
       "2020-05-30   NaN       \n",
       "2020-05-31   NaN       \n",
       "2020-06-01    7.215000 \n",
       "2020-06-02    6.890000 \n",
       "2020-06-03    8.210000 \n",
       "2020-06-04    7.005000 \n",
       "2020-06-05    6.250000 \n",
       "2020-06-06    6.990000 \n",
       "2020-06-07    6.933333 \n",
       "2020-06-08    7.220000 \n",
       "2020-06-09    6.990000 \n",
       "2020-06-10    6.307500 \n",
       "2020-06-11    6.245000 \n",
       "2020-06-12   NaN       \n",
       "2020-06-13   NaN       \n",
       "2020-06-14    4.240000 \n",
       "2020-06-15    7.745000 \n",
       "2020-06-16    6.370000 \n",
       "2020-06-17   NaN       \n",
       "2020-06-18   NaN       \n",
       "2020-06-19    1.750000 \n",
       "2020-06-20    2.990000 \n",
       "2020-06-21    4.990000 \n",
       "2020-06-22   NaN       \n",
       "2020-06-23    6.740000 \n",
       "2020-06-24   NaN       \n",
       "2020-06-25    4.690000 \n",
       "2020-06-26    5.500000 \n",
       "2020-06-27    5.990000 \n",
       "2020-06-28    3.750000 \n",
       "2020-06-29    4.995000 \n",
       "2020-06-30   NaN       \n",
       "2020-07-01    5.000000 \n",
       "2020-07-02   NaN       \n",
       "2020-07-03    6.240000 \n",
       "2020-07-04    5.970000 \n",
       "2020-07-05   NaN       \n",
       "2020-07-06    5.122500 \n",
       "2020-07-07   NaN       \n",
       "2020-07-08    2.000000 \n",
       "2020-07-09   NaN       \n",
       "2020-07-10    3.787500 \n",
       "2020-07-11    6.385000 \n",
       "2020-07-12   NaN       \n",
       "2020-07-13   NaN       \n",
       "2020-07-14    5.120000 \n",
       "2020-07-15    5.490000 \n",
       "2020-07-16    8.465000 \n",
       "2020-07-17    6.280000 \n",
       "2020-07-18   NaN       \n",
       "2020-07-19    7.870000 \n",
       "2020-07-20    5.870000 \n",
       "2020-07-21   NaN       \n",
       "2020-07-22   NaN       \n",
       "2020-07-23    5.950000 \n",
       "2020-07-24    7.250000 \n",
       "2020-07-25    6.775000 \n",
       "2020-07-26    5.500000 \n",
       "2020-07-27    5.485000 \n",
       "2020-07-28   NaN       \n",
       "2020-07-29   NaN       \n",
       "2020-07-30    6.500000 \n",
       "2020-07-31    6.960000 \n",
       "2020-08-01   NaN       \n",
       "2020-08-02    6.390000 \n",
       "2020-08-03   NaN       \n",
       "2020-08-04    5.736667 \n",
       "2020-08-05   NaN       \n",
       "2020-08-06    5.480000 \n",
       "2020-08-07    5.490000 \n",
       "2020-08-08    5.331667 \n",
       "2020-08-09   NaN       \n",
       "2020-08-10   NaN       \n",
       "2020-08-11   NaN       \n",
       "2020-08-12    5.990000 \n",
       "2020-08-13    4.750000 \n",
       "2020-08-14    9.740000 \n",
       "2020-08-15   NaN       \n",
       "2020-08-16    2.785000 \n",
       "2020-08-17   NaN       \n",
       "2020-08-18    5.965000 \n",
       "2020-08-19   NaN       \n",
       "2020-08-20   NaN       \n",
       "2020-08-21   NaN       \n",
       "2020-08-22    6.040000 \n",
       "2020-08-23    7.325000 \n",
       "2020-08-24   NaN       \n",
       "2020-08-25    7.562500 \n",
       "2020-08-26    6.000000 \n",
       "2020-08-27    5.197500 \n",
       "2020-08-28   NaN       \n",
       "2020-08-29    6.000000 \n",
       "2020-08-30    6.580000 \n",
       "2020-08-31   NaN       \n",
       "2020-09-01    6.990000 \n",
       "2020-09-02    5.990000 \n",
       "2020-09-03    6.540000 \n",
       "2020-09-04    6.250000 \n",
       "2020-09-05    6.025000 \n",
       "2020-09-06    4.275000 \n",
       "2020-09-07    8.000000 \n",
       "2020-09-08   NaN       \n",
       "2020-09-09    8.000000 \n",
       "2020-09-10    6.875000 \n",
       "2020-09-11    6.245000 \n",
       "2020-09-12   NaN       \n",
       "2020-09-13   NaN       \n",
       "2020-09-14    10.000000\n",
       "2020-09-15   NaN       \n",
       "2020-09-16    5.975000 \n",
       "2020-09-17    5.750000 \n",
       "2020-09-18    5.500000 \n",
       "2020-09-19   NaN       \n",
       "2020-09-20    5.915000 \n",
       "2020-09-21    7.910000 \n",
       "2020-09-22    6.140000 \n",
       "2020-09-23   NaN       \n",
       "2020-09-24   NaN       \n",
       "2020-09-25   NaN       \n",
       "2020-09-26    5.990000 \n",
       "2020-09-27   NaN       \n",
       "2020-09-28    2.925000 \n",
       "2020-09-29   NaN       \n",
       "Freq: D, Name: MINAVGMED, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overalldf[start_date:end_dataset]['MINAVGMED'].astype('float64').resample('D').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
