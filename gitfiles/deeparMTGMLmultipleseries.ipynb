{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==0.23.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.23.4)\n",
      "Requirement already satisfied: pytz>=2011k in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas==0.23.4) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas==0.23.4) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas==0.23.4) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas==0.23.4) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas==0.23.4\n",
    "#.asfreq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "\n",
    "import psycopg2\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import pytz\n",
    "import math as math\n",
    "from scipy.interpolate import interp1d\n",
    "import sys\n",
    "\n",
    "\n",
    "import sys\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = 'mtgmlbucketmultseries'  # replace with an existing bucket if needed\n",
    "s3_prefix = 'mtgml-notebook'    # prefix used for all data stored within the bucket\n",
    "\n",
    "role = sagemaker.get_execution_role()             # IAM role to use by SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    }
   ],
   "source": [
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End training: 2020-11-18 00:00:00, End testing: 2020-11-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "freq='D'\n",
    "prediction_length=7\n",
    "context_length=7\n",
    "\n",
    "start_date_othervarbs=pd.Timestamp(\"2019-10-16\", freq=freq)\n",
    "start_date = pd.Timestamp(\"2019-10-23\", freq=freq)\n",
    "end_dataset = pd.Timestamp(\"2020-11-25\", freq=freq)\n",
    "end_training = end_dataset-timedelta(days=7)\n",
    "mid_dataset=pd.Timestamp(\"2020-09-15\",freq=freq)\n",
    "end_testing = end_dataset\n",
    "print(f'End training: {end_training}, End testing: {end_testing}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur=conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('''SELECT ebayname.cardname FROM \n",
    "((SELECT DISTINCT(cardname) FROM transactions) AS ebayname \n",
    "INNER JOIN \n",
    "(SELECT pio.cardname FROM ((SELECT DISTINCT(cardname) FROM tourninfo) AS pio \n",
    "INNER JOIN \n",
    "(SELECT DISTINCT(cardname) FROM moderntourninfo) AS mod ON pio.cardname=mod.cardname)) \n",
    "as overall ON ebayname.cardname=overall.cardname)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "analynames=cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "701"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(analynames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Marauding Raptor',)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analynames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Marauding Raptor',)\n",
      "('Desecrated Tomb',)\n",
      "('Dragonskull Summit',)\n",
      "('Tocatli Honor Guard',)\n",
      "('Sphinx of Foresight',)\n",
      "('Dragonlord Dromoka',)\n",
      "('Hour of Revelation',)\n",
      "('Llanowar Wastes',)\n",
      "('Risk Factor',)\n",
      "('Linvala, the Preserver',)\n",
      "('Needle Spires',)\n",
      "('Steel Leaf Champion',)\n",
      "('Rekindling Phoenix',)\n",
      "('Fae of Wishes',)\n",
      "('Sunpetal Grove',)\n",
      "('Hagra Mauling',)\n",
      "('Narcomoeba',)\n",
      "('Zurgo Bellstriker',)\n",
      "('Escape to the Wilds',)\n",
      "('Klothys, God of Destiny',)\n",
      "('Kozilek, the Great Distortion',)\n",
      "('Phyrexian Revoker',)\n",
      "('Idyllic Tutor',)\n",
      "('Eldritch Evolution',)\n",
      "('Steam Vents',)\n",
      "(\"Sigarda's Aid\",)\n",
      "('Terror of the Peaks',)\n",
      "('Gideon of the Trials',)\n",
      "('Valakut Awakening',)\n",
      "('Elvish Clancaller',)\n",
      "('Omnath, Locus of Creation',)\n",
      "('Gideon, Ally of Zendikar',)\n",
      "('Icon of Ancestry',)\n",
      "(\"Vivien's Arkbow\",)\n",
      "('Finale of Devastation',)\n",
      "('Mausoleum Wanderer',)\n",
      "('Growth-Chamber Guardian',)\n",
      "('Karn, the Great Creator',)\n",
      "('Rotting Regisaur',)\n",
      "('Lazav, the Multifarious',)\n",
      "('Blast Zone',)\n",
      "('Confounding Conundrum',)\n",
      "('Castle Vantress',)\n",
      "('Benalish Marshal',)\n",
      "('Brightclimb Pathway',)\n",
      "('Dictate of Kruphix',)\n",
      "('Angrath, the Flame-Chained',)\n",
      "('Vraska, Golgari Queen',)\n",
      "('Oko, Thief of Crowns',)\n",
      "('Abrupt Decay',)\n",
      "('Sorin, Vengeful Bloodlord',)\n",
      "('Ramunap Excavator',)\n",
      "('Mistcaller',)\n",
      "('Siege-Gang Commander',)\n",
      "('Dragonlord Ojutai',)\n",
      "('Lutri, the Spellchaser',)\n",
      "('Hydroid Krasis',)\n",
      "('Kaervek, the Spiteful',)\n",
      "('Absorb',)\n",
      "('Supreme Verdict',)\n",
      "('Pia and Kiran Nalaar',)\n",
      "('Battle at the Bridge',)\n",
      "('Steel Overseer',)\n",
      "('Tithe Taker',)\n",
      "('Scheming Symmetry',)\n",
      "('Throne of the God-Pharaoh',)\n",
      "('Scab-Clan Berserker',)\n",
      "(\"Dromoka's Command\",)\n",
      "('Nivmagus Elemental',)\n",
      "('Spirit of the Labyrinth',)\n",
      "('Storm Herald',)\n",
      "('Mystic Forge',)\n",
      "('Bedevil',)\n",
      "('Kaheera, the Orphanguard',)\n",
      "('Agent of Treachery',)\n",
      "('Xenagos, God of Revels',)\n",
      "('Swiftblade Vindicator',)\n",
      "('Brimaz, King of Oreskos',)\n",
      "('Infernal Reckoning',)\n",
      "('Benthic Biomancer',)\n",
      "('Heliod, Sun-Crowned',)\n",
      "('Gallia of the Endless Dance',)\n",
      "('Legion Warboss',)\n",
      "('Anafenza, the Foremost',)\n",
      "('Primal Might',)\n",
      "(\"Grafdigger's Cage\",)\n",
      "('Blood Baron of Vizkopa',)\n",
      "('Naban, Dean of Iteration',)\n",
      "('Wayward Guide-Beast',)\n",
      "('Shifting Ceratops',)\n",
      "('Nissa, Vastwood Seer',)\n",
      "('Shalai, Voice of Plenty',)\n",
      "('Vivien, Champion of the Wilds',)\n",
      "('Deafening Clarion',)\n",
      "('Prophet of Kruphix',)\n",
      "('General Kudro of Drannith',)\n",
      "('Lavinia, Azorius Renegade',)\n",
      "('Speaker of the Heavens',)\n",
      "('Nicol Bolas, the Ravager',)\n",
      "('Arasta of the Endless Web',)\n",
      "('Chandra, Acolyte of Flame',)\n",
      "(\"Urza's Ruinous Blast\",)\n",
      "('Obzedat, Ghost Council',)\n",
      "('Sarkhan, Fireblood',)\n",
      "('Lotleth Troll',)\n",
      "('Siege Rhino',)\n",
      "('Luminarch Aspirant',)\n",
      "(\"Smuggler's Copter\",)\n",
      "('Temple of Epiphany',)\n",
      "('Endless One',)\n",
      "('Aven Mindcensor',)\n",
      "('Fires of Invention',)\n",
      "('Tidebinder Mage',)\n",
      "('Angel of Invention',)\n",
      "('Kenrith, the Returned King',)\n",
      "(\"Shadows' Verdict\",)\n",
      "('The Immortal Sun',)\n",
      "('Wicked Wolf',)\n",
      "(\"Gaea's Revenge\",)\n",
      "(\"Heliod's Intervention\",)\n",
      "('Once Upon a Time',)\n",
      "('Thorn Lieutenant',)\n",
      "('Archon of Emeria',)\n",
      "('Shacklegeist',)\n",
      "('Wasteland Strangler',)\n",
      "('Xathrid Necromancer',)\n",
      "('Glasspool Mimic',)\n",
      "(\"Archon of Sun's Grace\",)\n",
      "('Rally the Ancestors',)\n",
      "('Orbs of Warding',)\n",
      "('Chandra, Awakened Inferno',)\n",
      "('Silent Gravestone',)\n",
      "('Rest in Peace',)\n",
      "('Jolrael, Mwonvuli Recluse',)\n",
      "('Mana Confluence',)\n",
      "('Ajani Steadfast',)\n",
      "('Infinite Obliteration',)\n",
      "('Tezzeret, Artifice Master',)\n",
      "('Niv-Mizzet Reborn',)\n",
      "('Summary Dismissal',)\n",
      "('Vizier of Many Faces',)\n",
      "('Mastery of the Unseen',)\n",
      "('Indatha Triome',)\n",
      "('Saheeli Rai',)\n",
      "('Field of the Dead',)\n",
      "('Sunken Hollow',)\n",
      "('Radiant Flames',)\n",
      "('Surrak, the Hunt Caller',)\n",
      "('Perilous Vault',)\n",
      "(\"Kari Zev's Expertise\",)\n",
      "('Drawn from Dreams',)\n",
      "('Stonecoil Serpent',)\n",
      "('Archangel of Thune',)\n",
      "('Ghalta, Primal Hunger',)\n",
      "('Concealed Courtyard',)\n",
      "('Chevill, Bane of Monsters',)\n",
      "('Cascading Cataracts',)\n",
      "('Sulfur Falls',)\n",
      "('Castle Garenbrig',)\n",
      "('Aegis of the Gods',)\n",
      "('Charming Prince',)\n",
      "('Barrin, Tolarian Archmage',)\n",
      "(\"Karn's Bastion\",)\n",
      "('Sheltered Thicket',)\n",
      "('Ruric Thar, the Unbowed',)\n",
      "('Languish',)\n",
      "('Nylea, Keen-Eyed',)\n",
      "('Arch of Orazca',)\n",
      "('Genesis Ultimatum',)\n",
      "('Flamewake Phoenix',)\n",
      "('Gemrazer',)\n",
      "('Pelt Collector',)\n",
      "('Time Wipe',)\n",
      "('Ulamog, the Ceaseless Hunger',)\n",
      "(\"Assassin's Trophy\",)\n",
      "('Spell Queller',)\n",
      "('Obosh, the Preypiercer',)\n",
      "('Massacre Wurm',)\n",
      "(\"Angrath's Marauders\",)\n",
      "('Vivien, Arkbow Ranger',)\n",
      "('Gilded Goose',)\n",
      "('Golos, Tireless Pilgrim',)\n",
      "('Whir of Invention',)\n",
      "('Felidar Retreat',)\n",
      "('Nissa, Worldwaker',)\n",
      "('Anafenza, Kin-Tree Spirit',)\n",
      "('Possibility Storm',)\n",
      "('Stain the Mind',)\n",
      "('Monastery Mentor',)\n",
      "('Resolute Archangel',)\n",
      "('Fetid Pools',)\n",
      "('Marwyn, the Nurturer',)\n",
      "('Nissa, Steward of Elements',)\n",
      "('Banefire',)\n",
      "('Temple of Silence',)\n",
      "('Collected Company',)\n",
      "('Search for Azcanta',)\n",
      "('Domri Rade',)\n",
      "('Canopy Vista',)\n",
      "('Sorin, Imperious Bloodlord',)\n",
      "('Settle the Wreckage',)\n",
      "('Den Protector',)\n",
      "('Jeskai Ascendancy',)\n",
      "('Skysovereign, Consul Flagship',)\n",
      "('Walking Ballista',)\n",
      "('Realm-Cloaked Giant',)\n",
      "('Toolcraft Exemplar',)\n"
     ]
    }
   ],
   "source": [
    "#analyname='Supreme Verdict'\n",
    "#for loop over cardnames starts here\n",
    "training_data=[]\n",
    "test_data=[]\n",
    "target_series=[]\n",
    "for cat,analyname in enumerate(analynames):\n",
    "    cur.execute('''SELECT carddate,price,cardtype FROM public.totalgoatbot WHERE cardname ILIKE %s''',analyname)\n",
    "    print(analyname)\n",
    "    goatbotrows=cur.fetchall()\n",
    "\n",
    "    goatdf=pd.DataFrame(goatbotrows,columns=[\"carddate\",\"price\",\"cardtype\"]).set_index(\"carddate\")\n",
    "    \n",
    "    goatdf.index=pd.to_datetime(goatdf.index,infer_datetime_format=True)\n",
    "\n",
    "    goatdfarr=[]\n",
    "    for ctype in sorted(goatdf.cardtype.unique()):\n",
    "        goatdfarr.append(goatdf[goatdf['cardtype']==ctype])\n",
    "        goatdfarr[ctype]=goatdfarr[ctype].drop(columns='cardtype')\n",
    "        goatdfarr[ctype].columns=['price'+str(ctype)]\n",
    "\n",
    "    cur.execute('''SELECT carddate,AVG(pshipq) as AVGE,percentile_cont(0.5) WITHIN GROUP (ORDER BY pshipq) as MED\n",
    "        FROM (SELECT DISTINCT title,cardname,carddate,(price+shipping)/COALESCE(availablequant,cardquantity,maybecardquantity,1) as pshipq FROM public.transactions\n",
    "        WHERE possiblybad IS FALSE\n",
    "        AND ismisprint IS NULL\n",
    "        AND cardname=%s\n",
    "        AND isfoil IS FALSE\n",
    "        AND (cardlanguage='english' or cardlanguage IS NULL)\n",
    "        AND (cardset IS NULL OR cardset NOT IN('LEA','LEB','U'))\n",
    "        AND (isemblem IS NULL OR isemblem IS FALSE)\n",
    "        AND (lotis IS FALSE OR lotis IS NULL)\n",
    "        AND (isboosterbox IS NULL OR isboosterbox IS FALSE)\n",
    "        AND cardspecial IS NULL\n",
    "        AND issleeve IS NULL\n",
    "        AND isdeck IS NULL\n",
    "        AND isplaymat IS NULL\n",
    "        AND isultra IS NULL\n",
    "        AND (istoken IS NULL OR istoken IS FALSE)\n",
    "        AND (iscommanderdeck IS NULL OR iscommanderdeck IS FALSE)\n",
    "        AND isgraded IS NULL\n",
    "        AND (othercards IS NULL OR transplit IS TRUE)\n",
    "        AND saletype IN('normal')\n",
    "        AND price+shipping<100) as foo WHERE pshipq<50 \n",
    "        GROUP BY carddate ORDER BY carddate ASC''',analyname)\n",
    "\n",
    "    ebayrows=cur.fetchall()\n",
    "\n",
    "    #ebaydf=pd.DataFrame(ebayrows,columns=[\"carddate\",\"avg\",\"med\"]) \n",
    "    ebaydf=pd.DataFrame(ebayrows,columns=[\"carddate\",\"AVG\",\"MED\"]).set_index(\"carddate\")\n",
    "    ebaydf['MINAVGMED']=ebaydf.apply(lambda row: np.minimum(row.AVG,row.MED), axis = 1)\n",
    "    ebaydf=ebaydf.drop(columns=['AVG','MED'])\n",
    "    ebaydf.index=pd.to_datetime(ebaydf.index,infer_datetime_format=True)\n",
    "\n",
    "    cur.execute('''SELECT entrydate1 as entrydate,SUM(cardquant)/AVG(aggcount) as ratio FROM(\n",
    "    SELECT DISTINCT cardname0,entrydate1,cardquant,aggcount,tourntitle FROM\n",
    "    (SELECT cardname as cardname0,entrydate as entrydate0,cardquant,tourntitle FROM public.tourninfo WHERE cardname ILIKE %s) AS foo1,\n",
    "    (SELECT entrydate as entrydate1,SUM(cardquant) as aggcount FROM public.tourninfo GROUP BY entrydate) AS bar\n",
    "    WHERE entrydate0=entrydate1) AS foo\n",
    "    GROUP BY cardname0,entrydate1''',analyname)\n",
    "\n",
    "    piotournrows=cur.fetchall()\n",
    "\n",
    "    piotourndf=pd.DataFrame(piotournrows,columns=[\"carddate\",\"pioratio\"]).set_index(\"carddate\")\n",
    "    piotourndf=piotourndf.astype({'pioratio': 'float64'})\n",
    "    \n",
    "    piotourndf.index=pd.to_datetime(piotourndf.index,infer_datetime_format=True)\n",
    "\n",
    "    cur.execute('''SELECT entrydate1 as entrydate,SUM(cardquant)/AVG(aggcount) as ratio FROM(\n",
    "    SELECT DISTINCT cardname0,entrydate1,cardquant,aggcount,tourntitle FROM\n",
    "    (SELECT cardname as cardname0,entrydate as entrydate0,cardquant,tourntitle FROM public.moderntourninfo WHERE cardname ILIKE %s) AS foo1,\n",
    "    (SELECT entrydate as entrydate1,SUM(cardquant) as aggcount FROM public.moderntourninfo GROUP BY entrydate) AS bar\n",
    "    WHERE entrydate0=entrydate1) AS foo\n",
    "    GROUP BY cardname0,entrydate1''',analyname)\n",
    "\n",
    "    moderntournrows=cur.fetchall()\n",
    "\n",
    "    moderntourndf=pd.DataFrame(moderntournrows,columns=[\"carddate\",\"modratio\"]).set_index(\"carddate\")\n",
    "    moderntourndf=moderntourndf.astype({'modratio': 'float64'})\n",
    "    \n",
    "    moderntourndf.index=pd.to_datetime(moderntourndf.index,infer_datetime_format=True)\n",
    "        \n",
    "    missingdatefix=pd.date_range(start='2019-10-15',end=end_dataset)\n",
    "    \n",
    "    moderntourndf=moderntourndf.reindex(missingdatefix).fillna(\"NaN\")\n",
    "    \n",
    "    moderntourndf.index.name='carddate'\n",
    "    \n",
    "\n",
    "    cur.execute('''SELECT entrydate1 as entrydate,SUM(cardquant)/AVG(aggcount) as ratio FROM(\n",
    "    SELECT DISTINCT cardname0,entrydate1,cardquant,aggcount,tourntitle FROM\n",
    "    (SELECT cardname as cardname0,entrydate as entrydate0,cardquant,tourntitle FROM public.standardtourninfo WHERE cardname ILIKE %s) AS foo1,\n",
    "    (SELECT entrydate as entrydate1,SUM(cardquant) as aggcount FROM public.standardtourninfo GROUP BY entrydate) AS bar\n",
    "    WHERE entrydate0=entrydate1) AS foo\n",
    "    GROUP BY cardname0,entrydate1''',analyname)\n",
    "\n",
    "    standardtournrows=cur.fetchall()\n",
    "    \n",
    "    if len(standardtournrows)==0:\n",
    "        standardtourndf=pd.DataFrame([['2020-04-03',0],['2020-04-04',0]],columns=[\"carddate\",\"standratio\"]).set_index(\"carddate\")\n",
    "    else:\n",
    "        standardtourndf=pd.DataFrame(standardtournrows,columns=[\"carddate\",\"standratio\"]).set_index(\"carddate\")\n",
    "        \n",
    "    standardtourndf=standardtourndf.astype({'standratio': 'float64'})\n",
    "    standardtourndf.index=pd.to_datetime(standardtourndf.index,infer_datetime_format=True)\n",
    "\n",
    "    goatdfarr.insert(0,ebaydf)\n",
    "    goatdfarr.append(piotourndf)\n",
    "    goatdfarr.append(moderntourndf)\n",
    "    goatdfarr.append(standardtourndf)\n",
    "    overalldf=pd.concat(goatdfarr,axis=1,sort=True)\n",
    "\n",
    "#     overalldf.index=pd.to_datetime(overalldf.index,infer_datetime_format=True)\n",
    "\n",
    "#     overalldf.index.name='carddate'\n",
    "    \n",
    "    overalldf=pd.concat([overalldf['MINAVGMED'].fillna(\"NaN\"),\n",
    "                         overalldf.loc[:, overalldf.columns.difference(['MINAVGMED'])].fillna(method='ffill').fillna(method='bfill')],\n",
    "                        axis=1,join='inner')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    dynamic_feat_arr=[]\n",
    "    for col in overalldf.columns[1:]:\n",
    "        dynamic_feat_arr.append(list(overalldf[start_date-timedelta(prediction_length):end_training-timedelta(prediction_length)][col]))\n",
    "\n",
    "\n",
    "\n",
    "    dynamic_feat_arr_test=[]\n",
    "\n",
    "    for col in overalldf.columns[1:]:\n",
    "        dynamic_feat_arr_test.append(list(overalldf[start_date-timedelta(days=prediction_length):end_dataset-timedelta(days=prediction_length)][col]))\n",
    "    \n",
    "    if(np.array(dynamic_feat_arr).shape[1]==len(overalldf[start_date:end_training]['MINAVGMED']) and\n",
    "      np.array(dynamic_feat_arr_test).shape[1]==len(overalldf[start_date:end_dataset]['MINAVGMED'])):\n",
    "        training_data.append({\"start\":str(start_date),\n",
    "                       \"target\":list(overalldf[start_date:end_training]['MINAVGMED']),\n",
    "                        \"cat\":[cat],\n",
    "                       \"dynamic_feat\":dynamic_feat_arr})\n",
    "\n",
    "\n",
    "        test_data.append({\"start\":str(start_date),\n",
    "                       \"target\":list(overalldf[start_date:end_dataset]['MINAVGMED']),\n",
    "                          \"cat\":[cat],\n",
    "                       \"dynamic_feat\":dynamic_feat_arr_test})\n",
    "        \n",
    "        target_series.append(overalldf[start_date:end_dataset]['MINAVGMED'])\n",
    "    else:\n",
    "        print(str(analyname)+str('error'))\n",
    "        sys.exit()\n",
    "#for loop over cardnames ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overalldf['MINAVGMED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))\n",
    "\n",
    "\n",
    "write_dicts_to_file(\"train.json\", training_data)\n",
    "write_dicts_to_file(\"test.json\",test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    \n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print('File s3://{}/{} already exists.\\nSet override to upload anyway.\\n'.format(s3_bucket, s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_s3(\"train.json\", s3_data_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test.json\", s3_data_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overalldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.xlarge',\n",
    "    base_job_name='deepar-mtgml',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": \"{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"{}/test/\".format(s3_data_path)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, content_type=sagemaker.content_types.CONTENT_TYPE_JSON, **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + timedelta(1)\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "        prediction_index = pd.DatetimeIndex(start=prediction_time, freq=freq, periods=prediction_length)        \n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    predictor_cls=DeepARPredictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_feat_pred=[]\n",
    "for col in overalldf.columns[1:]:\n",
    "    dynamic_feat_pred.append(list(overalldf[start_date-timedelta(prediction_length):end_dataset][col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(ts=overalldf[start_date:end_dataset]['MINAVGMED'].astype('float64').asfreq(freq),dynamic_feat=dynamic_feat_pred, quantiles=[0.10, 0.5, 0.90])[\"0.5\"]\n",
    "#.resample('D').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(\n",
    "    predictor, \n",
    "    target_ts, \n",
    "    cat=None, \n",
    "    dynamic_feat=None, \n",
    "    forecast_date=end_training, \n",
    "    show_samples=False, \n",
    "    plot_history=7,\n",
    "    confidence=80\n",
    "):\n",
    "    print(\"calling served model to generate predictions starting from {}\".format(str(forecast_date)))\n",
    "    assert(confidence > 50 and confidence < 100)\n",
    "    low_quantile = 0.5 - confidence * 0.005\n",
    "    up_quantile = confidence * 0.005 + 0.5\n",
    "        \n",
    "    # we first construct the argument to call our model\n",
    "    args = {\n",
    "        \"ts\": target_ts[:forecast_date],\n",
    "        \"return_samples\": show_samples,\n",
    "        \"dynamic_feat\":dynamic_feat,\n",
    "        \"quantiles\": [low_quantile, 0.5, up_quantile],\n",
    "        \"num_samples\": 100\n",
    "    }\n",
    "\n",
    "\n",
    "    if dynamic_feat is not None:\n",
    "        args[\"dynamic_feat\"] = dynamic_feat\n",
    "        fig = plt.figure(figsize=(20, 6))\n",
    "        ax = plt.subplot(2, 1, 1)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(20, 3))\n",
    "        ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    if cat is not None:\n",
    "        args[\"cat\"] = cat\n",
    "        ax.text(0.9, 0.9, 'cat = {}'.format(cat), transform=ax.transAxes)\n",
    "\n",
    "    # call the end point to get the prediction\n",
    "    prediction = predictor.predict(**args)\n",
    "\n",
    "    # plot the samples\n",
    "    if show_samples: \n",
    "        for key in prediction.keys():\n",
    "            if \"sample\" in key:\n",
    "                #prediction[key].plot(color='lightskyblue', alpha=0.2, label='_nolegend_')\n",
    "                plt.plot(prediction[key])\n",
    "                \n",
    "                \n",
    "    # plot the target\n",
    "    target_section = target_ts[forecast_date-plot_history:forecast_date+prediction_length]\n",
    "    #print(target_section)\n",
    "    target_section=target_section.interpolate(method='linear')\n",
    "    #print(target_section)\n",
    "    #target_section.plot()\n",
    "    plt.plot(target_section)\n",
    "\n",
    "    \n",
    "    # plot the confidence interval and the median predicted\n",
    "    ax.fill_between(\n",
    "        prediction[str(low_quantile)].index, \n",
    "        prediction[str(low_quantile)].values, \n",
    "        prediction[str(up_quantile)].values, \n",
    "        color=\"b\", alpha=0.3, label='{}% confidence interval'.format(confidence)\n",
    "    )\n",
    "    print(prediction[\"0.5\"])\n",
    "    #prediction[\"0.5\"].plot(color=\"b\", label='P50').scatter()\n",
    "    plt.plot(prediction[\"0.5\"])\n",
    "    ax.legend(loc=2)    \n",
    "    \n",
    "    # fix the scale as the samples may change it\n",
    "    ax.set_ylim(target_section.min() * 0.5, target_section.max() * 1.5)\n",
    "    \n",
    "    if dynamic_feat is not None:\n",
    "        for i, f in enumerate(dynamic_feat, start=1):\n",
    "            ax = plt.subplot(len(dynamic_feat) * 2, 1, len(dynamic_feat) + i, sharex=ax)\n",
    "            feat_ts = pd.Series(\n",
    "                index=pd.DatetimeIndex(start=target_ts.index[0], freq=target_ts.index.freq, periods=len(f)),\n",
    "                data=f\n",
    "            )\n",
    "            #feat_ts[forecast_date-plot_history:forecast_date+prediction_length].plot(ax=ax, color='g')\n",
    "            plt.plot(feat_ts[forecast_date-plot_history:forecast_date+prediction_length])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = {'description_width': 'initial'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact_manual(\n",
    "    card_id=IntSlider(min=0, max=701, value=91, style=style), ###NEED TO DO SOMETHING WITH THIS\n",
    "    forecast_day=IntSlider(min=0, max=365, value=0, style=style),\n",
    "    confidence=IntSlider(min=60, max=95, value=80, step=5, style=style),\n",
    "    history_weeks_plot=IntSlider(min=0, max=10, value=1, style=style),\n",
    "    show_samples=Checkbox(value=False),\n",
    "    continuous_update=False\n",
    ")\n",
    "def plot_interact( forecast_day, confidence, history_weeks_plot, show_samples):\n",
    "    plot(\n",
    "        predictor,\n",
    "        dynamic_feat=dynamic_feat_pred,\n",
    "        target_ts=overalldf[start_date:end_dataset]['MINAVGMED'].astype('float64').asfreq(freq),\n",
    "        #.resample('D').mean(),\n",
    "        forecast_date=end_training + datetime.timedelta(days=forecast_day),\n",
    "        show_samples=show_samples,\n",
    "        plot_history=history_weeks_plot*7,\n",
    "        confidence=confidence\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_training"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
